# 媒体与认知 — 循环神经网络与图神经网络 讲义

## 9-1.pdf 第 1-59 页内容

### 第 1 页：Media and Cognition（媒体与认知）、Lecture 9: Recurrent Neural Networks & Graph Neural Networks（第 9 讲：循环神经网络与图神经网络）

- **课程信息**

  

  清华大学电子工程系（Dept. of EE, Tsinghua University）

  

  方璐（Lu FANG）

- **页面说明**

  

  本页为课程封面，明确第 9 讲核心主题为

  循环神经网络（RNN）

   

  与

  图神经网络（GNN）

  ，是媒体与认知课程中序列建模与非结构化数据建模的关键内容。

### 第 2 页：无明确标题（含乱码与数据）

- **页面内容说明**

  

  本页包含部分乱码（如 “TECCCAACATATG”“TATAGIAGTAATCG” 等）、数字（如 “26.4 228”“96.58”）及无意义符号，推测为文档转换过程中的格式错误或冗余信息，无实际教学内容，可忽略。

### 第 3 页：无明确标题（含数字与符号）

- **页面内容说明**

  

  本页仅包含数字（如 “38”“0”“32”“2”“3”）、英文单词 “waph” 及乱码（如 “税 0:2502634 芏”），无有效教学信息，推测为格式错误，可忽略。

### 第 4 页：Sequence Modeling Applications（序列建模应用）- One to one（一对一）

- **核心内容：一对一序列建模场景**

  

  一对一模式指 “

  一个输入对应一个标签

  ”，典型应用为

  分类任务

  ，即通过单一输入数据输出一个类别标签。

#### 1. 模型结构

- 输入：单个特征向量 X
- 中间层：通过权重矩阵 \(W_1\) 映射到隐藏层 h，再通过权重矩阵 \(W_2\) 输出预测结果 S
- 关键操作：`cat`（拼接）用于整合特征（如示例中 3072 维特征拼接后映射到 100 维隐藏层，再输出 10 维类别）

#### 2. 应用案例

- **图像分类**：输入一张图像（如青蛙、马、船、卡车），输出对应的类别标签（“frog”“horse”“ship”“truck”）。
- **二分类任务**：输入学生信息（如 “Student 1”“Student 2”“Student N”），输出 “通过（pass）” 或 “失败（fail）”；输入文本情感，输出 “积极（Positive）” 或 “消极（Negative）”。

**专业术语解释**

- **分类（Classification）**：根据输入数据的特征，将其分配到预定义类别的任务，是机器学习中最基础的任务之一。
- **特征向量（Feature Vector）**：将数据的属性量化后形成的向量，如图像的像素值、文本的词嵌入等。

### 第 5 页：Sequence Modeling Applications（序列建模应用）- One to many（一对多）

- **核心内容：一对多序列建模场景**

  

  一对多模式指 “

  一个输入对应一个序列输出

  ”，典型应用为

  图像描述（Image Captioning）

  ，即输入一张图像，输出描述图像内容的文字序列。

#### 1. 模型逻辑

- 输入：单张图像（视为一个整体输入）
- 输出：连续的单词序列（描述图像内容）
- 核心：模型需从图像中提取视觉特征（如物体、场景、动作），并将其转化为符合语法的自然语言序列。

#### 2. 应用案例

- 图像 1：“Two people walking on the beach with surfboards”（两个人拿着冲浪板在沙滩上行走）
- 图像 2：“A man riding a dirt bike on a dirt track”（一个人在土路上骑越野摩托车）
- 图像 3：“A cat is sitting on a tree branch”（一只猫坐在树枝上）
- 图像 4：“A dog is running in the grass with a frisbee”（一只狗叼着飞盘在草地上跑）

**专业术语解释**

- **图像描述（Image Captioning）**：结合计算机视觉（提取图像特征）和自然语言处理（生成文本）的跨模态任务，需同时理解视觉内容和语言语法。
- **视觉特征（Visual Feature）**：图像中可用于表征内容的属性，如物体轮廓、颜色、纹理、空间位置关系等。

### 第 6 页：Sequence Modeling Applications（序列建模应用）- Many to one（多对一）

- **核心内容：多对一序列建模场景**

  

  多对一模式指 “

  一个序列输入对应一个标签输出

  ”，典型应用包括

  情感分类

  、

  动作预测

  ，此外还补充了 “文本生成图像” 的反向场景。

#### 1. 典型应用

##### （1）情感分类（Sentiment Classification）

- 输入：单词序列（如 “Great service for an affordable price.”，意为 “价格实惠，服务优质”）
- 输出：单一情感标签（如 “积极”“消极”“中性”）
- 核心：模型需捕捉序列中的语义倾向，判断整体情感极性。

##### （2）动作预测（Action Prediction）

- 输入：视频帧序列（连续的图像帧，记录动作过程）
- 输出：单一动作类别（如示例中的 “击掌（high five）”）
- 来源：图像来自 MIT CSAIL（麻省理工学院计算机科学与人工智能实验室），展示了通过视频序列预测人类动作的场景。

#### 2. 补充应用：文本生成图像（Image Generation from Text）

- 输入：文本描述（如 “An astronaut riding a horse in a photorealistic style”，意为 “宇航员骑着马的照片级真实风格图像”）
- 输出：对应文本内容的图像
- 案例：
  - “Teddy bears mixing sparkling chemicals in a steampunk style”（泰迪熊在蒸汽朋克风格中混合闪光化学物质）
- 参考工具：OpenAI 的 DALL-E 2（文本生成图像的经典模型，链接：https://openai.com/dall-e-2/）

**专业术语解释**

- **情感极性（Sentiment Polarity）**：文本表达的情感倾向，通常分为积极、消极、中性三类，是情感分类的核心判断目标。
- **视频帧序列（Video Frame Sequence）**：视频由连续的静态图像帧组成，帧序列包含了动作的时间动态信息，是动作预测的输入基础。

### 第 7 页：Sequence Modeling Applications（序列建模应用）- Many to many（多对多）

- **核心内容：多对多序列建模场景**

  

  多对多模式指 “

  一个序列输入对应另一个序列输出

  ”，典型应用包括

  机器翻译

  、

  下一个单词预测

  、

  视频描述

  。

#### 1. 典型应用

##### （1）机器翻译（Machine Translation）

- 输入：源语言文本序列（如中文、日语、韩语）
- 输出：目标语言文本序列（如英文）
- 案例：输入 “A 文”（假设为中文 “越过线了！”），输出英文 “Over the line!”，并标注句末符号`<eos>`（End of Sentence，句子结束标记）
- 参考工具：Google 翻译、有道翻译（youdao Translate）

##### （2）下一个单词预测（Next Words Prediction）

- 输入：前缀文本序列（如 “what is the weather tomorrow”，意为 “明天天气如何”）
- 输出：后续文本序列（补全句子或预测下一个 / 多个单词）
- 案例：
  - 输入 “what is the water cycle”，输出 “what is the world environment day”（预测相关主题的句子）
  - 输入 “I'll meet you at the”，输出 “office”“airport”“cafe”（预测可能的地点名词）
  - 输入 “what is the w”，输出 “weather today”（补全 “今天天气如何”）

##### （3）视频描述（Video Captioning）

- 输入：视频帧序列（连续的图像帧，记录视频内容）
- 输出：描述视频内容的文本序列（可包含多个句子，即 “caption caption”）
- 案例：输入一段视频，输出 “A baby is dancing on a chair”（一个婴儿在椅子上跳舞）

**专业术语解释**

- **句末标记（End of Sentence, <eos>）**：在序列建模中用于标记句子结束的特殊符号，帮助模型区分句子边界，避免序列混淆。
- **前缀文本（Prefix Text）**：下一个单词预测任务中，作为输入的部分文本，模型需基于前缀的语义和语法规则，预测后续内容。

### 第 8 页：Build a Sequence Model（构建序列模型）- 机器翻译任务与传统方法局限

- **核心内容：以机器翻译为例，分析传统全连接神经网络（MLP）在序列建模中的不足**

#### 1. 机器翻译任务定义

- 输入：源语言单词序列 \(x^{(1)}, x^{(2)}, ..., x^{(t)}\)（如中文 “她在吃一个苹果”）
- 输出：目标语言单词的概率分布 \(\hat{y}^{(1)}, \hat{y}^{(2)}, ..., \hat{y}^{(t)}\)（如英文 “`She is eating an apple`” 的概率分布，每个位置对应一个单词的概率）

#### 2. 机器翻译技术发展历程（补充背景）

- 1950s：基于规则的机器翻译（Rule-based MT）
- 1980s：基于词典的机器翻译（Dictionary-based MT）
- 1990s：基于转移的机器翻译（Transfer-based MT）、基于中间语的机器翻译（Interlingual MT）、基于示例的机器翻译（Example-based MT）
- 2010s：基于统计的机器翻译（Statistical MT）
- 2015 年后：基于神经网络的机器翻译（Neural MT，如 Google Neural Machine Translation）
- 案例：输入土耳其语 “Cizgiyi gectin <eos>”，输出英文 “Over the line <eos>”

#### 3. 传统 MLP（全连接 DNN）的问题：整体应用于序列

- 模型逻辑：将整个输入序列（如 “她在吃一个苹果”）作为一个整体，输入全连接神经网络（FC-DNN），直接输出目标序列（如 “`She is eating an apple`”）
- 核心缺陷：**效率极低**
  - 长序列需构建超大尺寸的 DNN（如 10 个单词的序列需处理 10 倍于单个单词的特征，序列越长，模型规模呈指数级增长）
  - 理论上，DNN 永远无法满足超长序列的建模需求（序列长度无上限，模型规模不可能无限扩大）

**专业术语解释**

- **全连接神经网络（Fully-Connected DNN, MLP）**：每一层的所有神经元都与下一层的所有神经元连接的神经网络，参数数量大，适合处理固定长度的输入，但不适合序列数据。
- **概率分布（Probabilistic Distribution）**：在机器翻译中，输出每个位置的单词属于词汇表中每个词的概率，通过 Softmax 函数计算，概率最高的单词即为预测结果。

### 第 9 页：Build a Sequence Model（构建序列模型）- 传统 MLP 的另一局限

- **核心内容：分析 “将 MLP 单独应用于每个单词” 的序列建模方式及其缺陷**

#### 1. 模型逻辑

- 输入：序列中的每个单词单独作为输入（如 “她”“在”“吃”“苹果”）
- 操作：为每个单词分配一个独立的 MLP，每个 MLP 输出对应位置的目标单词（如 “她”→“She”，“在”→“is”，“吃”→“eating”，“苹果”→“apple”）
- 示例：输入中文 “她在吃苹果”，输出英文 “`She is eating apple`”（注意：此处 “一个” 的翻译 “an” 未被正确生成，暴露模型缺陷）

#### 2. 核心缺陷：**无法建模序列关系**

- 每个单词的预测仅依赖自身特征，与前后单词无关（如 “吃” 的翻译 “eating” 未考虑前面的 “在” 需搭配 “is”，“苹果” 的翻译 “apple” 未考虑前面需加不定冠词 “an”）
- 序列数据的核心价值在于 “时序依赖”（如语法规则、语义连贯性），而该方法完全忽略了这一点，导致预测结果不符合语言逻辑（如示例中缺少 “an”，句子语法错误）

**专业术语解释**

- **时序依赖（Sequential Dependency）**：序列数据中，当前元素的含义或取值依赖于前后元素的特性，如文本中单词的搭配、视频中帧的先后顺序，是序列建模的核心对象。
- **语法连贯性（Grammatical Coherence）**：自然语言中句子需符合语法规则（如冠词、介词的正确使用），时序依赖建模是保证语法连贯的关键。

### 第 10 页：Build a Sequence Model（构建序列模型）- 循环神经网络（RNN）原理

- **核心内容：介绍 RNN 的基本结构与优势，解决传统 MLP 无法建模时序依赖的问题**

#### 1. RNN 的核心特性

- **时序建模能力**：前一时刻的隐藏状态（\(h_{t-1}\)）会影响当前时刻的隐藏状态（\(h_t\)），进而影响当前输出（\(\hat{y}_t\)），实现 “记忆” 前序信息的功能。
- **可变长度输入**：可处理任意长度的输入序列（无需固定输入尺寸），适用于文本、视频等变长数据。

#### 2. RNN 的数学表达式

\(y_{t}=RNN\left(x_{t}, h_{t-1}\right)=RNN\left(x_{t}, x_{t-1}, ..., x_{2}, x_{1}\right)\)

- \(x_t\)：当前时刻的输入（如第t个单词）
- \(h_{t-1}\)：前一时刻的隐藏状态（包含前\(t-1\)个输入的信息）
- \(y_t\)：当前时刻的输出（如第t个单词的预测结果）
- 本质：当前输出依赖于所有历史输入（\(x_1\)到\(x_t\)），实现时序依赖建模。

#### 3. 应用案例：中文到英文的机器翻译

- 输入序列：中文 “她”“在”“吃”“一个”“苹果”（对应\(x^{(1)}\)到\(x^{(5)}\)）
- 隐藏状态：每个时刻的隐藏状态（\(h^{(1)}\)到\(h^{(5)}\)）继承前序信息（如\(h^{(2)}\)包含 “她” 和 “在” 的信息，\(h^{(3)}\)包含 “她”“在”“吃” 的信息）
- 输出序列：英文 “`She`”“`is`”“`eating`”“`an`”“`apple`”（每个 MLP 共享参数，基于当前隐藏状态输出正确单词，解决了传统 MLP 语法错误的问题）

**专业术语解释**

- **隐藏状态（Hidden State）**：RNN 中存储历史信息的核心组件，相当于模型的 “记忆”，当前隐藏状态由当前输入和前一隐藏状态共同计算得到，包含了所有前序输入的关键信息。
- **参数共享（Parameter Sharing）**：RNN 中所有时刻的 MLP 共享同一套权重参数（而非每个时刻独立参数），大幅减少模型参数数量，同时保证时序依赖的一致性（如语法规则在整个序列中统一）。

### 第 11 页：Build a Sequence Model（构建序列模型）- 简单 RNN 语言模型

- **核心内容：详细拆解简单 RNN 语言模型的结构、输入、隐藏状态与输出计算过程**

#### 1. 模型结构三部分

##### （1）输入层：单词嵌入（Word Embeddings）

- 输入：词汇表中的单词（如 “the”“students”“opened”“their”“exams”）
- 表示：每个单词被编码为固定维度的向量 \(x^{(t)} \in \mathbb{R}^{|V|}\)，其中\(|V|\)为词汇表大小（如词汇表有 10000 个词，则\(x^{(t)}\)为 10000 维向量，通常通过 One-Hot 编码或预训练词嵌入（如 Word2Vec）实现）
- 作用：将离散的单词转化为连续的向量，便于神经网络处理（离散值无法计算梯度，向量可参与反向传播）。

##### （2）隐藏层：隐藏状态（Hidden States）

- 计算方式：\(h^{(t)}=\sigma\left(W_{h} h^{(t-1)} + W_{x} x^{(t)} + b\right)\)
  - \(W_h\)：隐藏状态到隐藏状态的权重矩阵（控制历史信息的传递）
  - \(W_x\)：输入向量到隐藏状态的权重矩阵（控制当前输入的影响）
  - b：偏置项
  - \(\sigma(\cdot)\)：激活函数（通常为 Sigmoid 或 Tanh，用于引入非线性，增强模型表达能力）
  - \(h^{(0)}\)：初始隐藏状态（通常设为全零向量，代表 “初始无记忆”）
- 作用：整合当前输入（\(x^{(t)}\)）和历史信息（\(h^{(t-1)}\)），生成当前时刻的 “记忆向量”。

##### （3）输出层：输出分布（Output Distribution）

- 计算方式：\(\hat{y}^{(t)}=softmax\left(U h^{(t)} + b_{2}\right) \in \mathbb{R}^{|V|}\)
  - U：隐藏状态到输出层的权重矩阵
  - \(b_2\)：输出层偏置项
  - \(softmax(\cdot)\)：激活函数（将隐藏状态映射为词汇表中每个单词的概率，所有概率和为 1）
- 作用：输出当前时刻预测单词的概率分布，概率最高的单词即为预测结果。

#### 2. 示例：文本序列建模

- 输入序列：“the students opened their exams”（5 个单词，对应\(x^{(1)}\)到\(x^{(5)}\)）
- 隐藏状态：\(h^{(1)}\)（含 “the” 的信息）→ \(h^{(2)}\)（含 “the”“students” 的信息）→ ... → \(h^{(5)}\)（含全部 5 个单词的信息）
- 输出序列：\(\hat{y}^{(1)}\)（预测 “students” 的概率分布）→ \(\hat{y}^{(2)}\)（预测 “opened” 的概率分布）→ ... → \(\hat{y}^{(5)}\)（预测句末或下一个单词的概率分布）

**专业术语解释**

- **单词嵌入（Word Embedding）**：将单词映射到低维连续向量空间的技术，使语义相似的单词在向量空间中距离相近（如 “猫” 和 “狗” 的向量距离小于 “猫” 和 “汽车”），是自然语言处理的基础技术。
- **Softmax 函数**：将任意实数向量映射为概率分布的函数，公式为\(\hat{y}_i = \frac{e^{z_i}}{\sum_{j=1}^{|V|} e^{z_j}}\)，其中\(z_i\)为输出层未激活的 logit 值，确保输出概率非负且和为 1。

### 第 12 页：Train an RNN Sequence Model（训练 RNN 序列模型）- 数据与损失函数

- **核心内容：介绍 RNN 训练的数据集准备、损失函数定义（交叉熵）与整体损失计算**

#### 1. 训练数据准备

- 输入：大规模文本语料库（Corpus），包含连续的单词序列 \(x^{(1)}, ..., x^{(T)}\)（如 “the students opened their exams”，\(T=5\)为序列长度）
- 操作：将序列输入 RNN，在每个时间步t计算输出概率分布\(\hat{y}^{(t)}\)（预测下一个单词的概率）。

#### 2. 损失函数定义

##### （1）单个时间步的损失：交叉熵（Cross-Entropy, CE）

- 目标：衡量预测概率分布\(\hat{y}^{(t)}\)与真实标签\(y^{(t)}\)之间的差异，差异越小，损失越小。
- 真实标签\(y^{(t)}\)：通常为 “下一个单词的 One-Hot 编码”（如输入\(x^{(t)}\)为 “students” 时，真实标签\(y^{(t)}\)为 “opened” 的 One-Hot 向量，即该向量中 “opened” 对应位置为 1，其余为 0）。
- 数学公式：\(J^{(t)}(\theta)=CE\left(y^{(t)}, \hat{y}^{(t)}\right)=-\sum_{i=1}^{N} y_{i}^{(t)} log \hat{y}_{i}^{(t)}\)
  - \(\theta\)：模型的所有参数（\(W_h, W_x, b, U, b_2\)）
  - N：词汇表大小（即\(|V|\)）
  - \(y_i^{(t)}\)：真实标签\(y^{(t)}\)的第i个元素（One-Hot 编码，仅目标单词位置为 1，其余为 0）
  - \(\hat{y}_i^{(t)}\)：预测分布\(\hat{y}^{(t)}\)的第i个元素（目标单词的预测概率）
- 含义：当预测概率\(\hat{y}_i^{(t)}\)接近 1（预测正确）时，\(log \hat{y}_i^{(t)}\)接近 0，损失接近 0；当预测错误时，损失显著增大。

##### （2）整体损失：所有时间步损失的平均值

- 数学公式：\(J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta) = -\frac{1}{T} \sum_{t=1}^{T} \sum_{i=1}^{N} y_{i}^{(t)} log \hat{y}_{i}^{(t)}\)
  - T：序列长度（如示例中\(T=5\)）
  - 作用：综合整个序列的预测误差，作为模型参数更新的依据（通过梯度下降最小化整体损失）。

#### 3. 示例：文本序列 “the students opened their exams”

- 输入序列：\(x^{(1)}=\)“the”，\(x^{(2)}=\)“students”，\(x^{(3)}=\)“opened”，\(x^{(4)}=\)“their”，\(x^{(5)}=\)“exams”
- 真实标签：\(y^{(1)}=\)“students”（\(x^{(1)}\)的下一个单词），\(y^{(2)}=\)“opened”（\(x^{(2)}\)的下一个单词），...，\(y^{(5)}=\)`<eos>`（句子结束标记）
- 预测分布：每个时间步输出\(\hat{y}^{(1)}\)到\(\hat{y}^{(5)}\)，分别对应 “students”“opened” 等单词的概率
- 损失计算：对每个时间步计算交叉熵，再求平均值得到整体损失\(J(\theta)\)。

**专业术语解释**

- **One-Hot 编码（One-Hot Encoding）**：将离散类别（如单词）映射为向量的方式，向量长度等于类别数，仅目标类别对应位置为 1，其余为 0，优点是简单直观，缺点是维度高（词汇表大时向量稀疏）。
- **梯度下降（Gradient Descent）**：机器学习中最小化损失函数的优化算法，通过计算损失函数对参数的梯度（偏导数），沿梯度负方向更新参数，逐步降低损失，找到最优参数值。

### 第 13 页：Train an RNN Sequence Model（训练 RNN 序列模型）- 反向传播与梯度计算

- **核心内容：介绍 RNN 的反向传播（Backpropagation Through Time, BPTT）机制，重点分析重复权重矩阵\(W_h\)的梯度计算**

#### 1. RNN 反向传播的特殊性：时间维度的反向传播

- 传统 DNN 的反向传播仅沿 “层” 反向（从输出层到输入层），而 RNN 的反向传播需沿 “时间步” 反向（从最后一个时间步T到第一个时间步 1），因此称为 “**随时间反向传播（BPTT）**”。
- 核心原因：RNN 的隐藏状态\(h^{(t)}\)依赖前一时刻的\(h^{(t-1)}\)，而\(h^{(t-1)}\)又依赖\(h^{(t-2)}\)，直至\(h^{(0)}\)，因此参数（尤其是\(W_h\)）在每个时间步都被重复使用，梯度需累积所有时间步的贡献。

#### 2. 重复权重矩阵\(W_h\)的梯度计算

- 问题：损失函数\(J^{(t)}(\theta)\)对\(W_h\)的偏导数如何计算？
- 答案：\(W_h\)在每个时间步i（从 1 到t）都参与隐藏状态的计算，因此梯度为每个时间步梯度的总和：\(\frac{\partial J^{(t)}}{\partial W_{h}}=\left.\sum_{i=1}^{t} \frac{\partial J^{(t)}}{\partial W_{h}}\right|_{(i)}\)
  - \(\left.\frac{\partial J^{(t)}}{\partial W_{h}}\right|_{(i)}\)：第i个时间步中，\(W_h\)对损失\(J^{(t)}\)的梯度贡献
  - 含义：\(W_h\)的梯度需考虑其在所有历史时间步对当前损失的影响（如\(t=4\)时，\(W_h\)的梯度需包含\(i=1,2,3,4\)四个时间步的贡献）。

#### 3. 关键疑问：为何重复权重的梯度是各时间步梯度之和？

- 本质：多变量链式法则的应用（下一页面详细解释）。
- 直观理解：\(W_h\)控制隐藏状态的传递，每个时间步的\(W_h\)都影响后续所有时间步的隐藏状态，进而影响最终损失，因此所有时间步的梯度都需被累积，才能全面反映\(W_h\)对损失的影响。

**专业术语解释**

- **随时间反向传播（Backpropagation Through Time, BPTT）**：RNN 特有的反向传播算法，将时间维度视为 “层”，从最后一个时间步反向计算每个参数在所有时间步的梯度，是 RNN 训练的核心技术。
- **重复权重（Repeated Weights）**：RNN 中，\(W_h\)（隐藏层权重）和\(W_x\)（输入层权重）在所有时间步共享，即同一套参数被重复使用，这是 RNN 参数效率高的原因，但也导致梯度计算需累积时间步贡献。

### 第 14 页：Train an RNN Sequence Model（训练 RNN 序列模型）- 多变量链式法则

- **核心内容：通过多变量链式法则，解释 RNN 中重复权重梯度需求和的数学原理**

#### 1. 多变量链式法则的基本形式

- 给定多变量函数\(f(p, q)\)，其中p和q均为单变量函数\(p(t)\)和\(q(t)\)（即p和q依赖t），则f对t的导数为：\(\frac{d}{d t} f(p(t), q(t))=\frac{\partial f}{\partial p} \frac{\partial p}{\partial t}+\frac{\partial f}{\partial q} \frac{\partial q}{\partial t}\)
  - 含义：t通过p和q两个路径影响f，因此导数需包含两个路径的贡献之和。

#### 2. 多变量链式法则在 RNN 中的应用

- RNN 的场景：
  - 最终输出（如损失\(J^{(t)}\)）是 “中间输出”（如隐藏状态\(h^{(1)}, h^{(2)}, ..., h^{(t)}\)）的函数
  - 每个中间输出\(h^{(i)}\)是 “参数\(W_h\)” 的函数（\(h^{(i)}\)依赖\(W_h\)和\(h^{(i-1)}\)）
  - 因此，\(W_h\)通过多个中间输出（\(h^{(1)}\)到\(h^{(t)}\)）影响最终损失\(J^{(t)}\)，符合多变量链式法则的 “多路径影响” 场景。

#### 3. 直观类比：时间步与路径

- 输入t：对应 RNN 的参数\(W_h\)
- 中间变量\(p(t)\)和\(q(t)\)：对应 RNN 的不同时间步的隐藏状态\(h^{(i)}\)
- 输出\(f(p, q)\)：对应 RNN 的最终损失\(J^{(t)}\)
- 结论：\(W_h\)对\(J^{(t)}\)的梯度，需包含其通过每个时间步\(h^{(i)}\)对\(J^{(t)}\)的贡献，即各时间步梯度之和，与多变量链式法则的 “多路径求和” 逻辑一致。

**专业术语解释**

- **多变量链式法则（Multivariable Chain Rule）**：微积分中用于计算多变量复合函数导数的规则，当函数依赖多个中间变量，且中间变量又依赖同一自变量时，导数需包含所有中间变量路径的贡献之和。
- **中间输出（Intermediate Output）**：RNN 中，隐藏状态\(h^{(i)}\)可视为 “中间输出”，它们不直接作为模型的最终预测结果，但参与后续时间步的计算，是连接参数与最终损失的桥梁。

### 第 15 页：Train an RNN Sequence Model（训练 RNN 序列模型）- BPTT 的梯度累积

- **核心内容：结合多变量链式法则，推导 RNN 中\(W_h\)的梯度计算过程，明确梯度累积的数学依据**

#### 1. RNN 中\(W_h\)的多路径影响

- 在 RNN 中，参数\(W_h\)在每个时间步i（1 到t）都被表示为\(W_h|_{(i)}\)（尽管参数值相同，但在不同时间步的作用路径不同），即：
  - \(W_h|_{(1)}\)：影响\(h^{(1)}\)，进而影响\(h^{(2)}, ..., h^{(t)}\)和\(J^{(t)}\)
  - \(W_h|_{(2)}\)：影响\(h^{(2)}\)，进而影响\(h^{(3)}, ..., h^{(t)}\)和\(J^{(t)}\)
  - ...
  - \(W_h|_{(t)}\)：直接影响\(h^{(t)}\)和\(J^{(t)}\)
- 因此，\(W_h\)通过t条独立路径影响\(J^{(t)}\)，符合多变量链式法则的 “多路径” 场景。

#### 2. 梯度计算的推导

- 根据多变量链式法则，\(J^{(t)}\)对\(W_h\)的梯度为每条路径梯度的总和：\(\frac{\partial J^{(t)}}{\partial W_{h}} = \sum_{i=1}^{t} \frac{\partial J^{(t)}}{\partial W_h|_{(i)}} \cdot \frac{\partial W_h|_{(i)}}{\partial W_h}\)
  - 关键简化：由于所有\(W_h|_{(i)}\)都是同一参数\(W_h\)（仅路径不同，参数值相同），因此\(\frac{\partial W_h|_{(i)}}{\partial W_h} = 1\)（对自身的导数为 1）。
  - 最终结果：\(\frac{\partial J^{(t)}}{\partial W_{h}} = \sum_{i=1}^{t} \left. \frac{\partial J^{(t)}}{\partial W_h} \right|_{(i)}\)
  - 与第 13 页的结论一致，验证了重复权重梯度需求和的数学正确性。

#### 3. 示例：时间步\(t=4\)的梯度计算

- \(W_h\)的梯度需包含\(i=1,2,3,4\)四个时间步的贡献：
  - \(i=1\)：\(W_h|_{(1)}\)→\(h^{(1)}\)→\(h^{(2)}\)→\(h^{(3)}\)→\(h^{(4)}\)→\(J^{(4)}\)
  - \(i=2\)：\(W_h|_{(2)}\)→\(h^{(2)}\)→\(h^{(3)}\)→\(h^{(4)}\)→\(J^{(4)}\)
  - \(i=3\)：\(W_h|_{(3)}\)→\(h^{(3)}\)→\(h^{(4)}\)→\(J^{(4)}\)
  - \(i=4\)：\(W_h|_{(4)}\)→\(h^{(4)}\)→\(J^{(4)}\)
- 四条路径的梯度之和，即为\(W_h\)对\(J^{(4)}\)的最终梯度。

**专业术语解释**

- **路径贡献（Path Contribution）**：在复合函数中，自变量通过不同中间变量影响因变量的 “路径” 所带来的导数贡献，多变量链式法则要求将所有路径贡献相加，才能得到完整的导数。
- **参数共享的梯度累积（Gradient Accumulation for Parameter Sharing）**：RNN 中参数共享导致同一参数通过多条路径影响损失，因此梯度需累积所有路径的贡献，这是 RNN 梯度计算与传统 DNN 的核心区别。

### 第 16 页：Train an RNN Sequence Model（训练 RNN 序列模型）- BPTT 的实践与局限

- **核心内容：总结 BPTT 的流程，介绍实践中的 “截断” 策略，以及 RNN 训练的效率问题**

#### 1. BPTT 的完整流程

- 步骤 1：前向传播（Forward Pass）
  - 从时间步 1 到T，计算每个时间步的输入\(x^{(t)}\)、隐藏状态\(h^{(t)}\)和输出分布\(\hat{y}^{(t)}\)，并存储所有隐藏状态（供反向传播使用）。
- 步骤 2：计算最终损失（Loss Calculation）
  - 基于输出分布\(\hat{y}^{(t)}\)和真实标签\(y^{(t)}\)，计算每个时间步的交叉熵损失\(J^{(t)}\)，并求和得到整体损失\(J(\theta)\)。
- 步骤 3：反向传播（Backward Pass）
  - 从时间步T到 1，沿时间反向计算每个参数（\(W_h, W_x, b, U, b_2\)）的梯度，其中\(W_h\)的梯度为各时间步梯度之和（如第 15 页推导）。
- 步骤 4：参数更新（Parameter Update）
  - 使用梯度下降（或其变种，如 Adam、SGD）沿梯度负方向更新所有参数，最小化整体损失\(J(\theta)\)。

#### 2. 实践中的关键优化：截断随时间反向传播（Truncated BPTT）

- 问题：当序列长度T很大（如 1000 个时间步）时，BPTT 需计算 1000 个时间步的梯度，导致：
  - 计算量过大（时间复杂度\(O(T)\)，T越大，耗时越长）
  - 内存占用过高（需存储T个时间步的隐藏状态）。
- 解决方案：**截断 BPTT**
  - 做法：将长序列分割为短片段（如每个片段包含 20 个时间步），对每个片段独立进行 BPTT，梯度仅在片段内累积，不跨片段传递。
  - 示例：序列长度为 100，按 20 个时间步截断为 5 个片段，每个片段从第 1-20、21-40、...、81-100 步分别训练，片段间的隐藏状态作为 “初始状态” 传递（而非重新设为全零）。
  - 优势：大幅降低计算量和内存占用，使长序列训练成为可能；缺点是可能丢失长距离时序依赖（片段外的信息无法通过梯度传递）。

#### 3. 可视化：BPTT 的时间步反向过程

- 图中用圆圈表示时间步（从 1 到t），箭头表示反向传播方向（从t到 1），直观展示了梯度沿时间步累积的过程，以及截断后梯度仅在片段内传递的限制。

**专业术语解释**

- **截断随时间反向传播（Truncated BPTT）**：BPTT 的优化版本，通过将长序列分割为短片段，限制梯度反向传播的时间步范围，平衡训练效率和模型性能，是 RNN 训练长序列的标准方法。
- **片段（Segment）**：截断 BPTT 中，长序列被分割后的短序列单元，每个片段的长度通常设为 20-100 个时间步，具体需根据任务和计算资源调整。

### 第 17 页：Train an RNN Sequence Model（训练 RNN 序列模型）- RNN 的优缺点

- **核心内容：总结 RNN 在序列建模中的优势与缺陷，重点分析梯度消失 / 爆炸问题和计算效率问题**

#### 1. RNN 的优势（Pros）

##### （1）理论上的长距离依赖建模能力

- 原理：隐藏状态\(h^{(t)}\)包含所有前序输入（\(x^{(1)}\)到\(x^{(t)}\)）的信息，因此理论上可利用任意远的历史信息（如第 100 个时间步的输入可影响第 200 个时间步的输出）。
- 应用价值：适用于需要 “长期记忆” 的任务，如文本理解（理解上下文）、视频分析（分析动作序列）。

##### （2）可变长度输入处理

- 无需固定输入序列长度，可直接处理任意长度的文本、视频、语音等数据（传统 MLP 需固定输入尺寸，无法满足此需求）。

##### （3）模型规模与序列长度无关

- 模型参数数量（\(W_h, W_x, b, U, b_2\)）固定，不随输入序列长度增加而增长（如处理 10 个单词和 1000 个单词的序列，模型规模相同），参数效率高。

#### 2. RNN 的缺陷（Cons）

##### （1）实践中的梯度消失 / 爆炸问题

- 核心问题：当序列过长（如超过 50 个时间步）时，梯度在反向传播过程中会急剧变小（梯度消失）或变大（梯度爆炸），导致模型无法学习长距离依赖。
- 具体表现：远时间步（如第 1 个时间步）的梯度贡献几乎为零，模型仅能学习短距离依赖（如第t个时间步仅受第\(t-1\)和\(t-2\)个时间步的影响）。
- 示例：文本 “当她试图打印车票时，她发现打印机没墨了。她去文具店买了更多墨水。墨水很贵。将墨水装入打印机后，她终于打印出了她的_____。”（空格处应填 “车票”），RNN 无法学习 “车票”（第 7 个时间步）与空格处（最后一个时间步）的依赖，因为梯度消失导致远时间步信息无法传递。

##### （2）循环计算效率低

- 原理：RNN 的前向传播和反向传播均需沿时间步顺序执行（第t个时间步的计算依赖第\(t-1\)个时间步的结果），无法并行计算（传统 CNN 的卷积操作可并行）。
- 影响：序列越长，计算时间越长，难以满足实时性需求（如语音识别、实时翻译）。

#### 3. 示例：RNN 处理文本序列的局限

- 输入序列：“the students opened their exams”（5 个单词）
- 隐藏状态：\(h^{(1)}\)（“the”）→\(h^{(2)}\)（“the+students”）→\(h^{(3)}\)（“the+students+opened”）→\(h^{(4)}\)（“the+students+opened+their”）→\(h^{(5)}\)（“the+students+opened+their+exams”）
- 问题：若序列延长至 100 个单词，\(h^{(100)}\)中 “the” 的信息会因梯度消失而几乎无法传递到输出层，导致模型无法利用早期信息。

**专业术语解释**

- **梯度消失（Vanishing Gradients）**：在深度神经网络（包括 RNN）的反向传播中，梯度随着传播层数（或时间步）的增加而急剧减小（趋近于零），导致早期层（或远时间步）的参数无法有效更新，模型无法学习长距离依赖。
- **梯度爆炸（Exploding Gradients）**：与梯度消失相反，梯度随着传播层数（或时间步）的增加而急剧增大（趋近于无穷大），导致参数更新幅度过大，模型训练不稳定（如参数值溢出）。
- **并行计算（Parallel Computation）**：同时处理多个计算任务的方式，可大幅提升效率，CNN 的卷积操作因各局部区域独立而支持并行，RNN 因时间步依赖而无法并行。

### 第 18 页：Problems with RNN: Vanishing Gradients（RNN 的问题：梯度消失）

- **核心内容：通过数学公式和直观分析，解释 RNN 梯度消失的原理**

#### 1. 梯度消失的数学根源：隐藏状态的梯度链式乘积

- 以损失\(J^{(4)}\)对第一个时间步隐藏状态\(h^{(1)}\)的偏导数为例，推导梯度消失的过程：
  - 首先，\(h^{(2)}\)依赖\(h^{(1)}\)，\(h^{(3)}\)依赖\(h^{(2)}\)，\(h^{(4)}\)依赖\(h^{(3)}\)，\(J^{(4)}\)依赖\(h^{(4)}\)，因此根据链式法则：\(\frac{\partial J^{(4)}}{\partial h^{(1)}} = \frac{\partial h^{(2)}}{\partial h^{(1)}} \times \frac{\partial h^{(3)}}{\partial h^{(2)}} \times \frac{\partial h^{(4)}}{\partial h^{(3)}} \times \frac{\partial J^{(4)}}{\partial h^{(4)}}\)
  - 关键组件：\(\frac{\partial h^{(t)}}{\partial h^{(t-1)}}\)（当前隐藏状态对前一隐藏状态的导数），其值由 RNN 的激活函数和权重矩阵决定。

#### 2. \(\frac{\partial h^{(t)}}{\partial h^{(t-1)}}\)的计算

- 回顾 RNN 隐藏状态的计算公式：\(h^{(t)} = \sigma\left(W_h h^{(t-1)} + W_x x^{(t)} + b\right)\)
  - 对\(h^{(t-1)}\)求偏导（假设激活函数为 Sigmoid）：\(\frac{\partial h^{(t)}}{\partial h^{(t-1)}} = \sigma'\left(W_h h^{(t-1)} + W_x x^{(t)} + b\right) \times W_h\)
  - \(\sigma'(\cdot)\)：Sigmoid 函数的导数，其最大值为 0.25（当输入为 0 时），且在输入绝对值较大时趋近于 0。
  - 因此，\(\frac{\partial h^{(t)}}{\partial h^{(t-1)}}\)的每个元素值通常小于 1（0.25 乘以\(W_h\)的元素，而\(W_h\)初始化时通常为小数以避免梯度爆炸）。

#### 3. 梯度消失的直观分析

- 公式\(\frac{\partial J^{(4)}}{\partial h^{(1)}}\)是三个导数项的乘积（\(\frac{\partial h^{(2)}}{\partial h^{(1)}}\)、\(\frac{\partial h^{(3)}}{\partial h^{(2)}}\)、\(\frac{\partial h^{(4)}}{\partial h^{(3)}}\)），每个项的元素值都小于 1。
- 当序列长度增加时（如\(t=100\)），乘积项的数量增加到 99 个，每个项小于 1，导致最终梯度值急剧减小（趋近于零），即 “梯度消失”。
- 示例：若每个\(\frac{\partial h^{(t)}}{\partial h^{(t-1)}}\)的元素值为 0.5，那么 10 个时间步后，梯度乘积为\(0.5^{10} \approx 0.00097\)，几乎可忽略；20 个时间步后，乘积为\(0.5^{20} \approx 9.5 \times 10^{-7}\)，完全消失。

#### 4. 可视化：梯度传递的衰减过程

- 图中用 “W” 表示\(\frac{\partial h^{(t)}}{\partial h^{(t-1)}}\)的影响，箭头表示梯度传递方向，随着时间步增加，梯度信号（圆圈大小）逐渐变小，直观展示了梯度消失的过程。

**专业术语解释**

- **Sigmoid 函数导数（Sigmoid Derivative）**：Sigmoid 函数\(\sigma(x) = \frac{1}{1+e^{-x}}\)的导数为\(\sigma'(x) = \sigma(x)(1-\sigma(x))\)，其取值范围为 (0, 0.25]，最大值在\(x=0\)时取得，这是导致 RNN 梯度消失的重要原因之一。
- **权重初始化（Weight Initialization）**：神经网络训练前对参数（如\(W_h\)）的初始值设定，RNN 中通常将\(W_h\)初始化为小数（如服从均值为 0、方差为\(\frac{1}{hidden\_size}\)的正态分布），以避免初始梯度过大导致爆炸，但也加剧了梯度消失。

### 第 19 页：Problems with RNN: Vanishing Gradients（RNN 的问题：梯度消失）- 直观理解

- **核心内容：通过更直观的案例和图形，解释梯度消失的具体表现和影响**

#### 1. 核心问题：导数项过小时的乘积衰减

- 假设\(\frac{\partial h^{(2)}}{\partial h^{(1)}}\)、\(\frac{\partial h^{(3)}}{\partial h^{(2)}}\)、\(\frac{\partial h^{(4)}}{\partial h^{(3)}}\)的元素值均为 “小值”（如 0.1、0.2），那么它们的乘积会急剧衰减：
  - 示例：0.2 × 0.2 × 0.2 = 0.008（三个小值相乘后仅为 0.008，即原数值的 1/25）
  - 若序列长度为 10，乘积为\(0.2^{10} = 1.024 \times 10^{-7}\)，几乎为零，梯度完全消失。

#### 2. 图形化展示梯度传递

- 图中用节点表示隐藏状态\(h^{(1)}\)、\(h^{(2)}\)、\(h^{(3)}\)、\(h^{(4)}\)和损失\(J^{(4)}\)，用箭头表示梯度传递方向，箭头旁标注导数项\(\frac{\partial h^{(2)}}{\partial h^{(1)}}\)、\(\frac{\partial h^{(3)}}{\partial h^{(2)}}\)等。
- 直观观察：随着梯度从\(J^{(4)}\)向\(h^{(1)}\)传递，箭头的 “粗细” 逐渐变细（代表梯度值减小），最终传递到\(h^{(1)}\)时，梯度已非常微弱（箭头几乎不可见），即 “梯度消失”。

#### 3. 梯度消失的本质：远时间步信息的 “记忆衰减”

- RNN 的隐藏状态相当于 “短期记忆”，但这种记忆会随着时间步的增加而快速衰减：
  - 近时间步（如\(h^{(3)}\)、\(h^{(4)}\)）的信息对当前损失影响较大（梯度大）
  - 远时间步（如\(h^{(1)}\)、\(h^{(2)}\)）的信息对当前损失影响极小（梯度小）
- 类比：人类的短期记忆只能记住最近发生的事情，很久之前的事情会逐渐遗忘，RNN 的 “记忆” 也存在类似的衰减问题，且衰减速度更快。

**专业术语解释**

- **记忆衰减（Memory Decay）**：RNN 中，远时间步的信息在隐藏状态中逐渐被 “稀释” 的现象，导致模型无法长期保留历史信息，是梯度消失的直观表现，也是 RNN 处理长序列的核心障碍。
- **短期记忆（Short-Term Memory）**：RNN 的隐藏状态仅能短期保留信息（通常不超过 50 个时间步），无法像人类长期记忆那样存储和调用远距离信息，这一缺陷直接导致 RNN 在长文本、长视频等任务中性能不佳。

### 第 20 页：Problems with RNN: Vanishing Gradients（RNN 的问题：梯度消失）- 危害与影响

- **核心内容：分析梯度消失对 RNN 模型性能的具体危害，解释为何模型无法学习长距离依赖**

#### 1. 梯度消失的核心危害：远时间步信息丢失

- 梯度信号的大小直接决定了参数更新的幅度：梯度越大，参数更新越显著；梯度越小，参数更新越微弱。
- 梯度消失导致：
  - 远时间步（如\(h^{(1)}\)）对损失\(J^{(t)}\)的梯度贡献几乎为零，对应的参数（如\(W_h\)中控制远时间步信息传递的部分）无法有效更新。
  - 模型仅能 “关注” 近时间步的信息（如\(h^{(t-1)}\)、\(h^{(t-2)}\)），完全 “遗忘” 远时间步的信息（如\(h^{(1)}\)、\(h^{(2)}\)），即无法学习长距离依赖。

#### 2. 直观案例：远时间步与近时间步的梯度对比

- 图中展示损失\(J^{(2)}\)和\(J^{(4)}\)对隐藏状态\(h^{(1)}\)、\(h^{(3)}\)、\(h^{(4)}\)的梯度影响：
  - 对\(J^{(2)}\)：\(h^{(1)}\)是近时间步，梯度大（参数更新显著）；\(h^{(3)}\)、\(h^{(4)}\)尚未出现，无影响。
  - 对\(J^{(4)}\)：\(h^{(3)}\)、\(h^{(4)}\)是近时间步，梯度大；\(h^{(1)}\)是远时间步，梯度小（几乎为零，参数更新微弱）。
- 结论：模型权重仅能根据近时间步的信息更新，无法利用远时间步的信息，导致长距离依赖建模失败。

#### 3. 模型性能的具体表现

- 在语言建模、机器翻译等任务中，模型会出现 “语法错误” 或 “语义不连贯”：
  - 示例：文本 “我今天早上吃了面包，中午喝了牛奶，晚上准备吃_____。”（空格处应填 “米饭”），RNN 无法关联 “早上 / 中午” 的饮食场景与 “晚上” 的填空，可能错误预测为 “苹果”“书” 等无关词汇，因为 “面包”“牛奶” 的信息因梯度消失而无法传递到最后一个时间步。
- 在视频动作预测中，模型无法关联早期帧的动作起始与后期帧的动作结束，导致动作分类错误（如将 “开门→走进房间” 预测为 “走进房间”，忽略 “开门” 的起始动作）。

**专业术语解释**

- **参数更新幅度（Parameter Update Magnitude）**：在梯度下降中，参数的更新量为 “学习率 × 梯度”，梯度消失导致更新幅度趋近于零，参数几乎不变化，模型无法学习新特征。
- **长距离依赖建模失败（Failure to Model Long-Distance Dependencies）**：RNN 的核心缺陷，指模型无法捕捉序列中距离较远的元素之间的依赖关系，导致在长序列任务中性能大幅下降，这也是后续 LSTM、GRU 等模型提出的根本原因。

### 第 21 页：Problems with RNN: Vanishing Gradients（RNN 的问题：梯度消失）- 实例分析

- **核心内容：通过具体的语言建模实例，展示梯度消失对 RNN 性能的影响，明确长距离依赖建模的重要性**

#### 1. 实例任务：语言建模中的长距离依赖预测

- 输入文本：

  

  “When she tried to print her tickets, she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer, she finally printed her

   

  . [tickets]”

  （中文翻译：“当她试图打印车票时，她发现打印机没墨了。她去文具店买了更多墨水。墨水很贵。将墨水装入打印机后，她终于打印出了她的

  。[答案：车票]”）

#### 2. 任务核心需求：学习长距离依赖

- 填空处的正确答案 “tickets”（车票）依赖于文本中第 7 个单词 “tickets”（“print her tickets”），两者之间间隔了多个句子和单词（约 30 个时间步），属于典型的 “长距离依赖”。
- 模型需将早期的 “tickets” 信息存储在隐藏状态中，并传递到最后一个时间步，才能正确预测填空答案。

#### 3. 梯度消失的影响：模型无法学习长距离依赖

- 由于梯度消失，第 7 个时间步 “tickets” 对最后一个时间步损失的梯度贡献几乎为零：
  - 隐藏状态中 “tickets” 的信息随着时间步增加而逐渐衰减，传递到最后一个时间步时已几乎为零。
  - 模型参数无法根据 “tickets” 的信息更新，导致无法建立 “tickets” 与填空处的关联。
- 模型预测结果：无法正确输出 “tickets”，可能预测为 “toner”（墨水）、“printer”（打印机）等近期出现的单词，或完全无关的单词（如 “book”“pen”）。

#### 4. 测试时的缺陷：长距离依赖预测失败

- 训练阶段：模型因梯度消失无法学习长距离依赖，仅能学习短距离依赖（如 “printer” 与 “toner” 的关联）。
- 测试阶段：当遇到需要长距离依赖的句子时，模型无法正确预测，性能显著下降。
- 结论：梯度消失导致 RNN 的 “记忆” 能力有限，仅能处理短序列，无法满足长序列任务的需求。

**专业术语解释**

- **语言建模（Language Modeling）**：预测文本序列中下一个单词的概率分布的任务，是自然语言处理的基础任务（如语音识别、机器翻译、文本生成均依赖语言建模），长距离依赖建模是语言建模性能的关键。
- **测试阶段性能（Test-Time Performance）**：模型在未见过的测试数据上的表现，若训练阶段未学习到关键特征（如长距离依赖），测试阶段性能会大幅下降，这是梯度消失导致的直接后果。

### 第 22 页：Long Short-Term Memory RNNs (LSTMs)（长短期记忆网络）- 原理与结构

- **核心内容：介绍 LSTM 的提出背景、核心结构（细胞状态与门控机制），解决 RNN 的梯度消失问题**

#### 1. LSTM 的基本信息

- **提出者**：Hochreiter 和 Schmidhuber 于 1997 年提出
- **核心目标**：解决 RNN 的梯度消失问题，实现长距离依赖建模
- **关键创新**：引入 “细胞状态（Cell State）” 和 “门控机制（Gates）”，使信息能在序列中稳定传递，不被轻易遗忘或干扰。

#### 2. LSTM 的核心组件

##### （1）细胞状态（Cell State, \(c^{(t)}\)）

- 定义：LSTM 中用于长期存储信息的 “通道”，类似一条 “传送带”，直接贯穿整个序列，仅通过门控机制进行少量线性修改（而非 RNN 中隐藏状态的非线性更新）。
- 作用：长期保留历史信息（如第 7 个时间步的 “tickets” 信息），避免因非线性激活导致的梯度消失。
- 直观类比：细胞状态相当于 “长期记忆”，隐藏状态\(h^{(t)}\)相当于 “短期记忆”，长期记忆稳定传递，短期记忆根据当前输入动态更新。

##### （2）隐藏状态（Hidden State, \(h^{(t)}\)）

- 定义：LSTM 的输出，由细胞状态和输出门共同决定，包含当前时间步的关键信息，用于预测输出\(\hat{y}^{(t)}\)。
- 与 RNN 的区别：RNN 的隐藏状态同时承担 “短期记忆” 和 “信息传递” 功能，而 LSTM 的隐藏状态仅承担 “短期记忆” 功能，“信息传递” 由细胞状态负责，分工更明确。

##### （3）门控机制（Gates）

- 定义：控制信息在细胞状态中 “遗忘”“写入”“读取” 的模块，共 3 个门：
  - **遗忘门（Forget Gate）**：决定从细胞状态中删除哪些信息
  - **输入门（Input Gate）**：决定向细胞状态中写入哪些新信息
  - **输出门（Output Gate）**：决定从细胞状态中读取哪些信息到隐藏状态
- 门控原理：每个门通过 Sigmoid 激活函数输出 0-1 之间的值，0 表示 “完全关闭”（不允许信息通过），1 表示 “完全打开”（允许所有信息通过），实现对信息的精细控制。

#### 3. LSTM 的核心优势：信息的稳定传递

- 细胞状态的更新主要通过 “加法” 和 “元素乘法”（线性操作）实现，避免了 RNN 中隐藏状态的非线性更新（如 Sigmoid 激活），从而减少梯度消失的风险：
  - 线性操作的梯度更易传递（导数为常数，不随时间步衰减）
  - 门控机制可动态调整信息的保留与丢弃，确保有用的长距离信息被保留，无用信息被遗忘。

#### 4. 参考文献

- “Long short-term memory”, Hochreiter and Schmidhuber, 1997.（LSTM 的奠基性论文）
- “Learning to Forget: Continual Prediction with LSTM”, Gers, Schmidhuber, and Cummins, 2000.（改进 LSTM 的遗忘门机制，提升长序列处理能力）

**专业术语解释**

- **细胞状态（Cell State）**：LSTM 的核心组件，用于长期存储信息的线性通道，通过门控机制控制信息的增减，是解决梯度消失的关键，其更新过程以线性操作为主，梯度传递稳定。
- **门控机制（Gating Mechanism）**：LSTM 中控制信息流动的模块，通过 Sigmoid 激活函数实现 “开关” 功能，可动态调整信息的保留、写入和读取，使模型能自适应地处理不同长度的序列依赖。
- **线性操作（Linear Operation）**：数学上的一次函数操作（如加法、乘法），其导数为常数（如加法的导数为 1，乘法的导数为另一个因子），梯度在传递过程中不会衰减，这是 LSTM 梯度稳定的核心原因。

### 第 23 页：Long Short-Term Memory RNNs (LSTMs)（长短期记忆网络）- 细胞状态与门控细节

- **核心内容：详细拆解 LSTM 的细胞状态特性和门控机制的工作原理，包括 Sigmoid 和 Tanh 激活函数的作用**

#### 1. 细胞状态（Cell State）的核心特性

- **线性传递**：细胞状态\(c^{(t)}\)沿序列直接传递（类似 “传送带”），仅通过 “遗忘门” 和 “输入门” 进行线性修改（元素乘法和加法），无复杂的非线性变换。
- 数学表示：\(c^{(t)} = f^{(t)} \cdot c^{(t-1)} + i^{(t)} \cdot \tilde{c}^{(t)}\)（后续页面详细推导）
- 优势：线性操作的梯度传递稳定（导数为常数），避免了 RNN 中非线性激活导致的梯度消失，使长距离信息能稳定传递。
- 直观类比：细胞状态像一条 “信息高速公路”，信息在上面快速、稳定地传递，门控机制像 “收费站”，控制信息的进出。

#### 2. 门控机制（Gates）的工作原理

##### （1）门的通用结构

- 每个门（遗忘门、输入门、输出门）的输入均为 “前一隐藏状态\(h^{(t-1)}\) + 当前输入\(x^{(t)}\)”，通过线性变换和 Sigmoid 激活函数输出 0-1 之间的值：\(\text{Gate Value} = \sigma\left(W_{\text{gate}} h^{(t-1)} + U_{\text{gate}} x^{(t)} + b_{\text{gate}}\right)\)
  - \(W_{\text{gate}}\)、\(U_{\text{gate}}\)：门的权重矩阵
  - \(b_{\text{gate}}\)：门的偏置项
  - \(\sigma(\cdot)\)：Sigmoid 激活函数，确保输出在 0-1 之间，实现 “开关” 控制。

##### （2）Sigmoid 激活函数的作用

- 函数公式：\(\sigma(x) = \frac{1}{1 + e^{-x}}\)
- 输出范围：(0, 1)
- 功能：
  - 输出 = 1：门 “完全打开”，允许所有信息通过（如遗忘门输出 1 表示不遗忘任何历史信息）
  - 输出 = 0：门 “完全关闭”，禁止所有信息通过（如遗忘门输出 0 表示完全遗忘历史信息）
  - 输出介于 0-1 之间：门 “部分打开”，允许部分信息通过（如遗忘门输出 0.5 表示遗忘 50% 的历史信息）。

##### （3）Tanh 激活函数的作用

- 函数公式：\(\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\)
- 输出范围：(-1, 1)
- 功能：用于生成 “新细胞内容\(\tilde{c}^{(t)}\)”（后续页面介绍），将新信息归一化到 - 1 到 1 之间，避免数值过大导致的梯度爆炸，同时增强模型对正负信息的表达能力（如 “积极” 与 “消极” 语义）。

#### 3. 门控与细胞状态的交互

- 遗忘门：通过元素乘法（Hadamard Product，用 “・” 表示）控制细胞状态中历史信息的保留比例（\(f^{(t)} \cdot c^{(t-1)}\)）。
- 输入门：通过元素乘法控制新信息的写入比例（\(i^{(t)} \cdot \tilde{c}^{(t)}\)）。
- 输出门：通过元素乘法控制细胞状态中信息的读取比例（\(o^{(t)} \cdot \tanh(c^{(t)})\)）。
- 核心：元素乘法确保门控值能逐元素地控制信息的流动，实现对细胞状态的精细调整。

**专业术语解释**

- **元素乘法（Hadamard Product）**：两个维度相同的矩阵或向量之间的逐元素乘法，结果矩阵 / 向量的每个元素为对应位置元素的乘积，LSTM 中用于门控值对信息的逐元素控制（如遗忘门值 0.5 表示每个元素都保留 50%）。
- **Tanh 激活函数（Tanh Activation Function）**：双曲正切函数，将输入映射到 (-1, 1) 区间，相比 Sigmoid 函数，其输出中心在 0，梯度更大（在输入接近 0 时），能更好地处理正负信息，常用于 LSTM 的新细胞内容生成。
- **归一化（Normalization）**：将数据映射到特定范围（如 Tanh 的 (-1,1)）的操作，避免数据值过大导致模型训练不稳定（如梯度爆炸），同时使模型对输入的敏感性更一致。

### 第 24 页：Long Short-Term Memory RNNs (LSTMs)（长短期记忆网络）- 门控计算步骤 1-2

- **核心内容：拆解 LSTM 的前两个关键步骤 —— 遗忘门（决定遗忘信息）和输入门（决定写入新信息）的计算过程**

#### 步骤 1：遗忘门（Forget Gate）—— 决定遗忘哪些历史信息

- **目标**：根据当前输入和前一隐藏状态，判断细胞状态中哪些历史信息需要被遗忘（如过时的单词、无关的动作）。
- **计算过程**：
  1. 输入：前一隐藏状态\(h^{(t-1)}\)和当前输入\(x^{(t)}\)
  2. 线性变换：将\(h^{(t-1)}\)和\(x^{(t)}\)通过权重矩阵\(W_f\)、\(U_f\)和偏置\(b_f\)进行线性组合：\(W_f h^{(t-1)} + U_f x^{(t)} + b_f\)
  3. Sigmoid 激活：将线性组合的结果通过 Sigmoid 函数，得到遗忘门值\(f^{(t)}\)：\(f^{(t)} = \sigma\left(W_f h^{(t-1)} + U_f x^{(t)} + b_f\right)\)
- **输出含义**：\(f^{(t)}\)是与细胞状态\(c^{(t-1)}\)维度相同的向量，每个元素值在 0-1 之间：
  - 元素 = 1：完全保留对应位置的历史信息（不遗忘）
  - 元素 = 0：完全遗忘对应位置的历史信息
  - 元素介于 0-1 之间：部分保留历史信息（如 0.3 表示保留 30%，遗忘 70%）。
- **示例**：在文本 “她打印车票，然后买墨水，最后打印_____” 中，当处理到 “打印” 时，遗忘门会保留 “车票” 的信息（\(f^{(t)}\)对应 “车票” 的元素 = 1），遗忘 “墨水” 的无关信息（\(f^{(t)}\)对应 “墨水” 的元素 = 0.1）。

#### 步骤 2：输入门（Input Gate）与新细胞内容（New Cell Content）—— 决定写入哪些新信息

- **目标**：根据当前输入和前一隐藏状态，判断哪些新信息需要写入细胞状态（如当前单词、当前动作）。
- **计算过程**：分为两步 —— 计算输入门值和新细胞内容。

##### （1）输入门值计算

1. 输入：前一隐藏状态\(h^{(t-1)}\)和当前输入\(x^{(t)}\)
2. 线性变换：通过权重矩阵\(W_i\)、\(U_i\)和偏置\(b_i\)进行线性组合：\(W_i h^{(t-1)} + U_i x^{(t)} + b_i\)
3. Sigmoid 激活：得到输入门值\(i^{(t)}\)：\(i^{(t)} = \sigma\left(W_i h^{(t-1)} + U_i x^{(t)} + b_i\right)\)

- 输出含义：\(i^{(t)}\)的元素值在 0-1 之间，控制新信息的写入比例（1 = 完全写入，0 = 完全不写入）。

##### （2）新细胞内容计算

1. 输入：前一隐藏状态\(h^{(t-1)}\)和当前输入\(x^{(t)}\)
2. 线性变换：通过权重矩阵\(W_c\)、\(U_c\)和偏置\(b_c\)进行线性组合：\(W_c h^{(t-1)} + U_c x^{(t)} + b_c\)
3. Tanh 激活：将结果映射到 (-1, 1) 之间，得到新细胞内容\(\tilde{c}^{(t)}\)：\(\tilde{c}^{(t)} = \tanh\left(W_c h^{(t-1)} + U_c x^{(t)} + b_c\right)\)

- 输出含义：\(\tilde{c}^{(t)}\)包含当前时间步的新信息（如当前单词的语义、当前动作的特征），Tanh 激活确保信息值在合理范围，避免数值爆炸。
- **示例**：在文本 “最后打印_____” 中，当前输入为 “打印”，输入门\(i^{(t)}\)会将 “打印” 与 “车票” 的关联信息写入细胞状态（\(i^{(t)}\)对应 “车票”

#### 步骤 2：输入门与新细胞内容的协同作用

- **输入门与新细胞内容的结合**：

  新信息写入细胞状态需同时满足两个条件：输入门允许写入（\(i^{(t)}\)接近 1）且新细胞内容包含有效信息（

  \(\tilde{c}^{(t)}\)的对应元素非零）。

  例如，在文本 “After installing the toner into the printer, she finally printed her _____” 中，当处理到 “printed” 时：

  - 输入门\(i^{(t)}\)对 “tickets” 相关特征的输出接近 1（允许写入 “打印车票” 的关联信息）；
  - 新细胞内容\(\tilde{c}^{(t)}\)包含 “printed” 的动作特征，与历史 “tickets” 信息结合，为后续预测 “tickets” 奠定基础。

### 页码 25：Long Short-Term Memory RNNs (LSTMs)（长短期记忆网络）- 门控计算步骤 3-4

#### 步骤 3：更新细胞状态（Update Cell State）

- **目标**：整合历史信息（经遗忘门筛选后）和新信息（经输入门筛选后），生成当前时刻的细胞状态\(c^{(t)}\)，实现信息的 “筛选式传递”。

- **计算过程**：

  通过元素乘法（遗忘门控制历史信息保留）和元素加法（输入门控制新信息写入），公式如下：

  \(c^{(t)} = f^{(t)} \cdot c^{(t-1)} + i^{(t)} \cdot \tilde{c}^{(t)}\)

- **符号含义**：

  - \(f^{(t)} \cdot c^{(t-1)}\)：历史细胞状态\(c^{(t-1)}\)经遗忘门\(f^{(t)}\)筛选后的结果（保留有用历史信息，丢弃无用信息）；
  - \(i^{(t)} \cdot \tilde{c}^{(t)}\)：新细胞内容\(\tilde{c}^{(t)}\)经输入门\(i^{(t)}\)筛选后的结果（写入有用新信息，过滤噪声）；
  - “+”：线性加法操作，无非线性变换，确保梯度稳定传递（避免 RNN 中的梯度消失）。

- **示例**：在 “打印车票” 的文本序列中：

  - 历史细胞状态\(c^{(t-1)}\)包含 “tickets” 的信息，遗忘门\(f^{(t)}=1\)（完全保留）；
  - 新细胞内容\(\tilde{c}^{(t)}\)包含 “printed” 的动作信息，输入门\(i^{(t)}=0.8\)（保留 80% 新信息）；
  - 最终\(c^{(t)} = 1 \cdot c^{(t-1)} + 0.8 \cdot \tilde{c}^{(t)}\)，同时保留 “tickets” 和 “printed” 的关联信息。

#### 步骤 4：输出门（Output Gate）与隐藏状态更新

- **目标**：根据当前细胞状态和输入信息，决定哪些信息传递到隐藏状态\(h^{(t)}\)，进而用于输出预测\(\hat{y}^{(t)}\)。
- **计算过程**：分为两步 —— 计算输出门值和更新隐藏状态。

##### （1）输出门值计算

1. 输入：前一隐藏状态\(h^{(t-1)}\)和当前输入\(x^{(t)}\)；
2. 线性变换：通过权重矩阵\(W_o\)、\(U_o\)和偏置\(b_o\)进行线性组合：\(W_o h^{(t-1)} + U_o x^{(t)} + b_o\)
3. Sigmoid 激活：得到输出门值\(o^{(t)}\)：\(o^{(t)} = \sigma\left(W_o h^{(t-1)} + U_o x^{(t)} + b_o\right)\)

- 输出含义：\(o^{(t)}\)的元素值在 0-1 之间，控制细胞状态信息的读取比例（1 = 完全读取，0 = 完全不读取）。

##### （2）隐藏状态更新

1. 细胞状态归一化：对当前细胞状态\(c^{(t)}\)应用 Tanh 激活，将值映射到 (-1, 1) 之间，避免数值过大导致梯度爆炸：\(\tanh\left(c^{(t)}\right)\)
2. 输出门筛选：通过输出门\(o^{(t)}\)的元素乘法，筛选细胞状态中的有效信息，得到当前隐藏状态\(h^{(t)}\)：\(h^{(t)} = o^{(t)} \cdot \tanh\left(c^{(t)}\right)\)

- 输出含义：\(h^{(t)}\)包含当前时间步的关键信息，既用于预测输出\(\hat{y}^{(t)}\)，也作为下一时刻的历史信息传递到\(h^{(t+1)}\)。
- **示例**：在 “打印车票” 的最终预测步骤中：
  - 输出门\(o^{(t)}\)对 “tickets” 相关特征的输出接近 1（允许读取细胞状态中 “tickets” 的信息）；
  - 细胞状态\(c^{(t)}\)经 Tanh 归一化后，与\(o^{(t)}\)相乘得到\(h^{(t)}\)，\(h^{(t)}\)包含 “tickets” 的关键信息，最终输出预测 “tickets”。

**专业术语解释**

- **细胞状态更新（Cell State Update）**：LSTM 的核心操作，通过 “遗忘 - 写入” 两步实现信息的筛选与传递，线性加法确保梯度稳定，是解决 RNN 梯度消失的关键。
- **隐藏状态输出（Hidden State Output）**：LSTM 的对外输出，既包含当前时间步的关键信息，也承载历史信息传递功能，是连接 LSTM 与后续输出层（如 Softmax）的桥梁。

### 页码 26：Long Short-Term Memory RNNs (LSTMs)（长短期记忆网络）- 核心逻辑总结

- **核心框架：细胞状态与门控的协同工作流**

  

  LSTM 通过 “细胞状态长期存储 + 门控动态调控” 的机制，实现长距离依赖建模，完整工作流可概括为：

  1. **遗忘**：遗忘门筛选历史细胞状态中的无用信息，保留有用信息；
  2. **写入**：输入门筛选新细胞内容中的有效信息，写入当前细胞状态；
  3. **更新**：线性组合筛选后的历史信息与新信息，生成当前细胞状态；
  4. **读取**：输出门筛选细胞状态中的关键信息，更新隐藏状态并用于预测。

#### 各组件的核心功能

| 组件（Component）                 | 核心功能（Core Function）                                  | 数学操作（Mathematical Operation）               |
| --------------------------------- | ---------------------------------------------------------- | ------------------------------------------------ |
| 遗忘门（Forget Gate）             | 控制历史细胞状态的信息保留比例，避免无用信息累积           | Sigmoid 激活（0-1 输出）+ 元素乘法（逐元素筛选） |
| 输入门（Input Gate）              | 控制新细胞内容的信息写入比例，过滤新信息中的噪声           | Sigmoid 激活（0-1 输出）+ 元素乘法（逐元素筛选） |
| 新细胞内容（\(\tilde{c}^{(t)}\)） | 生成当前时间步的新信息，包含输入与历史隐藏状态的融合特征   | 线性组合 + Tanh 激活（-1-1 归一化）              |
| 细胞状态（\(c^{(t)}\)）           | 长期存储信息的线性通道，稳定传递长距离依赖，避免梯度消失   | 元素乘法（遗忘） + 线性加法（写入）              |
| 输出门（Output Gate）             | 控制细胞状态信息的读取比例，决定哪些信息传递到隐藏状态     | Sigmoid 激活（0-1 输出）+ 元素乘法（逐元素筛选） |
| 隐藏状态（\(h^{(t)}\)）           | 短期记忆载体，传递当前关键信息到下一时刻，同时用于输出预测 | Tanh 归一化（细胞状态） + 元素乘法（输出门筛选） |

#### LSTM 的核心优势：长距离依赖建模能力

- **梯度稳定传递**：细胞状态的更新以线性操作为主（加法、乘法），无 RNN 中隐藏状态的非线性激活（如 Sigmoid），梯度在反向传播时不易衰减，可传递到早期时间步（如第 1 个时间步的信息可传递到第 100 个时间步）。
- **自适应信息调控**：门控机制可根据输入动态调整信息的 “遗忘 - 写入 - 读取” 比例，例如：
  - 在长文本中，对 “主题词”（如 “tickets”）的遗忘门输出接近 1（长期保留），对 “修饰词”（如 “overpriced”）的遗忘门输出接近 0（短期遗忘）；
  - 在视频动作预测中，对 “动作起始帧”（如 “开门”）的输入门输出接近 1（写入关键动作信息），对 “背景帧” 的输入门输出接近 0（过滤无关信息）。

**专业术语解释**

- **自适应调控（Adaptive Regulation）**：LSTM 通过门控机制实现对信息流动的动态调整，无需人工干预，可根据输入数据的特征自动优化信息的保留与丢弃，适配不同长度、不同类型的序列数据（如文本、视频、语音）。
- **长距离依赖建模（Long-Distance Dependency Modeling）**：LSTM 的核心能力，指模型可捕捉序列中距离较远的元素之间的依赖关系（如文本中第 1 个单词与第 100 个单词的关联），解决了 RNN 的核心缺陷，是 LSTM 在长序列任务中性能优异的根本原因。

### 页码 27：LSTMs: Real-world Success（LSTM 的实际应用成果）

- **LSTM 的技术突破期（2013-2015）**

  

  2013 年起，LSTM 在多个领域取得突破性成果，成为序列建模的主流方法，核心原因是其解决了 RNN 的梯度消失问题，能有效处理长序列数据。

#### 1. 关键应用场景与成果

- **手写识别（Handwriting Recognition）**：LSTM 可处理变长的手写文本序列（如英文单词、中文汉字），通过学习笔画的时序依赖，实现高精度识别，识别准确率远超传统方法（如隐马尔可夫模型 HMM）。
- **语音识别（Speech Recognition）**：在语音转文字任务中，LSTM 可建模语音信号的时间动态特征（如音节、语调的连续变化），大幅降低识别错误率，成为工业界语音识别系统的核心组件（如 Google Speech、苹果 Siri 的早期版本）。
- **机器翻译（Machine Translation）**：LSTM-based 神经机器翻译（NMT）模型（如 Encoder-Decoder 架构）实现端到端翻译，相比传统统计机器翻译（SMT），翻译流畅度和准确性显著提升，2015 年后逐步取代 SMT 成为主流。
- **图像描述（Image Captioning）**：结合 CNN（提取图像特征）与 LSTM（生成文本序列），实现从图像到文字的跨模态生成，例如对 “猫坐在沙发上” 的图像，LSTM 可生成语法正确、语义准确的描述文本。
- **语言模型（Language Modeling）**：LSTM 语言模型可预测长文本的下一个单词，在文本生成（如小说续写、新闻生成）中表现优异，为后续 Transformer 语言模型（如 GPT）奠定基础。

#### 2. LSTM 的技术地位演变：从主流到过渡

- **WMT（机器翻译会议与竞赛）案例**：

  WMT（Workshop on Machine Translation）是机器翻译领域的权威竞赛，其报告中 “RNN”（含 LSTM）的提及次数变化反映了技术趋势：

  - 2014 年（WMT14）：0 个神经机器翻译系统，主流为统计机器翻译；
  - 2016 年（WMT16）：报告中 “RNN” 提及 44 次，LSTM-based NMT 成为主流；
  - 2019 年（WMT19）：报告中 “RNN” 仅提及 7 次，“Transformer” 提及 105 次，Transformer 凭借并行计算优势逐步取代 LSTM。

- **过渡原因**：

  - LSTM 的循环计算无法并行（需按时间步顺序执行），在超长序列（如 1000 + 时间步）任务中计算效率低；
  - Transformer 通过自注意力机制实现并行计算，同时可捕捉更长距离的依赖，在长序列任务中性能更优。

#### 3. 参考文献

- "Findings of the 2016 Conference on Machine Translation (WMT16)", Bojar et al. 2016（链接：http://www.statmt.org/wmt16/pdf/W16-2301.pdf）
- "Findings of the 2018 Conference on Machine Translation (WMT18)", Bojar et al. 2018（链接：http://www.statmt.org/wmt18/pdf/WMT028.pdf）
- "Findings of the 2019 Conference on Machine Translation (WMT19)", Barrault et al. 2019（链接：http://www.statmt.org/wmt19/pdf/W19-5302.pdf）

**专业术语解释**

- **神经机器翻译（Neural Machine Translation, NMT）**：基于神经网络的机器翻译方法，核心架构为 Encoder-Decoder（编码器 - 解码器），Encoder 用 LSTM/Transformer 将源语言序列编码为隐藏状态，Decoder 用 LSTM/Transformer 将隐藏状态解码为目标语言序列，实现端到端翻译。
- **并行计算（Parallel Computation）**：同时处理多个计算任务的方式，可大幅提升效率。LSTM 因时间步依赖无法并行，而 Transformer 的自注意力机制可同时计算所有位置的注意力权重，支持并行处理，适合大规模数据训练。

### 页码 28：Graph?（图的引入）

- **图的定义与核心价值**

  

  图（Graph）是一种灵活的数学结构，用于建模数据实体（节点）之间的成对关系（边），可表示非结构化、不规则的数据关联，弥补传统网格（如图像）和序列（如文本）数据结构的局限性。

- **引用定义**：“Graphs are flexible mathematical structures modeling pair-wise relations between data entities” —— Catherine (Cami) Amein（图神经网络领域学者）。

#### 1. 图的核心组件

- **节点（Node/Vertex）**：代表数据实体，如人、物体、单词、分子原子等；
- **边（Edge/Link）**：代表节点之间的关系，如社交网络中 “朋友” 关系、交通网络中 “连接” 关系、分子中 “化学键” 关系；
- **属性（Attribute）**：节点、边或整个图可携带属性信息，如：
  - 节点属性：社交网络中用户的年龄、性别，分子中原子的类型；
  - 边属性：交通网络中道路的长度、权重，社交网络中 “朋友” 关系的亲密度；
  - 图属性：社交网络的用户总数，分子的分子量。

#### 2. 图的核心优势：适配非规则数据

- 传统数据结构（如网格、序列）的局限性：
  - 网格数据（如图像）：节点（像素）的邻居数量固定（如 3×3 邻域），仅能表示规则空间关系；
  - 序列数据（如文本）：节点（单词）的邻居仅为前序和后序节点，仅能表示线性时序关系；
- 图的灵活性：
  - 节点的邻居数量可任意（如社交网络中一个用户可拥有 10 个或 1000 个朋友）；
  - 关系类型可多样（如社交网络中 “朋友”“家人”“同事” 等不同边类型）；
  - 可表示任意拓扑结构（如环形、星形、无规则形），适配现实世界中复杂的数据关联（如分子结构、脑网络、社交网络）。

**专业术语解释**

- **非结构化数据（Unstructured Data）**：不遵循传统网格或序列结构的数据，如社交网络、分子结构、知识图谱等，其节点关系无固定规律，需通过图结构建模。
- **拓扑结构（Topological Structure）**：图中节点与边的连接方式，如环形图（节点首尾相连）、星形图（一个中心节点连接所有其他节点）、无规则图（节点连接随机），拓扑结构决定了数据的关联模式，是图分析的核心研究对象。

### 页码 29：Data as Graphs - Explicit（显式图数据）

- **显式图数据的定义**

  

  显式图数据指节点和边的关系的明确、直接定义的图结构，无需通过算法推断，常见于现实世界中具有清晰关联的系统。

#### 1. 典型显式图数据类型

##### （1）社交网络（Social Graphs）

- **节点**：用户（如 Facebook、微信用户）；
- **边**：用户之间的社交关系（如 “朋友”“关注”“好友”）；
- **属性**：
  - 节点属性：用户的年龄、性别、地域、兴趣标签；
  - 边属性：关系的建立时间、互动频率（如每月聊天次数）；
- **应用**：好友推荐（基于共同好友的边预测）、兴趣推荐（基于用户兴趣标签的节点分类）、谣言传播分析（基于边的传播路径追踪）。

##### （2）交通网络（Transportation Graphs）

- **节点**：交通站点（如公交站、地铁站、机场）；
- **边**：站点之间的交通线路（如公交线路、地铁线路、航线）；
- **属性**：
  - 节点属性：站点的地理位置、人流量、设施类型（如机场含航站楼）；
  - 边属性：线路的长度、行驶时间、票价、运力（如地铁每小时载客量）；
- **应用**：最优路线规划（基于边的权重（时间 / 距离）寻找最短路径）、交通流量预测（基于节点的人流量历史数据预测未来流量）。

##### （3）脑网络（Brain Graphs）

- **节点**：脑区（如大脑皮层的前额叶、颞叶）或神经元；
- **边**：脑区 / 神经元之间的连接（如结构连接（神经纤维束）、功能连接（同步活动））；
- **属性**：
  - 节点属性：脑区的功能（如前额叶负责决策）、神经元的类型（如兴奋性神经元、抑制性神经元）；
  - 边属性：连接的强度（如功能连接的相关系数）、传导速度；
- **应用**：脑疾病诊断（如阿尔茨海默病患者的脑网络连接强度异常）、脑功能分析（如语言任务中脑区的连接激活模式）。

##### （4）网页网络（Web Graphs）

- **节点**：网页（如 HTML 页面、博客文章）；
- **边**：网页之间的超链接（如 A 网页引用 B 网页的链接）；
- **属性**：
  - 节点属性：网页的内容主题（如科技、娱乐）、访问量；
  - 边属性：链接的锚文本（如 “点击查看详情”）、跳转频率；
- **应用**：搜索引擎排序（如 Google 的 PageRank 算法，基于边的数量和质量评估网页重要性）、网页聚类（基于节点主题属性和边的关联聚类相似网页）。

##### （5）分子网络（Molecular Graphs）

- **节点**：分子中的原子（如碳、氢、氧原子）；
- **边**：原子之间的化学键（如单键、双键、三键）；
- **属性**：
  - 节点属性：原子的类型（如 C、H、O）、化合价、电负性；
  - 边属性：化学键的类型（单 / 双 / 三键）、键长、键能；
- **应用**：药物分子设计（基于分子图的结构预测药物与靶点蛋白的结合能力）、分子性质预测（如预测分子的溶解度、毒性）。

##### （6）基因网络（Gene Graphs）

- **节点**：基因或蛋白质；
- **边**：基因 / 蛋白质之间的相互作用（如基因调控（转录因子调控靶基因）、蛋白质相互作用（酶与底物结合））；
- **属性**：
  - 节点属性：基因的功能（如负责细胞代谢）、蛋白质的结构（如氨基酸序列）；
  - 边属性：相互作用的强度（如调控效率）、作用类型（激活 / 抑制）；
- **应用**：疾病基因识别（基于基因网络的异常连接寻找致病基因）、生物通路分析（如细胞凋亡通路中的基因相互作用网络）。

**专业术语解释**

- **显式图（Explicit Graph）**：节点和边的关系直接可见、无需推断的图结构，现实世界中多数实体网络（如社交、交通）均为显式图，其结构可通过直接观测或数据记录获取。
- **PageRank 算法**：Google 提出的网页排序算法，基于网页网络的边（超链接）评估网页重要性 —— 一个网页的重要性与指向它的网页数量（入边数）和指向它的网页的重要性成正比，是图算法在工业界的经典应用。

### 页码 30：Data as Graphs - Implicit（隐式图数据）

- **隐式图数据的定义**

  隐式图数据指节点和边的关系不直接定义，需通过数据的上下文、结构或语义关联推断得出的图结构，常见于图像、文本、社交媒体等非直接关联的数据中。

#### 1. 典型隐式图数据类型

##### （1）场景图（Scene Graphs）

- **数据来源**：图像（如日常场景照片、视频帧）；
- **节点推断**：图像中的物体（如 “person”“train”“platform”“sign”），通过目标检测算法（如 Faster R-CNN）识别；
- **边推断**：物体之间的空间或语义关系（如 “person walk toward train”“person stand on platform”“sign hang on platform”），通过关系检测算法（如 R-FCN）推断；
- **应用**：图像理解（如 “描述图像中物体的位置和互动关系”）、图像检索（如 “搜索‘人在站台上等火车’的图像”）、视觉问答（如 “图像中站台上的人在做什么？”）。

##### （2）社交媒体超图（Social Media Hypergraphs）

- **数据来源**：社交媒体内容（如 Twitter 推文、微信朋友圈）；
- **节点推断**：用户、推文、话题标签（如 “#showerthoughts”“#worldnews”）、图片；
- **边推断**：多类型关联（如 “用户发布推文”“推文包含话题标签”“用户点赞推文”“推文引用图片”），因涉及多类型节点，通常用超图（Hypergraph，边可连接多个节点）建模；
- **应用**：话题传播分析（如 “#COVID19” 话题在用户间的传播路径）、用户兴趣推荐（基于用户互动的推文标签推荐相似话题）。

##### （3）动态图（Dynamic Graphs）

- **数据来源**：3D 骨架序列、点云序列（如人体动作捕捉数据、自动驾驶的点云感知数据）；
- **节点推断**：3D 骨架的关节点（如 “head”“hand”“leg”）、点云的空间点；
- **边推断**：关节点之间的骨骼连接（如 “head-neck”“hand-arm”）、点云的时空关联（如相邻帧中同一物体的点），且边的连接关系随时间动态变化（如人体动作时关节点连接不变，但位置变化）；
- **应用**：动作识别（如基于 3D 骨架图的 “跑步”“跳跃” 动作分类）、点云分割（如自动驾驶中点云图的 “车辆”“行人”“道路” 分割）。



### 页码 31：Deep Learning meets Graphs（深度学习与图的结合）

- **传统深度学习的局限性**

  

  传统深度学习模型（CNN、RNN）针对规则结构数据设计，无法直接适配图的非规则特性，具体局限如下：

#### 1. 传统深度学习模型的适配缺陷

| 模型（Model）       | 适配数据类型                             | 图数据适配缺陷                                               |
| ------------------- | ---------------------------------------- | ------------------------------------------------------------ |
| CNN（卷积神经网络） | 固定尺寸网格（如图像、语音 spectrogram） | 图节点邻居数量不固定（如社交网络中用户好友数差异大），无法应用固定大小卷积核；图无固定空间顺序（如分子图的原子排列无统一顺序），卷积操作的局部平移不变性失效。 |
| RNN（循环神经网络） | 线性序列（如文本、视频帧）               | 图为非线性拓扑结构（如环形、星形），无法按线性时序传递信息；RNN 无法捕捉图中多路径依赖（如节点 A 到节点 B 有两条路径，RNN 仅能按单一序列处理）。 |

#### 2. 图数据的核心特性：非规则与非独立同分布

- **非规则拓扑（Irregular Topology）**：
  - 节点邻居数量任意（如分子图中碳原子可能连接 1-4 个原子）；
  - 节点无固定排序（如社交网络中用户列表顺序不影响关系本质）；
- **非独立同分布（Non-IID）**：
  - 图中节点的特征依赖邻居节点（如社交网络中用户的兴趣受好友影响），违反传统机器学习 “样本独立” 假设；
  - 传统深度学习模型（如 CNN）假设样本独立，直接应用于图数据会导致特征提取偏差。

#### 3. 图神经网络（GNN）的核心价值

- GNN 的提出旨在解决 “深度学习适配图数据” 的问题，核心能力包括：
  - **自适应邻居处理**：通过聚合邻居节点特征（如均值、最大值），适配不同邻居数量的节点；
  - **拓扑结构感知**：在特征提取中融入节点间的连接关系（如通过邻接矩阵表示拓扑），捕捉图的非规则关联；
  - **非 IID 数据建模**：通过消息传递机制（Message Passing），利用邻居信息更新节点特征，适配节点间的依赖关系。

#### 4. 典型图任务

GNN 可处理多种图相关任务，覆盖节点、边、图三个层级：

- **节点级任务**：节点分类（如社交网络中用户标签预测）、节点聚类（如网页按主题聚类）；
- **边级任务**：边预测（如社交网络中好友关系预测）、边分类（如分子中化学键类型分类）；
- **图级任务**：图分类（如分子毒性预测）、图生成（如药物分子生成）、图匹配（如两个场景图的相似性匹配）。

**专业术语解释**

- **消息传递机制（Message Passing）**：GNN 的核心机制，指节点通过接收邻居节点的 “消息”（如邻居特征的聚合结果）更新自身特征，实现拓扑结构信息的融入，公式通常表示为：\(h_v^{(t+1)} = \text{UPDATE}(h_v^{(t)}, \text{AGGREGATE}(\{h_u^{(t)} | u \in \mathcal{N}(v)\}))\)，其中\(\mathcal{N}(v)\)为节点v的邻居集合。
- **独立同分布（Independent and Identically Distributed, IID）**：传统机器学习的基本假设，指样本之间相互独立且服从同一概率分布。图数据因节点间的关联关系违反该假设，需 GNN 等专用模型处理。

### 页码 32：Information representation in a graph（图中的信息表示）

- **图的信息构成**

  

  图的信息主要存储在三个层级：节点（Vertex/Node）、边（Edge/Link）、全局（Global/Master Node），不同层级的属性共同构成图的完整表示。

#### 1. 三个层级的信息属性

##### （1）节点属性（Vertex/Node Attributes）

- **定义**：描述节点自身特征的信息，是图中最基础的信息单元；
- **示例**：
  - 社交网络中，用户节点的属性包括年龄、性别、地域、兴趣标签；
  - 分子图中，原子节点的属性包括原子类型（C、H、O）、化合价、电负性；
  - 场景图中，物体节点的属性包括类别（“person”“car”）、尺寸、颜色；
- **数学表示**：设图中有N个节点，每个节点的特征维度为\(d_v\)，则节点属性矩阵为\(V \in \mathbb{R}^{N \times d_v}\)（如\(N=8\)个节点，\(d_v=10\)维特征，则V为 8×10 矩阵）。

##### （2）边属性与方向（Edge/Link Attributes and Directions）

- **定义**：描述边的特征及连接方向的信息，反映节点间关系的特性；
- **属性示例**：
  - 交通网络中，道路边的属性包括长度、限速、通行时间；
  - 社交网络中，好友边的属性包括亲密度（如互动频率）、关系类型（“family”“colleague”）；
- **方向示例**：
  - 有向图（如网页超链接：A 网页指向 B 网页，边方向为 A→B）；
  - 无向图（如社交网络好友关系：A 是 B 的好友，B 也是 A 的好友，边无方向）；
- **数学表示**：设图中有M个边，每个边的特征维度为\(d_e\)，则边属性矩阵为\(E \in \mathbb{R}^{M \times d_e}\)（如\(M=8\)个边，\(d_e=5\)维特征，则E为 8×5 矩阵）；方向通过邻接矩阵的非对称元素表示（有向图邻接矩阵A中\(A[i][j] \neq A[j][i]\)，无向图\(A[i][j] = A[j][i]\)）。

##### （3）全局属性（Global/Master Node Attributes）

- **定义**：描述整个图的宏观特征，反映图的整体属性；
- **示例**：
  - 社交网络的全局属性包括用户总数、平均好友数、社区数量；
  - 分子图的全局属性包括分子量、分子极性、是否含环状结构；
- **数学表示**：全局属性通常整合节点和边的信息，可表示为\(U \in \mathbb{R}^{(M+N) \times \max(d_v, d_e)}\)（将节点和边特征按最大维度补齐后拼接，或通过全局池化（如节点特征均值）得到固定维度向量），也可通过 “主节点”（Master Node）表示（新增一个节点连接所有其他节点，其特征代表全局信息）。

#### 2. 图信息的完整性

- 完整的图表示需同时包含三个层级的信息：
  - 仅节点属性：无法区分相同节点特征但不同连接关系的图（如两个均含 “人”“车” 节点的场景图，一个是 “人开车”，一个是 “人推车”）；
  - 仅边属性：无法区分相同边关系但不同节点特征的图（如两个均含 “好友” 边的社交网络，一个是 “青少年用户”，一个是 “中老年用户”）；
  - 全局属性：为图级任务（如图分类）提供关键信息，如分子毒性预测需依赖分子的全局结构属性。

**专业术语解释**

- **邻接矩阵（Adjacency Matrix）**：表示图中节点连接关系的矩阵，若节点i与节点j相连，则\(A[i][j] = 1\)（或边权重），否则为 0。有向图邻接矩阵非对称，无向图对称，是 GNN 中表示拓扑结构的核心工具。
- **全局池化（Global Pooling）**：从节点或边特征中提取全局图特征的操作，常见方法包括均值池化（所有节点特征的平均值）、最大值池化（所有节点特征的最大值）、求和池化（所有节点特征的和），用于图级任务的特征表示。

### 页码 33：Representing traditional data using graph（用图表示传统数据）

- **核心思路**：将传统规则结构数据（图像、文本）转化为图结构，利用 GNN 的优势挖掘数据的潜在关联，拓展深度学习的适用场景。

#### 1. 图像数据的图表示（Images as Graphs）

- **转化逻辑**：
  - **节点定义**：每个像素作为一个节点，若图像为彩色，则节点属性为 3D RGB 向量（如\([R, G, B] = [255, 0, 0]\)表示红色像素）；若为灰度图，则节点属性为 1D 亮度值；
  - **边定义**：每个像素与相邻像素（通常为 8 邻域：上、下、左、右、左上、右上、左下、右下）连接，边无属性（或边权重设为 1，表示相邻关系）；
  - **特殊处理**：非边界像素有 8 个邻居，边界像素邻居数量少于 8（如左上角像素仅有 3 个邻居），符合图的非规则拓扑特性；
- **示例**：5×5 像素图像的图表示，共 25 个节点，每个非边界节点连接 8 个邻居，边界节点连接 3-5 个邻居；
- **应用**：图像分割（基于像素图的节点分类，将像素分为 “背景”“目标” 类别）、图像修复（基于像素邻居关系补全缺失像素）。

#### 2. 文本数据的图表示（Texts as Graphs）

- **转化逻辑**：
  - **节点定义**：每个字符、单词或实体作为一个节点，节点属性为词嵌入（如 Word2Vec 向量）或字符编码（如 One-Hot 编码）；
  - **边定义**：
    - 线性连接：每个节点与前序、后序节点连接（如文本 “Graphs are all around us” 中，“Graphs”→“are”→“all”→“around”→“us”，形成线性边）；
    - 语义连接：基于语义关联添加边（如 “Graphs” 与 “us” 无直接线性连接，但因 “Graphs are around us” 的语义关联添加边）；
  - **邻接矩阵特性**：仅线性连接的文本图，邻接矩阵为对角线上次对角线和下次对角线元素为 1（如节点i与\(i+1\)连接），其余为 0，呈 “对角线线” 结构；
- **示例**：文本 “Graphs are all around us” 的单词级图表示，共 5 个节点，“Graphs” 与 “are”、“are” 与 “all” 等线性连接，“Graphs” 与 “us” 可添加语义边；
- **应用**：文本摘要（基于文本图的关键节点提取）、问答系统（基于问题与文本的图匹配寻找答案）。

#### 3. 传统数据图表示的优势

- **保留局部关联**：图像像素的邻域边保留空间局部信息（如边缘像素的相邻关系），文本的线性边保留时序局部信息（如语法顺序）；
- **拓展全局关联**：通过语义边、空间远邻边（如图像中远距离相关像素），挖掘传统模型难以捕捉的全局关联（如文本中跨句子的实体关联）；
- **适配 GNN 分析**：转化为图后，可利用 GNN 的消息传递机制，同时处理局部与全局信息，提升任务性能（如图像分类的准确率、文本理解的深度）。

**专业术语解释**

- **8 邻域（8-Neighborhood）**：图像处理中像素邻域的一种定义，指一个像素周围的 8 个相邻像素（上、下、左、右、左上、右上、左下、右下），相比 4 邻域（仅上下左右），能更完整地保留像素的空间关联信息，常用于图像分割、边缘检测等任务。
- **词嵌入（Word Embedding）**：将文本中的单词映射到低维稠密向量的技术（如 Word2Vec、GloVe），使语义相似的单词在向量空间中距离相近，是文本图节点属性的常用表示方式，解决了 One-Hot 编码维度高、语义无关的问题。

### 页码 34：What tasks to perform on graphs?（图上可执行的任务）

- **图任务的层级分类**

  

  根据预测目标的层级，图任务可分为

  图级任务

  、

  节点级任务

  、

  边级任务

  ，覆盖从局部到全局的不同分析需求。

#### 1. 图级任务（Graph-level Tasks）

- **目标**：预测整个图的单一属性或类别，将图视为一个整体进行分析；
- **典型任务**：图分类、图回归、图生成；
- **示例**：
  - 分子图分类：输入分子图，预测分子是否具有毒性（二分类）或属于哪种药物类型（多分类）；
  - 分子性质回归：输入分子图，预测分子的溶解度、沸点等连续值属性；
  - 图生成：生成符合特定规则的图（如生成具有抗癌活性的分子图、生成社交网络的用户关系图）；
- **类比传统任务**：类似图像分类（将图像视为整体预测类别），图级任务需通过全局池化（如节点特征均值）提取图的全局特征，再输入分类器 / 回归器。

#### 2. 节点级任务（Node-level Tasks）

- **目标**：预测图中每个节点的属性或类别，聚焦局部节点的分析；
- **典型任务**：节点分类、节点回归、节点聚类；
- **示例**：
  - 社交网络节点分类：输入社交网络（含部分标签节点），预测未标签用户的兴趣标签（如 “体育爱好者”“科技爱好者”）；
  - 网页节点分类：输入网页网络，预测网页的主题类别（如 “新闻”“娱乐”“电商”）；
  - 节点聚类：将图中节点按特征和连接关系聚为多个簇（如社交网络中的社区检测，将用户分为不同兴趣社区）；
- **类比传统任务**：类似图像分割（将图像像素分为不同类别），节点级任务需利用节点自身特征和邻居特征（通过 GNN 聚合），实现局部节点的精准预测。

#### 3. 边级任务（Edge-level Tasks）

- **目标**：预测图中边的属性或存在性，分析节点间关系的特性；
- **典型任务**：边预测、边分类、边权重预测；
- **示例**：
  - 社交网络边预测：输入社交网络（现有用户关系），预测两个未连接用户未来是否会建立好友关系；
  - 分子边分类：输入分子图，预测原子间化学键的类型（单键、双键、三键）；
  - 交通网络边权重预测：输入交通网络，预测道路边的通行时间（边权重）；
- **类比传统任务**：类似图像中的关系检测（预测物体间的关系），边级任务需利用边连接的两个节点特征（如节点u和v的特征拼接），结合图的拓扑结构，实现边属性的预测。

#### 4. 任务间的关联与应用场景

- **多任务协同**：不同层级的任务可相互辅助，如：
  - 节点分类结果可用于边预测（如 “同类节点更可能连接”）；
  - 边预测结果可用于图生成（如根据预测的边构建新图）；
- **典型应用场景**：
  - 生物信息学：分子图分类（药物筛选）、基因网络节点分类（疾病基因识别）；
  - 社交网络：用户节点分类（兴趣推荐）、好友边预测（关系拓展）；
  - 计算机视觉：场景图边分类（物体关系识别）、3D 点云图节点分割（目标检测）。

**专业术语解释**

- **图回归（Graph Regression）**：图级任务的一种，预测目标为连续值（如分子的溶解度、社交网络的活跃度），与图分类（预测离散类别）的核心区别在于输出类型，通常使用均方误差（MSE）作为损失函数。
- **社区检测（Community Detection）**：节点聚类的一种具体应用，指将社交网络、网页网络等图中的节点分为多个 “社区”，社区内节点连接紧密，社区间节点连接稀疏，常用算法包括 Louvain 算法、谱聚类算法。

### 页码 35：The simplest GNN（最简单的图神经网络）

- **基础 GNN 的核心架构**

  

  最简单的 GNN 遵循 “图输入→层变换→图输出” 的流程，通过多层感知机（MLP）对图的节点、边、全局属性进行优化变换，实现端到端的图特征学习。

#### 1. 架构定义：“Graph-in, Graph-out”

- **输入**：原始图\(G = \{V, E, U\}\)，其中V为节点属性矩阵，E为边属性矩阵，U为全局属性矩阵；
- **层变换**：每个 GNN 层由三个独立的 MLP 组成，分别更新节点、边、全局属性：
  - 节点属性更新：\(V^{(l+1)} = \text{MLP}_V(V^{(l)}, \mathcal{N}(V^{(l)}))\)，其中\(\mathcal{N}(V^{(l)})\)为节点\(V^{(l)}\)的邻居特征聚合结果（如均值、最大值）；
  - 边属性更新：\(E^{(l+1)} = \text{MLP}_E(E^{(l)}, V^{(l)}_u, V^{(l)}_v)\)，其中\(V^{(l)}_u\)和\(V^{(l)}_v\)为边连接的两个节点特征；
  - 全局属性更新：\(U^{(l+1)} = \text{MLP}_U(U^{(l)}, \text{Pool}(V^{(l)}), \text{Pool}(E^{(l)}))\)，其中\(\text{Pool}(\cdot)\)为全局池化操作（如节点特征均值）；
- **输出**：经过N层变换后的图\(G^{(N)} = \{V^{(N)}, E^{(N)}, U^{(N)}\}\)，输出的节点、边、全局特征可用于后续任务（如节点分类、图分类）。

#### 2. 核心组件：图独立层（Graph Independent Layer）

- **定义**：每个 GNN 层对节点、边、全局属性的更新相互独立（MLP_V、MLP_E、MLP_U 为独立网络），仅在全局属性更新时通过池化融入节点和边的信息；
- **优势**：
  - 结构简单，易于实现，适合作为 GNN 的入门架构；
  - 计算效率高，节点、边的更新可并行进行（无需等待其他属性更新）；
- **局限**：
  - 属性间交互不足：节点更新未直接融入边信息，边更新未直接融入全局信息，可能丢失跨属性关联（如边的权重影响节点特征）；
  - 拓扑结构利用有限：仅通过邻居聚合融入拓扑，未考虑更复杂的图结构（如路径、子图）。

#### 3. 简单 GNN 的输出应用

- **节点级任务**：将输出节点特征\(V^{(N)}\)输入线性分类器 / 回归器，预测每个节点的类别 / 属性；
- **边级任务**：将输出边特征\(E^{(N)}\)或输出节点特征\(V^{(N)}_u, V^{(N)}_v\)（边连接的节点）拼接，输入分类器 / 回归器，预测边的类别 / 权重；
- **图级任务**：将输出全局特征\(U^{(N)}\)或输出节点 / 边特征的全局池化结果，输入分类器 / 回归器，预测图的类别 / 属性。

**专业术语解释**

- **图独立层（Graph Independent Layer）**：GNN 的基础层结构，对图的节点、边、全局属性采用独立的 MLP 进行更新，仅通过全局池化实现有限的属性交互，结构简单但表达能力有限，常用于 GNN 的基础教学或简单任务。
- **邻居特征聚合（Neighbor Feature Aggregation）**：GNN 中融入拓扑结构的核心操作，指节点通过聚合邻居节点的特征（如均值、最大值、注意力加权）更新自身特征，使节点特征包含拓扑信息，常见聚合函数包括 Mean Aggregator、Max Aggregator、GAT 的 Attention Aggregator。

### 页码 36：GNN Predictions by Pooling Information（GNN 通过池化实现预测）

- **池化的核心作用**：解决 “不同层级特征适配不同任务” 的问题，通过聚合低层级特征（节点、边）得到高层级特征（全局、边关联节点），为各类图任务提供适配的特征输入。

#### 1. 节点级任务的池化：直接利用节点特征

- **适用场景**：图中节点已包含足够任务信息（如节点属性丰富、拓扑关系简单）；
- **流程**：
  1. GNN 层更新节点特征：通过邻居聚合和 MLP，得到包含拓扑信息的节点特征\(V^{(N)}\)；
  2. 分类 / 回归：对每个节点的特征\(v_i^{(N)} \in V^{(N)}\)，输入线性分类器（如 Softmax）或回归器，输出节点的预测结果（如类别标签、连续值）；
- **示例**：社交网络节点分类，GNN 更新用户节点特征（融入好友的兴趣标签）后，输入分类器预测用户的兴趣类别；
- **关键条件**：节点特征需包含任务相关信息，若节点无属性（仅拓扑结构），需先通过边特征聚合生成节点特征（见下文）。

#### 2. 边级任务的池化：从节点到边的特征聚合

- **适用场景**：边无属性或属性不足，需通过连接的节点特征补充信息；
- **流程**：
  1. 聚合节点特征：对每条边\(e_{u,v}\)（连接节点u和v），聚合节点u和v的特征（如拼接、均值、差值），得到边的初始特征\(e_{u,v}^{(init)} = \text{Agg}(v_u^{(N)}, v_v^{(N)})\)；
  2. 边特征更新：将\(e_{u,v}^{(init)}\)输入 MLP，得到最终边特征\(e_{u,v}^{(N)}\)；
  3. 分类 / 回归：对\(e_{u,v}^{(N)}\)输入分类器 / 回归器，预测边的类别 / 属性；
- **示例**：分子边分类，边无属性时，聚合原子节点的类型、电负性特征，输入 MLP 后预测化学键类型；
- **聚合方式选择**：
  - 拼接（Concatenation）：保留节点特征的完整信息，适合节点特征差异大的场景；
  - 差值（Difference）：突出节点特征的差异，适合边关系与节点差异相关的场景（如 “敌对” 关系的边）。

#### 3. 图级任务的池化：从节点 / 边到全局的特征聚合

- **适用场景**：图级任务需全局特征，需聚合所有节点 / 边的特征；

- **流程**：

  1. 全局池化：对节点特征\(V^{(N)}\)或边特征\(E^{(N)}\)进行全局聚合（如均值、最大值、求和），得到全局特征\(U^{(N)} = \text{GlobalPool}(V^{(N)}, E^{(N)})\)；
  2. 分类 / 回归：将\(U^{(N)}\)输入分类器 / 回归器，预测图的类别 / 属性；

- **示例**：分子图分类，全局池化原子节点的特征（如均值），得到分子的全局特征，输入分类器预测分子是否有毒；

- **池化方式对比**：

  | 池化方式（Pooling Method） | 优势                       | 劣势                           | 适用场景                                   |
  | -------------------------- | -------------------------- | ------------------------------ | ------------------------------------------ |
  | 均值池化（Mean Pooling）   | 保留整体趋势，鲁棒性强     | 易受异常值影响                 | 节点特征分布均匀的图（如社交网络）         |
  | 最大值池化（Max Pooling）  | 突出关键特征，对异常值敏感 | 丢失整体信息                   | 存在显著关键节点的图（如分子中的活性位点） |
  | 求和池化（Sum Pooling）    | 保留总量信息               | 对节点数量敏感（节点多则和大） | 需关注节点总量的图（如交通网络的总流量）   |

**专业术语解释**

- **全局池化（Global Pooling）**：GNN 中从节点 / 边特征提取全局图特征的操作，将变长的节点 / 边特征转化为固定长度的全局特征，是图级任务的关键步骤，类比 CNN 中的 Global Average Pooling（全局平均池化）。
- **边特征聚合（Edge Feature Aggregation）**：边级任务中，从连接节点特征生成边特征的操作，通过聚合节点信息补充边的特征不足，使边特征包含节点关联信息，提升边级任务的预测精度。

### 页码 37：GNN Predictions by Pooling Information（GNN 通过池化实现预测）（续）

#### 4. 特殊场景的池化策略

##### （1）仅边特征的节点级任务

- **问题**：图中仅边有属性（如交通网络中仅道路边有 “通行时间” 属性），节点无属性，如何预测节点类别？
- **解决方案**：从边特征聚合到节点特征：
  1. 邻居边聚合：对每个节点v，聚合其所有入射边的特征（如均值、最大值），得到节点的初始特征\(v^{(init)} = \text{Agg}(\{e_{u,v} | u \in \mathcal{N}(v)\})\)；
  2. 节点特征更新：将\(v^{(init)}\)输入 MLP，得到包含边信息的节点特征\(v^{(N)}\)；
  3. 节点预测：对\(v^{(N)}\)输入分类器，完成节点级任务；
- **示例**：交通网络节点分类，仅道路边有 “通行时间” 属性，聚合节点的所有入射边通行时间，得到节点的 “平均周边通行时间” 特征，预测节点（站点）的 “繁忙程度” 类别。

##### （2）仅节点特征的边级任务

- **问题**：图中仅节点有属性（如社交网络中仅用户有 “兴趣标签” 属性），边无属性，如何预测边的存在性？
- **解决方案**：从节点特征生成边特征：
  1. 节点特征交互：对候选边\(e_{u,v}\)，通过节点u和v的特征交互（如拼接、点积、差值）生成边的初始特征\(e_{u,v}^{(init)}\)；
  2. 边特征更新：将\(e_{u,v}^{(init)}\)输入 MLP，得到边特征\(e_{u,v}^{(N)}\)；
  3. 边预测：对\(e_{u,v}^{(N)}\)输入二分类器（如 Sigmoid），预测边是否存在；
- **示例**：社交网络边预测，仅用户有 “兴趣标签” 属性，计算用户u和v的兴趣标签相似度（点积），生成边的 “兴趣相似度” 特征，预测两人是否会成为好友。

##### （3）端到端图任务的池化整合

- **完整流程**：以图分类任务为例，整合节点、边、全局特征：
  1. GNN 层更新：通过多层 GNN，交替更新节点特征\(V^{(l)}\)（融入邻居和边信息）、边特征\(E^{(l)}\)（融入节点信息）；
  2. 全局池化：对最终层的\(V^{(N)}\)和\(E^{(N)}\)分别进行全局池化（如均值），得到\(\text{Pool}(V^{(N)})\)和\(\text{Pool}(E^{(N)})\)；
  3. 全局特征拼接：将\(\text{Pool}(V^{(N)})\)、\(\text{Pool}(E^{(N)})\)和初始全局属性U拼接，得到最终全局特征\(U^{(final)}\)；
  4. 图分类：将\(U^{(final)}\)输入 Softmax 分类器，输出图的类别预测；
- **优势**：端到端训练，无需人工特征工程，池化操作自动整合多源信息，提升任务性能。

#### 5. 池化的关键原则

- **适配任务层级**：节点级任务需保留节点个体特征（不全局池化），图级任务需全局特征（必须全局池化）；
- **保留关键信息**：根据任务需求选择池化方式（如关键节点突出选 Max Pooling，整体趋势选 Mean Pooling）；
- **减少信息损失**：多层池化时，可通过 “局部池化 + 全局池化”（如先聚合节点的局部邻居，再全局聚合）逐步提炼信息，避免单次池化导致的信息丢失。

**专业术语解释**

- **入射边（Incident Edges）**：与某个节点直接连接的所有边，对节点v而言，入射边为\(\{e_{u,v} | u \in \mathcal{N}(v)\}\)，即从邻居节点u指向v的边（有向图）或无向边，是节点聚合边特征的核心对象。
- **端到端训练（End-to-End Training）**：从输入（原始图）到输出（预测结果）的整个流程由模型自动学习，无需人工干预特征提取或预处理，GNN 通过池化和 MLP 的结合实现端到端训练，简化了图任务的流程，提升了模型的泛化能力。

### 页码 38：Inter-layer message passing（层间消息传递）

- **消息传递的核心定义**

  

  消息传递是 GNN 的核心机制，指节点通过接收和处理邻居节点的 “消息”（特征）更新自身特征，实现拓扑结构信息的逐层传递，解决传统模型无法利用图拓扑的问题。

#### 1. 消息传递的基本流程

以节点特征更新为例，消息传递通常包含四个步骤：

1. **消息生成（Message Generation）**：邻居节点u为中心节点v生成消息，消息通常基于邻居节点特征和边特征，公式表示为：\(m_{u \to v}^{(t)} = \text{MSG}(h_u^{(t)}, h_v^{(t)}, e_{u,v}^{(t)})\)

   其中\(h_u^{(t)}\)为邻居节点u在第t层的特征，\(h_v^{(t)}\)为中心节点v在第t层的特征，\(e_{u,v}^{(t)}\)为边\(e_{u,v}\)在第t层的特征，\(\text{MSG}(\cdot)\)为消息生成函数（如节点特征拼接、边特征加权）。

2. **消息聚合（Message Aggregation）**：中心节点v聚合所有邻居节点发送的消息，得到聚合消息，公式表示为：\(m_v^{(t)} = \text{AGG}\left(\{m_{u \to v}^{(t)} | u \in \mathcal{N}(v)\}\right)\)

   其中\(\mathcal{N}(v)\)为节点v的邻居集合，\(\text{AGG}(\cdot)\)为聚合函数（如均值、最大值、注意力加权），需满足置换不变性（邻居顺序不影响聚合结果）。

3. **特征更新（Feature Update）**：中心节点v结合自身当前特征和聚合消息，更新为第\(t+1\)层的特征，公式表示为：\(h_v^{(t+1)} = \text{UPDATE}(h_v^{(t)}, m_v^{(t)})\)

   其中\(\text{UPDATE}(\cdot)\)为更新函数（如 MLP、残差连接），使节点特征融入邻居信息。

4. **跨层传递（Cross-layer Transfer）**：将第\(t+1\)层的节点特征\(h_v^{(t+1)}\)传递到下一层，重复上述步骤，直至最后一层，得到包含多层拓扑信息的节点特征。

#### 2. 消息传递的扩展：边与全局消息

- **边的消息传递**：边特征更新需利用连接的两个节点特征和相邻边特征，公式表示为：\(e_{u,v}^{(t+1)} = \text{MSG}_{edge}(h_u^{(t+1)}, h_v^{(t+1)}, \{e_{u,w}^{(t)} | w \in \mathcal{N}(u)\}, \{e_{v,w}^{(t)} | w \in \mathcal{N}(v)\})\)

  即边特征更新融入了更新后的节点特征和相邻边特征，捕捉边与节点、边与边的关联。

- **全局的消息传递**：全局特征更新需利用所有节点和边的聚合特征，公式表示为：\(U^{(t+1)} = \text{MSG}_{global}(\text{Pool}(h_v^{(t+1)}), \text{Pool}(e_{u,v}^{(t+1)}), U^{(t)})\)

  即全局特征更新融入了当前层的节点和边全局池化结果，以及上一层的全局特征，保持全局信息的连贯性。

#### 3. 消息传递与卷积的类比

- **与 CNN 卷积的相似性**：
  - CNN 中，像素通过聚合局部邻域（如 3×3 窗口）的特征更新自身（卷积操作）；
  - GNN 中，节点通过聚合邻居节点的特征更新自身（消息传递）；
  - 核心共性：均通过局部邻域聚合提取局部特征，再逐层传递实现全局信息捕捉。
- **与 CNN 卷积的差异**：
  - CNN 邻域固定（如 3×3 窗口），GNN 邻域不固定（邻居数量随节点变化）；
  - CNN 卷积核参数共享（同一卷积核应用于所有像素），GNN 消息函数参数共享（同一消息函数应用于所有节点对）；
  - CNN 依赖空间排序（像素按网格排列），GNN 不依赖节点排序（邻居顺序不影响聚合结果）。

#### 4. 消息传递的关键作用

- **拓扑信息融入**：通过邻居聚合，节点特征包含了连接关系，解决了传统模型无法利用图拓扑的问题；
- **特征交互增强**：节点、边、全局特征通过消息传递相互交互（如节点特征融入边信息，全局特征融入节点信息），提升特征的表达能力；
- **多尺度信息捕捉**：多层消息传递后，节点特征可包含多跳邻居的信息（如 3 层消息传递后，节点包含 3 跳内所有邻居的信息），实现多尺度拓扑的建模。

**专业术语解释**

- **置换不变性（Permutation Invariance）**：聚合函数的关键性质，指邻居节点的顺序变化不影响聚合结果（如邻居顺序为\([u1, u2]\)和\([u2, u1]\)的聚合结果相同），确保 GNN 对节点无固定排序的图具有鲁棒性，常见聚合函数（均值、最大值）均满足该性质。
- **多跳邻居（Multi-hop Neighbors）**：节点的k跳邻居指通过k条边可到达的节点（1 跳邻居为直接连接的节点，2 跳邻居为邻居的邻居），多层消息传递使节点特征包含多跳邻居信息，可捕捉图的长距离依赖。

### 页码 39：Message passing as convolution（作为卷积的消息传递）

- **谱图卷积的核心思想**

  

  消息传递可从

  谱域

  （频率域）角度解释为 “图上的卷积操作”，通过图的拉普拉斯矩阵（Laplacian Matrix）实现频率域滤波，类比传统信号处理中的卷积。

#### 1. 谱图理论基础

- **图的拉普拉斯矩阵（Laplacian Matrix, L）**：
  - 定义：由邻接矩阵A和度矩阵D计算得到，公式为\(L = D - A\)；
  - 邻接矩阵A：\(A[i][j] = 1\)（节点i与j相连），否则为 0（或边权重）；
  - 度矩阵D：对角矩阵，\(D[i][i] = \sum_j A[i][j]\)（节点i的度，即邻居数量）；
  - 性质：拉普拉斯矩阵为实对称矩阵，可正交对角化（\(L = U \Lambda U^T\)，其中U为正交特征向量矩阵，\(\Lambda\)为对角特征值矩阵），且所有特征值非负（\(\lambda_0 \leq \lambda_1 \leq ... \leq \lambda_{N-1}\)）。
- **图信号的频率**：
  - 图信号：定义在图节点上的向量\(f \in \mathbb{R}^N\)（如节点属性向量）；
  - 频率含义：拉普拉斯矩阵的特征值\(\lambda_i\)对应图信号的频率，特征值越小，频率越低（信号变化平缓，如节点特征相似）；特征值越大，频率越高（信号变化剧烈，如节点特征差异大）；
  - 示例：全连接图中，节点特征均相同的信号为低频信号（\(\lambda=0\)）；节点特征交替变化（如 0,1,0,1）的信号为高频信号（\(\lambda\)大）。

#### 2. 谱图卷积的定义

- **传统信号卷积**：时域卷积等价于频域乘积，即\(f * g = \mathcal{F}^{-1}(\mathcal{F}(f) \odot \mathcal{F}(g))\)，其中\(\mathcal{F}\)为傅里叶变换，\(\odot\)为元素乘法。

- **图信号卷积**：类比传统卷积，谱图卷积定义为：

  \(f *_{\mathcal{G}} g = U \left( U^T f \odot U^T g \right) U^T\)

  其中：

  - U为拉普拉斯矩阵L的特征向量矩阵（图傅里叶变换的基）；
  - \(U^T f\)为图信号f的傅里叶变换（频域表示）；
  - \(U^T g\)为滤波器g的傅里叶变换；
  - \(\odot\)为元素乘法（频域滤波）；
  - \(U (\cdot) U^T\)为逆傅里叶变换（频域转回时域）。

- **简化形式**：若滤波器g的傅里叶变换为对角矩阵\(H_{\theta} = \text{diag}(U^T g)\)（可学习参数），则谱图卷积简化为：

  \(f *_{\mathcal{G}} g = U H_{\theta} U^T f\)

  即通过学习对角矩阵\(H_{\theta}\)，实现对图信号的频域滤波（如保留低频信号、抑制高频信号）。

#### 3. 谱图卷积与消息传递的关联

- **谱图卷积的时域解释**：谱图卷积的时域操作等价于节点特征的线性组合，即：

  

  \((U H_{\theta} U^T f)_v = \sum_{u \in \mathcal{N}(v) \cup \{v\}} W_{v,u} f_u\)

  

  其中

  \(W_{v,u}\)

  为卷积权重（由

  \(U H_{\theta} U^T\)

  计算得到），表示节点

  u

  对节点

  v

  的贡献。

- **与消息传递的等价性**：

  - 消息传递中，节点v的特征更新为\(h_v^{(t+1)} = \text{UPDATE}(h_v^{(t)}, \sum_{u \in \mathcal{N}(v)} W_{v,u} h_u^{(t)})\)；
  - 谱图卷积中，节点v的特征更新为\(f_v' = \sum_{u \in \mathcal{N}(v) \cup \{v\}} W_{v,u} f_u\)；
  - 核心关联：两者均通过邻居节点的加权求和（消息 / 卷积权重）更新中心节点特征，消息传递可视为谱图卷积的时域实现，谱图卷积为消息传递提供了理论基础。

#### 4. 谱图卷积的优势与局限

- **优势**：
  - 理论严谨：基于谱图理论，为 GNN 的卷积操作提供了数学解释；
  - 全局滤波：可对图信号进行全局频率域滤波，适合处理全局拓扑相关的任务（如图分类）。
- **局限**：
  - 计算复杂：需计算拉普拉斯矩阵的特征分解（时间复杂度\(O(N^3)\)），不适用于大规模图（N大）；
  - 图依赖性：特征向量U依赖具体图结构，不同图的特征向量不同，模型泛化能力差（如训练图的U无法用于测试图）；
  - 局部性差：谱图卷积的权重\(W_{v,u}\)涉及所有节点对，不具有局部性（消息传递仅涉及邻居节点），计算效率低。

**专业术语解释**

- **谱域（Spectral Domain）**：信号处理中的频率域，图信号的谱域指通过拉普拉斯矩阵特征分解得到的频率空间，谱图卷积在谱域进行滤波操作，类比传统信号的频域滤波（如低通滤波、高通滤波）。
- **图傅里叶变换（Graph Fourier Transform, GFT）**：将图信号从时域（节点域）转换到谱域（频率域）的操作，定义为\(\hat{f} = U^T f\)，其中U为拉普拉斯矩阵的特征向量矩阵，\(\hat{f}\)为谱域信号（傅里叶系数），逆变换为\(f = U \hat{f}\)。

### 页码 40：Extend message passing to the whole graph（消息传递扩展到全图）

- **全图消息传递的核心目标**

  

  突破节点、边、全局属性的独立更新限制，实现三者的深度交互，使消息传递覆盖全图所有属性，提升 GNN 对复杂图结构的建模能力。

#### 1. 跨属性消息传递的挑战

- **属性维度差异**：节点、边、全局属性的维度和形状可能不同（如节点特征为\(d_v\)维，边特征为\(d_e\)维，全局特征为\(d_u\)维），直接交互需解决维度不匹配问题；
- **属性关联复杂**：节点与边、边与全局、节点与全局的关联模式多样（如节点属性影响边属性，边属性影响全局属性），需设计通用的交互机制。

#### 2. 跨属性消息传递的实现方案

- **核心思路**：引入可学习的映射函数（\(\rho_{A \to B}\)），将属性A的特征映射到属性B的维度空间，再通过更新函数融入属性B的特征，实现跨属性交互。

##### （1）节点属性的跨属性更新

- 节点特征更新需融入边和全局信息，公式表示为：

  

  \(V_{n+1} = f_{V_n} \left( V_n + \rho_{E \to V}(E_n) + \rho_{U \to V}(U_n) \right)\)

  

  其中：

  

  - \(\rho_{E \to V}(E_n)\)：将第n层边特征\(E_n\)映射到节点特征维度的函数（如对每条边连接的节点，分配边特征的均值，再聚合到节点）；
  - \(\rho_{U \to V}(U_n)\)：将第n层全局特征\(U_n\)映射到节点特征维度的函数（如将全局特征复制到每个节点，与节点特征拼接）；
  - \(f_{V_n}\)：节点特征的更新函数（如 MLP），融合节点自身特征、映射后的边特征和全局特征。

##### （2）边属性的跨属性更新

- 边特征更新需融入节点和全局信息，公式表示为：

  

  \(E_{n+1} = f_{E_n} \left( E_n + \rho_{V \to E}(V_n) + \rho_{U \to E}(U_n) \right)\)

  

  其中：

  

  - \(\rho_{V \to E}(V_n)\)：将第n层节点特征\(V_n\)映射到边特征维度的函数（如对边\(e_{u,v}\)，拼接节点u和v的特征，再通过线性层映射到边维度）；
  - \(\rho_{U \to E}(U_n)\)：将第n层全局特征\(U_n\)映射到边特征维度的函数（如将全局特征与边特征拼接，再通过线性层映射）；
  - \(f_{E_n}\)：边特征的更新函数（如 MLP），融合边自身特征、映射后的节点特征和全局特征。

##### （3）全局属性的跨属性更新

- 全局特征更新需融入节点和边信息，公式表示为：

  

  \(U_{n+1} = f_{U_n} \left( U_n + \rho_{V \to U}(V_n) + \rho_{E \to U}(E_n) \right)\)

  

  其中：

  

  - \(\rho_{V \to U}(V_n)\)：将第n层节点特征\(V_n\)映射到全局特征维度的函数（如对节点特征进行全局均值池化，得到全局节点特征）；
  - \(\rho_{E \to U}(E_n)\)：将第n层边特征\(E_n\)映射到全局特征维度的函数（如对边特征进行全局均值池化，得到全局边特征）；
  - \(f_{U_n}\)：全局特征的更新函数（如 MLP），融合全局自身特征、映射后的节点特征和边特征。

#### 3. 全图消息传递的架构：Graph Nets Layer

- **架构图示**：
  - 输入：第n层的图\(G_n = \{V_n, E_n, U_n\}\)；
  - 映射模块：\(\rho_{E \to V}\)、\(\rho_{U \to V}\)、\(\rho_{V \to E}\)、\(\rho_{U \to E}\)、\(\rho_{V \to U}\)、\(\rho_{E \to U}\)，实现跨属性维度映射；
  - 更新模块：\(f_{V_n}\)、\(f_{E_n}\)、\(f_{U_n}\)（MLP），实现各属性的特征更新；
  - 输出：第\(n+1\)层的图\(G_{n+1} = \{V_{n+1}, E_{n+1}, U_{n+1}\}\)；
- **核心优势**：
  - 全属性交互：节点、边、全局属性通过映射函数深度交互，无属性信息孤立；
  - 通用性强：适用于任意类型的图（显式图、隐式图）和任意图任务（节点级、边级、图级）；
  - 可扩展性好：可堆叠多层 Graph Nets Layer，逐步提升特征的表达能力。

#### 4. 应用示例：分子图分类

- **流程**：
  1. 输入分子图\(G_0 = \{V_0, E_0, U_0\}\)，其中\(V_0\)为原子特征（类型、电负性），\(E_0\)为化学键特征（类型、键长），\(U_0\)为分子初始全局特征（分子量）；
  2. 多层 Graph Nets Layer 更新：
     - 节点更新：原子特征融入化学键（如键长影响原子反应活性）和全局特征（如分子量影响原子排列）；
     - 边更新：化学键特征融入原子（如原子电负性影响键极性）和全局特征；
     - 全局更新：全局特征融入原子和化学键的全局池化结果；
  3. 图分类：对最终层的全局特征\(U_N\)输入 Softmax 分类器，预测分子是否具有毒性。

**专业术语解释**

- **映射函数（Mapping Function）**：全图消息传递中用于解决属性维度不匹配的函数，通常为线性层或 MLP，将一种属性的特征映射到另一种属性的维度空间，确保跨属性特征可直接交互，是实现全图属性融合的关键组件。
- **Graph Nets Layer**：实现全图消息传递的 GNN 层结构，通过跨属性映射和更新函数，实现节点、边、全局属性的深度交互，是通用图神经网络（如 Graph Networks）的核心层，适用于复杂图结构的建模。



### 页码 41：Performance tricks for GNNs（图神经网络的性能优化技巧）

- **核心目标**：在保证模型表达能力的前提下，提升 GNN 的训练效率、泛化能力，避免过拟合或欠拟合，适配不同规模和类型的图任务。

#### 1. 模型结构优化

##### （1）GNN 层数（深度）选择

- **影响机制**：
  - 层数过少（如 1-2 层）：节点仅能捕捉 1-2 跳邻居信息，无法利用长距离拓扑关联（如分子中远端原子的相互作用），导致欠拟合；
  - 层数过多（如 10 + 层）：节点特征过度平滑（所有节点特征趋于一致），丢失局部区分性（如社交网络中不同社区的用户特征无差异），同时增加计算成本和过拟合风险；
- **优化策略**：
  - 小规模图（节点数 < 1000）：可适当增加层数（3-5 层），充分利用拓扑信息；
  - 大规模图（节点数 > 10000）：建议层数控制在 2-3 层，平衡性能与效率；
  - 残差连接（Residual Connection）：在多层 GNN 中加入残差边（如\(h_v^{(t+1)} = h_v^{(t+1)} + h_v^{(t)}\)），缓解特征平滑和梯度消失，支持更深层训练。

##### （2）属性维度（参数）调整

- **影响机制**：
  - 维度过低（如节点特征维度 < 16）：特征表达能力不足，无法捕捉复杂属性关联（如分子中原子的多维度化学属性）；
  - 维度过高（如节点特征维度 > 1024）：参数数量激增（如 10000 节点 ×1024 维度的特征矩阵规模达 10^7），导致训练缓慢、过拟合；
- **优化策略**：
  - 参考任务复杂度：简单任务（如二分类）维度设为 32-64，复杂任务（如多分类、图生成）设为 64-256；
  - 维度压缩：通过线性层或 PCA 对高维属性（如文本词嵌入）降维，保留关键信息的同时减少参数；
  - 参数共享：在多层 GNN 中共享部分映射函数（如\(\rho_{E \to V}\)）参数，减少参数总量。

#### 2. 属性交互与全局表示优化

##### （1）增强属性交互

- **核心逻辑**：节点、边、全局属性的交互越充分，特征包含的信息越完整，模型性能越优；
- **优化策略**：
  - 交叉属性聚合：节点更新时不仅聚合邻居节点特征，还聚合邻居边特征（如\(m_v^{(t)} = \text{Agg}(h_u^{(t)}, e_{u,v}^{(t)})\)）；
  - 多轮交互：在每层 GNN 中先更新边特征，再更新节点特征，最后更新全局特征，形成 “边→节点→全局” 的交互闭环；
- **示例**：分子图中，原子节点更新时融入化学键边的 “键能” 特征，更准确反映原子的反应活性。

##### （2）显式全局表示学习

- **核心逻辑**：全局特征（如分子的分子量、社交网络的社区数）对图级任务至关重要，显式学习全局表示可提升任务精度；
- **优化策略**：
  - 专用全局 MLP：为全局属性设计独立的 MLP（\(f_{U_n}\)），而非仅依赖节点 / 边的池化结果；
  - 全局注意力（Global Attention）：通过注意力机制（如\(U^{(t+1)} = \sum_v \alpha_v h_v^{(t)}\)，\(\alpha_v\)为节点注意力权重），突出关键节点对全局特征的贡献；
- **示例**：社交网络图分类中，通过全局注意力赋予 “社区中心节点” 更高权重，使全局特征更准确反映社区结构。

#### 3. 节点与边表示的优先级

- **经验结论**：节点特征通常比边特征包含更关键的任务信息（如社交网络中用户属性比好友关系属性更能预测用户行为）；
- **优化策略**：
  - 资源倾斜：为节点属性分配更多参数（如节点 MLP 层数比边 MLP 多 1 层）；
  - 边特征增强：若边特征稀疏，通过节点特征生成边特征（如\(e_{u,v} = h_u \odot h_v\)），补充边信息；
- **例外场景**：边级任务（如边预测、边分类）需优先优化边特征，可增加边 MLP 维度、引入边注意力机制。

**专业术语解释**

- **特征平滑（Feature Smoothing）**：多层 GNN 中，节点通过聚合邻居特征逐渐趋同的现象，导致节点失去局部区分性，常见于无残差连接的深层 GNN，可通过残差连接、注意力机制缓解。
- **残差连接（Residual Connection）**：源于 ResNet，在 GNN 中用于保留前一层节点特征，避免特征过度更新导致的信息丢失，公式通常为\(h_v^{(t+1)} = \text{UPDATE}(h_v^{(t)}, m_v^{(t)}) + h_v^{(t)}\)，使梯度可直接通过残差边传递，支持深层训练。

### 页码 42：Aggregation trick for GNNs（图神经网络的聚合技巧）

- **聚合函数的核心作用**：将邻居节点 / 边的特征整合为中心节点的消息，聚合函数的选择直接影响消息的质量和模型的表达能力，需根据任务特性适配。

#### 1. 常见聚合函数及特性

| 聚合函数（Aggregation Function） | 计算方式                                                     | 优势                                             | 劣势                                                   | 适用场景                                                     |
| -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------ | ------------------------------------------------------ | ------------------------------------------------------------ |
| 均值池化（Mean Pooling）         | \(m_v = \frac{1}{\|\mathcal{N}(v)\|} \sum_{u \in \mathcal{N}(v)} h_u\) | 保留整体趋势，鲁棒性强，对噪声不敏感             | 抑制关键邻居的突出贡献（如分子中的活性位点节点）       | 邻居特征分布均匀的图（如社交网络、交通网络）                 |
| 最大值池化（Max Pooling）        | \(m_v = \max_{u \in \mathcal{N}(v)} h_u\)（逐元素最大）      | 突出关键特征，捕捉局部极值（如节点的最大属性值） | 丢失整体信息，对异常值敏感                             | 存在显著关键节点的图（如分子中的活性位点、网页网络中的核心网页） |
| 求和池化（Sum Pooling）          | \(m_v = \sum_{u \in \mathcal{N}(v)} h_u\)                    | 保留总量信息（如邻居节点的属性总和）             | 对邻居数量敏感（邻居多则和大），易受节点密度影响       | 需关注总量的场景（如交通网络中节点的总流量、社交网络中用户的总互动数） |
| 注意力池化（Attention Pooling）  | \(m_v = \sum_{u \in \mathcal{N}(v)} \alpha_{v,u} h_u\)，\(\alpha_{v,u} = \frac{\exp(\text{score}(h_v, h_u))}{\sum_{w \in \mathcal{N}(v)} \exp(\text{score}(h_v, h_w))}\) | 自适应分配权重，突出重要邻居，抑制噪声           | 计算复杂度高（需为每个节点对计算注意力分数），易过拟合 | 邻居重要性差异大的图（如知识图谱中核心实体与边缘实体的连接） |

#### 2. 聚合函数的局限性：无法区分的图对

- **核心问题**：部分聚合函数无法区分拓扑结构或特征分布不同但聚合结果相同的图，导致模型误判；
- **示例**：
  - 均值池化：两个图均含 3 个节点，图 A 节点特征为 [1,2,3]，图 B 节点特征为 [2,2,2]，均值均为 2，均值池化无法区分；
  - 最大值池化：两个图均含 3 个节点，图 A 节点特征为 [3,1,1]，图 B 节点特征为 [1,3,1]，最大值均为 3，最大值池化无法区分；
- **解决方案**：
  - 组合聚合：同时使用两种聚合函数（如\(m_v = [\text{Mean}(h_u), \text{Max}(h_u)]\)），拼接结果作为消息，提升区分能力；
  - 高阶统计量：引入方差、标准差等高阶统计量（如\(m_v = [\text{Mean}(h_u), \text{Std}(h_u)]\)），捕捉特征分布差异；
  - 图结构特征：在聚合时融入节点的度、聚类系数等结构特征（如\(m_v = \text{Agg}(h_u, \text{degree}(u))\)），区分拓扑不同的图。

#### 3. 聚合函数的选择原则

- **适配任务目标**：
  - 节点级任务（如分类）：优先选择注意力池化或均值池化，保留节点个体差异；
  - 图级任务（如分类）：优先选择均值 + 求和池化，兼顾整体趋势与总量信息；
  - 边级任务（如预测）：优先选择注意力池化，突出关键边连接的节点贡献；
- **适配数据特性**：
  - 噪声数据：选择均值池化，增强鲁棒性；
  - 稀疏数据：选择最大值池化，突出有限的关键信息；
  - 稠密数据：选择注意力池化，自适应筛选重要信息；
- **实验验证**：通过对比不同聚合函数的验证集性能（如准确率、损失），最终确定最优方案，避免依赖经验选择。

**专业术语解释**

- **注意力分数（Attention Score）**：注意力池化中衡量邻居节点重要性的指标，通常通过节点特征的相似度（如点积、MLP 打分）计算，分数越高表示邻居对中心节点的贡献越大，实现 “关注重要邻居，忽略噪声邻居” 的效果。
- **高阶统计量（High-Order Statistics）**：除均值、最大值外的统计量（如方差、标准差、偏度），可捕捉特征分布的离散程度、对称性等信息，补充基础聚合函数的不足，提升图的区分能力。

### 页码 43：Train the GNN parameters（图神经网络的参数训练）

- **GNN 训练的核心流程**：通过反向传播优化模型参数（如 MLP 权重、映射函数\(\rho\)参数），最小化损失函数，使模型输出符合任务目标。

#### 1. 训练参数与可微性

##### （1）待训练参数

- GNN 的可训练参数主要包括：
  - 聚合函数参数：如注意力池化的打分 MLP 权重、均值池化的线性层权重；
  - 更新函数参数：节点 / 边 / 全局更新 MLP（\(f_{V_n}, f_{E_n}, f_{U_n}\)）的权重和偏置；
  - 映射函数参数：跨属性映射函数（\(\rho_{E \to V}, \rho_{V \to E}\)等）的权重和偏置；
- **参数组织**：参数通常按层存储（如第n层的\(f_{V_n}\)参数独立于第\(n+1\)层），支持逐层训练或端到端训练。

##### （2）可微性保证

- **关键前提**：所有模块（聚合、更新、映射）需为可微函数，确保损失函数对参数的梯度可计算；
- **可微模块设计**：
  - 聚合函数：均值、最大值、求和池化均为可微操作（最大值池化通过子梯度实现可微）；
  - 更新函数：MLP 使用可微激活函数（如 ReLU、Tanh），避免阶跃函数（如 Sigmoid 在两端梯度接近 0，建议仅用于门控）；
  - 映射函数：线性层或 MLP，确保输入到输出的梯度可传递；
- **例外处理**：若存在不可微操作（如硬阈值筛选节点），需通过软化操作（如 Softmax 替代 argmax）使其可微，或使用强化学习的策略梯度方法训练。

#### 2. 训练流程与优化器

##### （1）端到端训练流程

1. **数据预处理**：
   - 图结构整理：将原始数据（如分子 SMILES 字符串、社交网络邻接表）转化为标准图格式\(G = \{V, E, U\}\)；
   - 特征归一化：对节点 / 边属性进行归一化（如\(h_v = \frac{h_v - \mu}{\sigma}\)），避免特征尺度差异导致的训练不稳定；
   - 数据划分：按 7:2:1 比例划分为训练集、验证集、测试集，确保数据分布一致。
2. **前向传播**：
   - 初始化：随机初始化模型参数（如使用 Xavier 初始化 MLP 权重），设置初始图\(G_0 = \{V_0, E_0, U_0\}\)；
   - 多层更新：堆叠N层 GNN，按 “边→节点→全局” 顺序更新属性，得到最终层图\(G_N = \{V_N, E_N, U_N\}\)；
   - 预测输出：根据任务类型提取对应特征（节点级任务用\(V_N\)，图级任务用\(U_N\)），输入分类器 / 回归器得到预测结果\(\hat{y}\)。
3. **损失计算**：
   - 分类任务：使用交叉熵损失（Cross-Entropy Loss），公式为\(J = -\sum_{i} y_i \log \hat{y}_i\)（\(y_i\)为真实标签，\(\hat{y}_i\)为预测概率）；
   - 回归任务：使用均方误差损失（MSE Loss），公式为\(J = \frac{1}{M} \sum_{i} (y_i - \hat{y}_i)^2\)（M为样本数）；
   - 图生成任务：使用对抗损失（GAN Loss）或重构损失（Reconstruction Loss），确保生成图与真实图结构一致。
4. **反向传播与参数更新**：
   - 梯度计算：通过自动微分框架（如 PyTorch 的 autograd、TensorFlow 的 GradientTape）计算损失函数对所有参数的梯度；
   - 梯度裁剪（Gradient Clipping）：对梯度进行裁剪（如最大范数裁剪\(\|\nabla\| \leq 5\)），防止梯度爆炸；
   - 优化器更新：使用优化器（如 Adam、SGD）沿梯度负方向更新参数，公式为\(\theta = \theta - \eta \cdot \nabla J(\theta)\)（\(\eta\)为学习率，通常设为 1e-3~1e-4）。
5. **模型评估与早停**：
   - 验证集评估：每轮训练后在验证集上计算性能指标（如准确率、MAE）；
   - 早停（Early Stopping）：若验证集性能连续 3-5 轮无提升，停止训练，避免过拟合；
   - 测试集测试：训练结束后在测试集上评估最终性能，确保模型泛化能力。

#### 3. 训练技巧：提升泛化能力

- **正则化（Regularization）**：
  - Dropout：在 MLP 层加入 Dropout（概率 0.1~0.3），随机失活部分神经元，防止过拟合；
  - L2 正则化：在损失函数中加入参数范数惩罚（如\(J = J + \lambda \sum \|\theta\|^2\)，\(\lambda = 1e-5\)），抑制参数过大；
- **数据增强（Data Augmentation）**：
  - 节点扰动：随机添加 / 删除少量节点（如分子图中随机移除一个非活性位点原子），增强模型鲁棒性；
  - 边扰动：随机翻转少量边（如社交网络中随机改变一对用户的 “好友” 关系为 “非好友”），提升模型对拓扑变化的适应能力；
- **预训练（Pretraining）**：
  - 在大规模无标签图（如 PubMed 论文引用图、Reddit 社交图）上预训练 GNN，再在下游任务（如节点分类）上微调，提升泛化能力。

**专业术语解释**

- **自动微分（Automatic Differentiation）**：深度学习框架（如 PyTorch、TensorFlow）提供的梯度计算工具，通过追踪前向传播的计算路径，自动计算损失函数对参数的梯度，无需手动推导，是 GNN 高效训练的基础。
- **早停（Early Stopping）**：防止过拟合的训练策略，当验证集性能不再提升时停止训练，保留验证集性能最优的模型参数，避免模型在训练集上过度拟合噪声数据。

### 页码 44：Pursuing better performance?（追求更好的性能？）

- **GNN 性能提升的核心方向**：突破传统 “基于邻居池化” 的局限，充分利用图的拓扑结构、频率特性，结合卷积、频率分析等理论，提升模型对复杂图的建模能力。

#### 1. 现有 GNN 的局限：邻居池化的不足

- **核心问题**：
  - 拓扑利用不充分：传统 GNN 仅通过邻居聚合利用局部拓扑，忽略图的全局结构（如路径、子图、连通分量）；
  - 频率信息缺失：未考虑图信号的频率特性（如低频信号对应全局趋势，高频信号对应局部细节），无法像 CNN 那样通过频率滤波提取层次化特征；
- **示例**：在分子图中，传统 GNN 无法有效捕捉 “环状结构” 的全局拓扑，也无法区分分子信号的低频（整体极性）和高频（局部键能）成分，导致分子性质预测精度受限。

#### 2. 潜在提升方向：图的矩阵表示与频率分析

##### （1）图的矩阵表示

- **核心逻辑**：图的拓扑结构和属性可通过矩阵（如邻接矩阵、拉普拉斯矩阵、属性矩阵）定量表示，矩阵的代数特性（如特征值、特征向量）蕴含图的深层信息；
- **关键矩阵**：
  - 邻接矩阵A：表示节点连接关系，非零元素位置反映拓扑，非零元素值反映边权重；
  - 拉普拉斯矩阵\(L = D - A\)：反映图的连通性（如特征值 0 的数量等于连通分量数），是频率分析的核心；
  - 属性矩阵X：将节点 / 边属性按行 / 列组织（如节点属性矩阵\(X = V^T\)），便于矩阵运算；
- **应用**：通过矩阵分解（如 SVD）对属性矩阵降维，提取关键属性；通过拉普拉斯矩阵特征分解，分析图的频率特性。

##### （2）频率分析与卷积结合

- **核心逻辑**：借鉴传统信号处理的频率分析思想，将图信号分解为不同频率成分，通过卷积滤波提取有用频率，提升特征表达能力；
- **关键概念**：
  - 图信号频率：拉普拉斯矩阵的特征值\(\lambda_i\)对应频率，\(\lambda_i\)越小，频率越低（信号变化平缓）；
  - 频率滤波：通过谱图卷积（如第 39 页所述\(f *_{\mathcal{G}} g = U H_{\theta} U^T f\)）保留低频信号（全局趋势）、抑制高频信号（噪声），或反之；
- **优势**：
  - 全局特征提取：通过低频滤波捕捉图的全局拓扑（如分子的环状结构、社交网络的社区划分）；
  - 局部特征增强：通过高频滤波突出局部细节（如分子中的活性位点、图像中的边缘像素）；
- **挑战**：
  - 计算复杂度高：拉普拉斯矩阵特征分解时间复杂度为\(O(N^3)\)，不适用于大规模图；
  - 图依赖性：特征向量U与具体图结构绑定，泛化能力差。

#### 3. 未来探索方向

- **高效谱图卷积**：设计无需特征分解的谱图卷积变种（如 ChebNet、GCN），降低计算复杂度；
- **图与 Transformer 结合**：引入自注意力机制捕捉全局拓扑（如 Graph Transformer），同时保留频率分析能力；
- **动态图频率分析**：针对动态图（如时序社交网络），设计时变拉普拉斯矩阵，分析频率随时间的变化，提升动态任务（如时序边预测）性能。

**专业术语解释**

- **图信号（Graph Signal）**：定义在图节点上的向量，可表示节点属性（如分子中原子的电负性、社交网络中用户的活跃度），是图频率分析和卷积操作的处理对象，类比传统信号处理中的时域信号。
- **谱图卷积变种（Spectral GCN Variants）**：为解决传统谱图卷积计算复杂问题而提出的改进模型，如 ChebNet（用切比雪夫多项式近似谱图卷积）、GCN（简化 ChebNet 至一阶近似），将计算复杂度从\(O(N^3)\)降至\(O(E)\)（E为边数），适配大规模图。

### 页码 45：Recall: Laplacian in 2D shape（回顾：二维形状的拉普拉斯算子）

- **核心目标**：通过回顾二维欧氏空间（如图像）的拉普拉斯算子，建立 “传统信号拉普拉斯” 与 “图拉普拉斯” 的关联，为图频率分析奠定基础。

#### 1. 二维空间的拉普拉斯算子

- **定义**：二维拉普拉斯算子是二阶偏导数的和，用于描述函数在空间中的 “曲率” 或 “变化率的变化率”，公式为：

  \(\Delta f = \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}\)

  其中\(f(x,y)\)为二维空间中的函数（如图像的亮度函数），\(\frac{\partial^2 f}{\partial x^2}\)为 x 方向的二阶偏导数，\(\frac{\partial^2 f}{\partial y^2}\)为 y 方向的二阶偏导数。

- **离散近似（图像中的应用）**：

  - 在数字图像中，连续偏导数用离散差分近似，x 方向二阶差分公式为：

    

    \(\frac{\partial^2 f}{\partial x^2} \approx f(x+1,y) - 2f(x,y) + f(x-1,y)\)

  - y 方向二阶差分公式类似，因此二维离散拉普拉斯算子为：

    

    \(\Delta f(x,y) \approx [f(x+1,y) + f(x-1,y) + f(x,y+1) + f(x,y-1)] - 4f(x,y)\)

  - **含义**：衡量像素\(f(x,y)\)与其 4 邻域像素的差异，差异越大（如边缘像素），拉普拉斯值越大；差异越小（如平坦区域像素），拉普拉斯值越小。

#### 2. 二维拉普拉斯的频率特性

- **频率含义**：拉普拉斯算子对高频信号（如图像边缘、噪声）敏感，对低频信号（如平坦区域、缓慢变化的纹理）不敏感；
- **示例**：
  - 低频信号：图像中大面积的蓝色天空，像素值变化平缓，拉普拉斯值接近 0；
  - 高频信号：图像中物体的边缘（如黑白交界处），像素值急剧变化，拉普拉斯值显著不为 0；
- **频率响应函数**：在频域中，拉普拉斯算子的频率响应函数为\(f(\omega_x, \omega_y) = -(\omega_x^2 + \omega_y^2)\)，其中\(\omega_x, \omega_y\)为 x、y 方向的频率：
  - 频率为 0（\(\omega_x = \omega_y = 0\)）：\(f(\omega_x, \omega_y) = 0\)，对低频信号无响应；
  - 频率增大（\(\omega_x, \omega_y\)增大）：\(f(\omega_x, \omega_y)\)绝对值增大，对高频信号响应增强。

#### 3. 与图拉普拉斯的关联

- **核心类比**：
  - 二维空间：拉普拉斯算子衡量像素与其邻域的差异，反映信号的局部变化；
  - 图空间：图拉普拉斯矩阵衡量节点与其邻居的差异，反映图信号的局部变化；
- **数学关联**：
  - 二维离散拉普拉斯：\(\Delta f(x,y) = \sum_{(x',y') \in \mathcal{N}(x,y)} [f(x,y) - f(x',y')]\)（4 邻域）；
  - 图拉普拉斯：\(\Delta f_i = \sum_{j \in \mathcal{N}(i)} [f_i - f_j]\)（节点i的邻居集合\(\mathcal{N}(i)\)）；
  - **共性**：均通过 “中心元素与邻居元素的差异和” 衡量局部变化，差异和越大，信号频率越高。

#### 4. 关键结论

- 二维拉普拉斯算子是图拉普拉斯矩阵在 “规则网格图”（如图像像素网格）上的特例；
- 图拉普拉斯矩阵是二维拉普拉斯算子在 “非规则图”（如社交网络、分子图）上的推广；
- 两者的核心作用一致：捕捉信号的局部变化，为频率分析和滤波提供工具。

**专业术语解释**

- **离散差分（Discrete Difference）**：在数字信号处理中，用相邻离散点的差值近似连续导数的方法，是将连续数学算子（如拉普拉斯）应用于离散数据（如图像、图）的关键技术，确保算子可计算。
- **频率响应函数（Frequency Response Function）**：描述算子在频域中对不同频率信号的响应强度，拉普拉斯算子的频率响应函数表明其对高频信号的增强作用，是边缘检测、噪声去除等任务的理论基础。

### 页码 46：Building Laplacian Matrix for Graphs（构建图的拉普拉斯矩阵）

- **图拉普拉斯矩阵的核心构建流程**：基于图的邻接矩阵（A）和度矩阵（D），通过\(L = D - A\)计算得到，是图频率分析和谱图卷积的基础。

#### 1. 核心矩阵定义

##### （1）邻接矩阵（Adjacency Matrix, A）

- **定义**：\(N \times N\)矩阵（N为节点数），若节点i与节点j直接相连（无向图），则\(A[i][j] = 1\)（或边权重）；否则\(A[i][j] = 0\)；
- **特性**：
  - 无向图：A为对称矩阵（\(A[i][j] = A[j][i]\)）；
  - 有向图：A非对称（\(A[i][j] \neq A[j][i]\)，如\(A[i][j] = 1\)表示\(i \to j\)的边，\(A[j][i] = 0\)表示无\(j \to i\)的边）；
  - 自环（节点连接自身）：\(A[i][i] = 1\)，否则为 0（多数图任务中无自环，\(A[i][i] = 0\)）；
- **示例**：8 节点无向图的邻接矩阵A，\(A[1][2] = 1\)（节点 1 与 2 相连），\(A[1][3] = 0\)（节点 1 与 3 不相连）。

##### （2）度矩阵（Degree Matrix, D）

- **定义**：\(N \times N\)对角矩阵，对角元素\(D[i][i] = \sum_{j=1}^N A[i][j]\)（节点i的度，即邻居数量或边权重和），非对角元素均为 0；
- **特性**：
  - 无向图：节点i的度等于入射边数量（如节点 2 有 3 个邻居，则\(D[2][2] = 3\)）；
  - 有向图：分为入度矩阵（\(D_{in}[i][i] = \sum_j A[j][i]\)）和出度矩阵（\(D_{out}[i][i] = \sum_j A[i][j]\)）；
- **示例**：8 节点无向图的度矩阵D，节点 3 有 4 个邻居，则\(D[3][3] = 4\)；节点 8 有 1 个邻居，则\(D[8][8] = 1\)。

##### （3）拉普拉斯矩阵（Laplacian Matrix, L）

- **定义**：\(L = D - A\)，即度矩阵减去邻接矩阵；
- **示例**：8 节点无向图的拉普拉斯矩阵L，\(L[1][1] = D[1][1] - A[1][1] = 1 - 0 = 1\)，\(L[1][2] = 0 - A[1][2] = -1\)，\(L[2][1] = 0 - A[2][1] = -1\)，\(L[2][2] = D[2][2] - A[2][2] = 3 - 0 = 3\)，符合\(L[i][j] = \begin{cases} D[i][i] & i=j \\ -A[i][j] & i \neq j \end{cases}\)。

#### 2. 图拉普拉斯矩阵的关键性质

- **行和为 0**：对任意节点i，\(\sum_{j=1}^N L[i][j] = D[i][i] - \sum_{j=1}^N A[i][j] = 0\)，表明拉普拉斯矩阵的每行元素之和为 0；
- **实对称矩阵**：无向图的A和D均为对称矩阵，因此\(L = D - A\)也为实对称矩阵，可正交对角化（\(L = U \Lambda U^T\)，\(U^T = U^{-1}\)）；
- **半正定矩阵**：所有特征值非负（\(\lambda_i \geq 0\)），最小特征值\(\lambda_0 = 0\)，对应特征向量为全 1 向量（\(U_0 = [1,1,...,1]^T\)）；
- **连通分量与特征值**：若图有k个连通分量（子图间无连接），则拉普拉斯矩阵有k个特征值为 0，对应每个连通分量的全 1 特征向量。

#### 3. 拉普拉斯矩阵的物理含义

- **局部平滑度**：对图信号f，\(f^T L f = \frac{1}{2} \sum_{i,j=1}^N A[i][j] (f_i - f_j)^2\)，衡量图信号的 “平滑程度”：
  - \(f^T L f\)小：信号平滑（相邻节点特征相似，如同一社区的用户兴趣）；
  - \(f^T L f\)大：信号粗糙（相邻节点特征差异大，如边缘节点）；
- **示例**：社交网络中，用户兴趣信号f的\(f^T L f\)小，表明兴趣相似的用户多为好友，符合社交网络的 “同质性” 特性。

**专业术语解释**

- **正交对角化（Orthogonal Diagonalization）**：实对称矩阵的重要性质，指存在正交矩阵U（\(U^T = U^{-1}\)）和对角矩阵\(\Lambda\)，使\(L = U \Lambda U^T\)，其中\(\Lambda\)的对角元素为L的特征值，U的列向量为对应的正交特征向量，是谱图分析的数学基础。
- **半正定矩阵（Positive Semi-Definite Matrix）**：所有特征值非负的实对称矩阵，图拉普拉斯矩阵的半正定性确保其特征值可用于表示频率（非负），且二次型\(f^T L f \geq 0\)，符合平滑度的非负特性。

### 页码 47：Building Laplacian Matrix for Graphs（构建图的拉普拉斯矩阵）（续）

#### 4. 具体构建示例（8 节点无向图）

- **步骤 1：确定图结构**

  8 节点无向图的连接关系：

  - 节点 1：连接节点 2；
  - 节点 2：连接节点 1、3、6；
  - 节点 3：连接节点 2、4、6、7；
  - 节点 4：连接节点 3、5；
  - 节点 5：连接节点 4、6；
  - 节点 6：连接节点 2、3、5、7；
  - 节点 7：连接节点 3、6、8；
  - 节点 8：连接节点 7。

- **步骤 2：构建邻接矩阵A**A为 8×8 对称矩阵，非零元素位置对应节点连接：

  \(A = \begin{pmatrix}  0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\  0 & 1 & 0 & 1 & 0 & 1 & 1 & 0 \\  0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\  0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 \\  0 & 1 & 1 & 0 & 1 & 0 & 1 & 0 \\  0 & 0 & 1 & 0 & 0 & 1 & 0 & 1 \\  0 & 0 & 0 & 0 & 0 & 0 & 1 & 0  \end{pmatrix}\)

- **步骤 3：构建度矩阵D**D为 8×8 对角矩阵，对角元素为节点度：

  \(D = \begin{pmatrix}  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 3 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 4 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 2 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 2 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 4 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 3 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 1  \end{pmatrix}\)

  （如节点 2 的度为 3，\(D[2][2] = 3\)；节点 3 的度为 4，\(D[3][3] = 4\)）

- **步骤 4：计算拉普拉斯矩阵\(L = D - A\)**

  逐元素计算\(L[i][j] = D[i][j] - A[i][j]\)，结果为：

  \(L = \begin{pmatrix}  1 & -1 & 0 & 0 & 0 & 0 & 0 & 0 \\  -1 & 3 & -1 & 0 & 0 & -1 & 0 & 0 \\  0 & -1 & 4 & -1 & 0 & -1 & -1 & 0 \\  0 & 0 & -1 & 2 & -1 & 0 & 0 & 0 \\  0 & 0 & 0 & -1 & 2 & -1 & 0 & 0 \\  0 & -1 & -1 & 0 & -1 & 4 & -1 & 0 \\  0 & 0 & -1 & 0 & 0 & -1 & 3 & -1 \\  0 & 0 & 0 & 0 & 0 & 0 & -1 & 1  \end{pmatrix}\)

  - 对角元素：\(L[i][i] = D[i][i]\)（如\(L[2][2] = 3\)）；
  - 非对角元素：\(L[i][j] = -A[i][j]\)（如\(L[2][1] = -1\)，因\(A[2][1] = 1\)）；
  - 行和验证：任意行元素之和为 0（如第 2 行：\(-1 + 3 -1 + 0 + 0 -1 + 0 + 0 = 0\)），符合拉普拉斯矩阵性质。

#### 5. 图拉普拉斯算子的离散形式

- **节点i的拉普拉斯值**：对图信号f（节点特征向量），节点i的拉普拉斯值为：

  

  \(\Delta f_i = \sum_{j \in \mathcal{N}(i)} (f_i - f_j) = D[i][i] f_i - \sum_{j \in \mathcal{N}(i)} f_j\)

  

  - 含义：衡量节点i与其所有邻居节点的特征差异之和，差异越大，\(\Delta f_i\)越大；

- **向量形式**：所有节点的拉普拉斯值构成向量\(\Delta f = L f\)，即：

  

  \(\begin{pmatrix}  \Delta f_1 \\  \Delta f_2 \\  \vdots \\  \Delta f_N  \end{pmatrix}  =  L \begin{pmatrix}  f_1 \\  f_2 \\  \vdots \\  f_N  \end{pmatrix}\)

  

  - 示例：8 节点图中，若\(f = [1,2,3,4,5,6,7,8]^T\)，则\(\Delta f_2 = L[2] \cdot f = (-1)×1 + 3×2 + (-1)×3 + 0×4 + 0×5 + (-1)×6 + 0×7 + 0×8 = -1 + 6 -3 -6 = -4\)，反映节点 2 与其邻居的特征差异。

#### 6. 关键结论

- 图拉普拉斯矩阵的构建依赖于图的拓扑结构（邻接矩阵）和节点连接度（度矩阵），无属性依赖，适用于任意无向图；
- 拉普拉斯矩阵的元素值直接反映节点间的连接关系和差异，是图信号频率分析、谱图卷积的核心工具；
- 实际应用中，可通过网络分析库（如 NetworkX、DGL）自动构建拉普拉斯矩阵，无需手动计算，提升效率。

**专业术语解释**

- **图信号的拉普拉斯变换（Laplacian Transform of Graph Signal）**：将图信号f通过拉普拉斯矩阵映射为\(\Delta f = L f\)，得到信号的局部变化向量，是图信号预处理、频率分析的基础操作，类比传统信号的拉普拉斯变换。
- **网络分析库（Network Analysis Libraries）**：用于图数据处理的工具库，如 Python 的 NetworkX 可通过`nx.laplacian_matrix(G)`直接计算图G的拉普拉斯矩阵，DGL（Deep Graph Library）可在深度学习框架中高效处理图数据及拉普拉斯矩阵，简化 GNN 的开发流程。



### 页码 48：Laplacian Matrix as an Operator（作为算子的拉普拉斯矩阵）

- **核心视角**：将图拉普拉斯矩阵视为 “图信号的变换算子”，通过分析其对图信号的作用，理解其在频率分析、平滑度衡量中的核心价值。

#### 1. 拉普拉斯矩阵的算子定义

- **数学表达**：对任意图信号 \(f \in \mathbb{R}^N\)（N 为节点数），拉普拉斯算子的作用的结果为：

  \(h = L f = (D - A) f = D f - A f\)

  - D f：度矩阵与信号的乘积，每个元素为 \(D[i][i] \cdot f[i]\)（节点度加权的信号值）；
  - A f：邻接矩阵与信号的乘积，每个元素为 \(\sum_{j \in \mathcal{N}(i)} f[j]\)（邻居节点信号的求和）；
  - 最终元素：对节点 i，\(h[i] = D[i][i] f[i] - \sum_{j \in \mathcal{N}(i)} f[j] = \sum_{j \in \mathcal{N}(i)} (f[i] - f[j])\)，即节点 i 与所有邻居的信号差异之和。

- **物理含义**：

  - \(h[i] > 0\)：节点 i 的信号值大于多数邻居，为 “信号高峰”；
  - \(h[i] < 0\)：节点 i 的信号值小于多数邻居，为 “信号低谷”；
  - \(h[i] = 0\)：节点 i 与邻居的信号值平均相等，信号平滑。

- **示例**：社交网络中，用户活跃度信号 f 中，高活跃度用户（\(f[i]\) 大）的 \(h[i]\) 可能为正，低活跃度用户的 \(h[i]\) 可能为负，反映用户间的活跃度差异。

#### 2. 拉普拉斯二次型：总变差（Total Variation）

- **定义**：图信号 f 的总变差通过拉普拉斯二次型定义，公式为：

  \(f^T L f = \frac{1}{2} \sum_{i,j=1}^N A[i][j] (f[i] - f[j])^2\)

- **推导过程**：

  \(\begin{align*}  f^T L f &= f^T (D - A) f = f^T D f - f^T A f \\  &= \sum_{i=1}^N D[i][i] f[i]^2 - \sum_{i,j=1}^N f[i] A[i][j] f[j] \\  &= \frac{1}{2} \left( \sum_{i,j=1}^N A[i][j] f[i]^2 - 2 \sum_{i,j=1}^N f[i] A[i][j] f[j] + \sum_{i,j=1}^N A[i][j] f[j]^2 \right) \\  &= \frac{1}{2} \sum_{i,j=1}^N A[i][j] (f[i] - f[j])^2  \end{align*}\)

  - 关键步骤：利用 \(D[i][i] = \sum_j A[i][j]\)（节点度等于邻居数），将对角项展开为邻接矩阵的求和形式，最终化简为邻居信号差异的平方和。

- **总变差的意义**：

  - **信号平滑度**：\(f^T L f\) 越小，信号越平滑（相邻节点差异小）；反之，信号越粗糙（相邻节点差异大）；
  - **频率关联**：总变差与图信号的频率正相关，\(f^T L f\) 小对应低频信号（全局趋势），\(f^T L f\) 大对应高频信号（局部细节 / 噪声）；

- **示例**：

  - 低频信号：分子图中所有原子的 “电负性” 信号平缓，\(f^T L f\) 小；
  - 高频信号：图像像素的 “边缘” 信号（黑白交界处），\(f^T L f\) 大。

#### 3. 低频与高频图信号的对比

| 信号类型 | 总变差（\(f^T L f\)） | 信号特征                           | 示例                                             |
| -------- | --------------------- | ---------------------------------- | ------------------------------------------------ |
| 低频信号 | 小                    | 相邻节点特征相似，全局变化平缓     | 社交网络中同一社区用户的 “兴趣标签” 信号         |
| 高频信号 | 大                    | 相邻节点特征差异显著，局部变化剧烈 | 分子图中 “活性位点” 与周围原子的 “反应活性” 信号 |

**专业术语解释**

- **二次型（Quadratic Form）**：由矩阵和向量构成的二次函数，图拉普拉斯二次型 \(f^T L f\) 是衡量图信号平滑度的核心指标，其值非负（因 L 为半正定矩阵），为频率分析提供了量化工具。
- **总变差（Total Variation）**：源于信号处理，在图信号中表示信号的 “整体波动程度”，总变差越小，信号越稳定，是图信号去噪、压缩的关键衡量标准。

### 页码 49：Eigen-decomposition of Laplacian Matrix（拉普拉斯矩阵的特征分解）

- **特征分解的核心价值**：将拉普拉斯矩阵分解为特征向量和特征值，为图信号的频率分析、谱图卷积提供数学基础，是谱图理论的核心内容。

#### 1. 拉普拉斯矩阵的特征分解形式

- **实对称矩阵的优势**：无向图的拉普拉斯矩阵 L 为实对称矩阵，根据线性代数理论，可通过正交矩阵进行正交相似对角化：

  

  \(L = U \Lambda U^T\)

  

  - U：\(N \times N\) 正交矩阵，列向量为 L 的正交特征向量（\(U[:,i] = u_i\)），满足 \(U^T U = I\)（I 为单位矩阵）、\(U^{-1} = U^T\)；
  - \(\Lambda\)：\(N \times N\) 对角矩阵，对角元素为 L 的特征值（\(\Lambda[i][i] = \lambda_i\)），按非降序排列（\(0 = \lambda_0 \leq \lambda_1 \leq \dots \leq \lambda_{N-1}\)）。

#### 2. 拉普拉斯矩阵的特征值特性

- **最小特征值 \(\lambda_0 = 0\)**：
  - 对应特征向量 \(u_0 = [1, 1, \dots, 1]^T\)（全 1 向量），验证：\(L u_0 = (D - A) u_0 = D u_0 - A u_0 = D \cdot 1 - A \cdot 1 = D \cdot 1 - D \cdot 1 = 0 = \lambda_0 u_0\)；
  - 物理含义：全 1 信号为最平滑的低频信号，总变差 \(u_0^T L u_0 = 0\)，对应频率为 0。
- **特征值非负**：因 L 为半正定矩阵，所有特征值 \(\lambda_i \geq 0\)，确保频率非负（符合信号处理的频率定义）。
- **连通分量与特征值**：
  - 若图有 k 个连通分量（子图间无连接），则 L 有 k 个特征值为 0，每个特征值对应一个连通分量的全 1 特征向量；
  - 示例：两个独立的 4 节点子图构成的 8 节点图，L 有 2 个特征值为 0，分别对应两个子图的全 1 向量。
- **特征值的频率意义**：
  - 特征值越小，对应特征向量的频率越低（信号越平滑）；
  - 特征值越大，对应特征向量的频率越高（信号变化越剧烈）；
- **示例**：\(\lambda_1\)（第二小特征值）称为 “代数连通度”，反映图的连通紧密程度，\(\lambda_1\) 越大，图的连通性越强。

#### 3. 特征分解的关键应用

- **图信号的频率分解**：任意图信号 f 可表示为特征向量的线性组合（图傅里叶级数）：\(f = \sum_{i=0}^{N-1} \hat{f}_i u_i\)，其中 \(\hat{f}_i = u_i^T f\) 为傅里叶系数（频率域表示）；
- **谱图卷积**：基于特征分解实现频域滤波（如第 39 页所述 \(f *_{\mathcal{G}} g = U (U^T f \odot U^T g) U^T\)）；
- **图聚类**：利用特征向量（如前 k 个最小特征值对应的特征向量）进行谱聚类，将节点分为 k 个簇（如社区检测）。

**专业术语解释**

- **正交相似对角化（Orthogonal Similarity Diagonalization）**：实对称矩阵的特有分解方式，正交矩阵 U 确保特征向量正交，简化后续计算（如逆变换仅需转置），是谱图分析的数学基础。
- **代数连通度（Algebraic Connectivity）**：拉普拉斯矩阵的第二小特征值 \(\lambda_1\)，衡量图的连通性，\(\lambda_1 > 0\) 表示图连通，\(\lambda_1 = 0\) 表示图不连通，\(\lambda_1\) 越大，图的抗干扰能力越强（如删除少量边仍保持连通）。

### 页码 50：Eigenvectors as Graph Signals（作为图信号的特征向量）

- **核心逻辑**：拉普拉斯矩阵的特征向量本身就是一种特殊的图信号，其频率由对应的特征值决定，通过分析特征向量的分布的模式，可直观理解图信号的频率特性。

#### 1. 特征向量与频率的关联

- **数学推导**：对特征向量 \(u_i\)，其满足 \(L u_i = \lambda_i u_i\)，两边同时左乘 \(u_i^T\)（因 \(u_i\) 正交且归一化，\(u_i^T u_i = 1\)）：

  

  \(u_i^T L u_i = u_i^T \lambda_i u_i = \lambda_i\)

  

  - 结合拉普拉斯二次型的定义 \(u_i^T L u_i = \frac{1}{2} \sum_{j,k=1}^N A[j][k] (u_i[j] - u_i[k])^2\)，可得：

    

    \(\lambda_i = \frac{1}{2} \sum_{j,k=1}^N A[j][k] (u_i[j] - u_i[k])^2\)

  - 结论：特征值 \(\lambda_i\) 等于特征向量 \(u_i\) 作为图信号的总变差（乘以 1/2），直接反映特征向量的频率 ——\(\lambda_i\) 越大，特征向量的频率越高。

#### 2. 特征向量的可视化与频率特性

- **低频特征向量（小 \(\lambda_i\)）**：
  - 信号特点：相邻节点的特征向量值差异小，全局分布平缓，无明显正负交替；
  - 示例：\(\lambda_0 = 0\) 对应的特征向量 \(u_0 = [1,1,...,1]^T\)，所有节点值相同，无变化；\(\lambda_1 = 0.04\) 对应的特征向量，节点值在小范围内轻微波动（如社区 1 的节点值为 0.2，社区 2 的节点值为 0.1）；
  - 颜色表示：若用蓝色表示正值、红色表示负值，低频特征向量的颜色分布均匀，无频繁正负交替。
- **高频特征向量（大 \(\lambda_i\)）**：
  - 信号特点：相邻节点的特征向量值差异大，局部变化剧烈，正负交替频繁；
  - 示例：\(\lambda = 1.49\) 对应的特征向量，节点值在相邻节点间频繁切换（如节点 1 为 0.8，节点 2 为 - 0.7，节点 3 为 0.6）；
  - 颜色表示：颜色在相邻节点间频繁切换（蓝→红→蓝→红），直观反映高频信号的剧烈变化。

#### 3. 特征向量的应用价值

- **图信号的频率滤波**：利用不同频率的特征向量，可对图信号进行滤波（如保留低频特征向量，去除高频噪声）；
- **节点嵌入（Node Embedding）**：将节点映射到特征向量空间（如取前 k 个特征向量作为节点嵌入），使相似节点（拓扑或属性相似）在嵌入空间中距离相近；
- **图分类**：对图级任务，将所有节点的特征向量进行全局池化（如均值），得到图的频率特征，用于分类（如分子毒性预测）。

**专业术语解释**

- **节点嵌入（Node Embedding）**：将图中的节点映射到低维向量空间的技术，特征向量嵌入是早期常用方法（如取前 50 个特征向量作为节点嵌入），后续被 DeepWalk、Node2Vec 等基于随机游走的方法替代，但在谱图分析中仍有重要应用。
- **频率滤波（Frequency Filtering）**：在图信号中保留特定频率范围的信号，去除其他频率的操作，如低通滤波（保留 \(\lambda_i < \tau\) 的特征向量）用于去噪，高通滤波（保留 \(\lambda_i > \tau\) 的特征向量）用于增强局部细节。

### 页码 51：Graph Fourier Transform (GFT) and IGFT（图傅里叶变换与逆变换）

- **核心目标**：类比传统信号处理的傅里叶变换，定义图信号的傅里叶变换（GFT）与逆变换（IGFT），实现图信号在 “节点域” 与 “谱域”（频率域）之间的转换，为谱图卷积奠定基础。

#### 1. 图傅里叶变换（GFT）

- **定义**：图信号 \(f \in \mathbb{R}^N\) 的傅里叶变换（谱域表示）为 \(\hat{f} \in \mathbb{R}^N\)，每个元素（傅里叶系数）定义为：

  

  \(\hat{f}_i = u_i^T f\)

  

  - \(u_i\)：拉普拉斯矩阵 L 的第 i 个特征向量（图傅里叶基）；
  - 向量形式：\(\hat{f} = U^T f\)，其中 U 为特征向量矩阵；

- **物理含义**：

  - \(\hat{f}_i\) 表示图信号 f 在第 i 个频率分量（特征向量 \(u_i\)）上的 “投影系数”；
  - \(\hat{f}_i\) 越大，说明 f 包含该频率分量的比例越高；

- **示例**：若 f 为低频信号（如全 1 向量），则 \(\hat{f}_0 = u_0^T f = N\)（因 \(u_0 = [1/\sqrt{N}, ..., 1/\sqrt{N}]^T\)），其他 \(\hat{f}_i \approx 0\)，表明 f 主要由低频分量构成。

#### 2. 逆图傅里叶变换（IGFT）

- **定义**：谱域信号 \(\hat{f} \in \mathbb{R}^N\) 的逆傅里叶变换（节点域重构）为 \(f \in \mathbb{R}^N\)，公式为：

  

  \(f = U \hat{f}\)

  

  - 展开形式（图傅里叶级数）：\(f = \sum_{i=0}^{N-1} \hat{f}_i u_i\)，即图信号可表示为特征向量（傅里叶基）的线性组合，组合系数为傅里叶系数；

- **物理含义**：从谱域的频率分量重构节点域的原始信号，验证了 “任意图信号均可分解为不同频率的特征向量”；

- **示例**：若 \(\hat{f} = [5, 0, 0, ..., 0]^T\)（仅低频分量），则 \(f = 5 u_0 = [5/\sqrt{N}, ..., 5/\sqrt{N}]^T\)，重构为低频信号。

#### 3. GFT 与 IGFT 的核心性质

- **线性性**：GFT 与 IGFT 均为线性变换，即 \(\text{GFT}(a f + b g) = a \text{GFT}(f) + b \text{GFT}(g)\)，\(\text{IGFT}(a \hat{f} + b \hat{g}) = a \text{IGFT}(\hat{f}) + b \text{IGFT}(\hat{g})\)（\(a,b\) 为常数）；
- **正交性**：因 \(U^T U = I\)，GFT 与 IGFT 互为逆操作，即 \(\text{IGFT}(\text{GFT}(f)) = f\)，确保信号重构无损失；
- **能量守恒**：图信号的 “能量”（\(\|f\|^2 = f^T f\)）在变换前后守恒，即 \(\|f\|^2 = \|\hat{f}\|^2\)（因 \(f^T f = (U \hat{f})^T (U \hat{f}) = \hat{f}^T U^T U \hat{f} = \hat{f}^T \hat{f}\)）。

#### 4. GFT 的应用场景

- **谱图卷积**：基于 GFT 实现图信号的卷积（如第 39 页所述 \(f *_{\mathcal{G}} g = U (U^T f \odot U^T g) U^T\)）；
- **图信号去噪**：在谱域中抑制高频分量（噪声），保留低频分量（有用信号），再通过 IGFT 重构去噪后的信号；
- **图压缩**：在谱域中保留幅值大的傅里叶系数（主要频率分量），丢弃幅值小的系数，实现信号压缩。

**专业术语解释**

- **傅里叶基（Fourier Basis）**：图傅里叶变换的基向量，即拉普拉斯矩阵的特征向量 \(u_0, u_1, ..., u_{N-1}\)，类比传统傅里叶变换的正弦 / 余弦基，不同特征向量对应不同频率的基信号。
- **能量守恒（Energy Conservation）**：图信号的能量在节点域和谱域中保持不变，确保 GFT 不会丢失信号的能量信息，是信号重构、去噪的重要理论保障。

### 页码 52：What can we do with GFT?（图傅里叶变换的应用）

- **核心应用**：基于 GFT 设计谱图卷积（Spectral Graph CNNs），实现图信号的频率域滤波，解决传统 GNN 无法利用频率信息的问题。

#### 1. 谱图卷积的定义（基于卷积定理）

- **传统卷积定理**：时域卷积等价于频域乘积，即 \(f * g = \mathcal{F}^{-1}(\mathcal{F}(f) \odot \mathcal{F}(g))\)；

- **图卷积定理**：类比传统卷积，图信号 f 与滤波器 h 的谱图卷积定义为：

  

  \(h *_{\mathcal{G}} f = U \left( U^T h \odot U^T f \right)\)

  

  - 符号说明：
    - U：拉普拉斯矩阵的特征向量矩阵（GFT 基）；
    - \(U^T h\)：滤波器 h 的 GFT（谱域表示）；
    - \(U^T f\)：图信号 f 的 GFT（谱域表示）；
    - \(\odot\)：元素乘法（频域滤波，滤波器与信号的谱域乘积）；
    - \(U (\cdot)\)：IGFT（谱域转回节点域）；

- **简化形式**：若定义滤波器的谱域表示为对角矩阵 \(h_{\theta} = \text{diag}(U^T h)\)（\(\theta\) 为可学习参数），则谱图卷积可简化为：

  

  \(h *_{\mathcal{G}} f = U h_{\theta} U^T f\)

  

  - 核心：谱图卷积的关键是设计滤波器 \(h_{\theta}\)，通过学习 \(h_{\theta}\) 的对角元素，实现对不同频率信号的滤波（如保留低频、抑制高频）。

#### 2. 谱图卷积的核心挑战：滤波器设计

- **传统谱图卷积的局限**：
  - 滤波器依赖图结构：U 与具体图的拉普拉斯矩阵绑定，不同图的 U 不同，导致滤波器无法泛化到新图；
  - 计算复杂度高：特征向量矩阵 U 的计算复杂度为 \(O(N^3)\)，不适用于大规模图（\(N > 10000\)）；
  - 仅支持无向图：U 需拉普拉斯矩阵实对称（无向图），无法处理有向图或动态图。
- **滤波器设计的改进方向**：
  - **参数化滤波器**：将 \(h_{\theta}\) 设计为特征值 \(\lambda_i\) 的函数（如多项式函数 \(h_{\theta}(\lambda_i) = \theta_0 + \theta_1 \lambda_i + \dots + \theta_k \lambda_i^k\)），减少参数数量，提升泛化能力；
  - **无特征向量依赖**：通过近似方法（如切比雪夫多项式）避免直接计算 U，降低计算复杂度（如 ChebNet 将复杂度降至 \(O(E)\)，E 为边数）。

#### 3. 谱图卷积的应用示例

- **图信号去噪**：设计低通滤波器 \(h_{\theta}\)（对 \(\lambda_i < \tau\) 的频率分量，\(h_{\theta}(\lambda_i) = 1\)；对 \(\lambda_i > \tau\)，\(h_{\theta}(\lambda_i) = 0\)），抑制高频噪声；
- **节点分类**：通过谱图卷积提取节点的频率特征（如融合低频全局特征和高频局部特征），输入分类器预测节点类别；
- **分子性质预测**：对分子图，通过谱图卷积提取分子的频率特征（如环状结构对应低频，活性位点对应高频），预测分子的溶解度、毒性等属性。

**专业术语解释**

- **参数化滤波器（Parameterized Filter）**：将谱图卷积的滤波器表示为可学习参数的函数（如多项式、指数函数），使滤波器不依赖具体图结构，可泛化到不同图，是谱图 CNN（如 GCN）的核心设计思路。
- **低通滤波器（Low-Pass Filter）**：保留低频信号、抑制高频信号的滤波器，在图信号中常用于去噪（高频对应噪声）、提取全局趋势（如分子的整体极性）。

### 页码 53：Spectral Graph CNNs --- Spectral Networks（谱图卷积神经网络 —— 谱网络）

- **谱网络的核心定位**：早期的谱图 CNN 模型，直接基于谱图卷积定义，为后续 GCN、ChebNet 等模型奠定基础，但存在计算复杂、泛化能力差的局限。

#### 1. 谱网络的定义与结构

- **滤波器简化**：将谱图卷积的滤波器 \(h_{\theta}\) 定义为对角矩阵 \(W = \text{diag}(U^T h)\)，则谱图卷积简化为：

  

  \(h *_{\mathcal{G}} f = U W U^T f\)

  

  - W 为可学习参数矩阵，对角元素对应不同频率的滤波权重（\(W[i][i]\) 控制第 i 个频率分量的保留比例）；

- **网络结构**：

  - 输入层：原始图信号 \(f \in \mathbb{R}^N\)（节点属性向量）；
  - 谱卷积层：\(f_1 = \sigma(U W_1 U^T f + b_1)\)（\(\sigma\) 为激活函数，如 ReLU，\(b_1\) 为偏置）；
  - 全连接层：\(f_2 = \sigma(W_2 f_1 + b_2)\)；
  - 输出层：\(\hat{y} = \text{Softmax}(W_3 f_2 + b_3)\)（节点分类任务）；

- **示例**：节点分类任务中，输入节点属性 f，通过谱卷积层提取频率特征，全连接层整合特征，输出层预测节点类别。

#### 2. 谱网络的局限性

- **图依赖性强**：滤波器 W 依赖特征向量矩阵 U，而 U 与具体图结构绑定，训练好的模型无法泛化到其他图（如训练图为社交网络，测试图为分子图时，U 完全不同）；
- **仅支持无向图**：U 需拉普拉斯矩阵实对称（无向图特性），无法处理有向图（非对称拉普拉斯矩阵难以正交对角化）；
- **计算复杂度高**：特征向量矩阵 U 的计算复杂度为 \(O(N^3)\)，大规模图（\(N > 1000\)）难以处理；
- **内存占用大**：U 为 \(N \times N\) 矩阵，大规模图的 U 需占用 \(O(N^2)\) 内存（如 \(N=10000\) 时，U 需 100MB 以上内存）。

#### 3. 改进方向：突破特征向量依赖

- **核心问题**：如何避免直接计算 U，同时保留谱图卷积的频率滤波能力？
- **解决方案**：
  - **多项式近似**：将滤波器 W 表示为拉普拉斯矩阵 L 的多项式函数（如 \(W = \theta_0 I + \theta_1 L + \theta_2 L^2 + \dots + \theta_k L^k\)），利用 \(L = U \Lambda U^T\)，可得 \(W = U \text{diag}(\theta_0 + \theta_1 \lambda_i + \dots + \theta_k \lambda_i^k) U^T\)，无需直接计算 U；
  - **切比雪夫多项式近似**：用切比雪夫多项式 \(T_k(x)\) 近似 L 的多项式，进一步降低计算复杂度（如 ChebNet）；
- **关键突破**：多项式近似使谱图卷积不再依赖 U，仅需拉普拉斯矩阵 L，大幅提升计算效率和泛化能力。

**专业术语解释**

- **谱网络（Spectral Networks）**：首个谱图 CNN 模型，由 Bruna 等人于 2014 年提出，直接基于谱图卷积定义，验证了谱图理论在 GNN 中的可行性，但因计算复杂、泛化能力差，未广泛应用，但其思想为后续模型（如 GCN）提供了基础。
- **多项式近似（Polynomial Approximation）**：将复杂函数（如滤波器 W）表示为简单多项式的线性组合，在谱图卷积中用于避免特征向量计算，使滤波器仅依赖拉普拉斯矩阵 L，是谱图 CNN 走向实用的关键改进。

### 页码 54：Spectral graph CNNs --- Simplifying U（谱图卷积神经网络 —— 简化特征向量矩阵）

- **核心目标**：通过多项式近似简化谱图卷积，避免直接计算特征向量矩阵 U，降低计算复杂度，提升模型泛化能力。

#### 1. 多项式近似的原理

- **滤波器的多项式表示**：将谱图卷积的滤波器 \(\Theta(\Lambda)\)（对角矩阵）表示为特征值 \(\Lambda\) 的多项式函数：

  

  \(\Theta(\Lambda) = \sum_{k=0}^{K-1} \theta_k \Lambda^k\)

  

  - \(\theta_0, \dots, \theta_{K-1}\)：可学习参数（多项式系数）；
  - K：多项式阶数（控制滤波器的频率捕捉范围，K 越大，可捕捉的频率范围越广）；

- **谱图卷积的简化**：结合 \(L = U \Lambda U^T\)（\(L^k = U \Lambda^k U^T\)），谱图卷积可改写为：

  

  \(h *_{\mathcal{G}} f = U \Theta(\Lambda) U^T f = U \sum_{k=0}^{K-1} \theta_k \Lambda^k U^T f = \sum_{k=0}^{K-1} \theta_k L^k f\)

  

  - 关键简化：谱图卷积不再依赖 U，仅需计算 \(L^k f\)（拉普拉斯矩阵的 k 次幂与信号的乘积），大幅降低计算复杂度。

#### 2. 多项式近似的优势

- **泛化能力提升**：滤波器仅依赖拉普拉斯矩阵 L，不依赖具体图的 U，训练好的模型可泛化到不同结构的图（如从社交网络泛化到分子图）；
- **计算复杂度降低**：
  - 传统谱网络：计算 U 的复杂度为 \(O(N^3)\)；
  - 多项式近似：计算 \(L^k f\) 的复杂度为 \(O(k E)\)（E 为边数），k 通常取 3-5，大规模图可高效处理；
- **支持有向图扩展**：通过对有向图的拉普拉斯矩阵（如对称化处理）进行多项式近似，可扩展到有向图场景。

#### 3. 多项式阶数 K 的意义

- **频率捕捉范围**：K 决定滤波器可捕捉的 “多跳邻居” 范围：
  - \(K=1\)：仅捕捉 1 跳邻居信息（\(L^1 f\)），对应局部高频信号；
  - \(K=3\)：捕捉 3 跳邻居信息（\(L^1 f + L^2 f + L^3 f\)），兼顾局部与全局信号；
- **模型表达能力**：K 越大，滤波器的表达能力越强，但计算复杂度和过拟合风险也随之增加，实际应用中 K 通常取 3-5。

#### 4. 参考文献与理论基础

- Hammond 等人（2011）：提出基于谱图理论的图小波变换，为多项式近似提供理论基础；
- Defferrard 等人（2016）：在 NIPS 论文中提出基于多项式近似的谱图 CNN，验证了该方法的有效性，为后续 ChebNet、GCN 的提出奠定基础。

**专业术语解释**

- **多跳邻居范围（Multi-hop Neighbor Range）**：多项式阶数 K 对应滤波器可捕捉的邻居范围，\(L^k f\) 包含节点的 k 跳邻居信息，因此 K 越大，滤波器可整合的全局信息越多，是平衡局部与全局特征的关键参数。
- **图小波变换（Wavelets on Graphs）**：基于谱图理论的图信号变换方法，将传统小波变换推广到图结构，Hammond 等人的工作为谱图卷积的多项式近似提供了小波理论支持，使滤波器设计更具解释性。

### 页码 55：ChebNet --- Simplifying \(L^k\)（切比雪夫网络 —— 简化拉普拉斯矩阵幂）

- **ChebNet 的核心创新**：用切比雪夫多项式（Chebyshev Polynomial）近似拉普拉斯矩阵的幂 \(L^k\)，进一步降低计算复杂度，同时保证滤波器的频率捕捉能力，是谱图 CNN 的重要突破。

#### 1. 切比雪夫多项式近似的原理

- **切比雪夫多项式定义**：切比雪夫多项式 \(T_k(x)\) 是定义在区间 \([-1, 1]\) 上的正交多项式，递归公式为：

  \(T_0(x) = 1, \quad T_1(x) = x, \quad T_k(x) = 2x T_{k-1}(x) - T_{k-2}(x)\)

  - 优势：切比雪夫多项式具有良好的逼近性质，可用低阶多项式逼近复杂函数，且计算量小。

- **拉普拉斯矩阵的归一化**：因拉普拉斯矩阵 L 的特征值范围为 \([0, 2]\)（无向图），需归一化到切比雪夫多项式的定义域 \([-1, 1]\)，公式为：

  \(\tilde{L} = \frac{2}{\lambda_{max}} L - I\)

  - \(\lambda_{max}\)：L 的最大特征值（可通过快速幂迭代法估算，无需特征分解）；
  - I：单位矩阵；
  - 归一化后，\(\tilde{L}\) 的特征值范围为 \([-1, 1]\)，满足切比雪夫多项式的要求。

- **谱图卷积的切比雪夫近似**：将 \(L^k\) 用切比雪夫多项式 \(T_k(\tilde{L})\) 近似，谱图卷积改写为：

  \(h *_{\mathcal{G}} f \approx \sum_{k=0}^{K-1} \theta_k T_k(\tilde{L}) f\)

  - \(\theta_k\)：切比雪夫多项式的系数（可学习参数）；
  - K：多项式阶数（通常取 3-5）。

#### 2. ChebNet 的核心优势

- **计算复杂度大幅降低**：
  - 传统多项式近似：计算 \(L^k f\) 需 k 次矩阵乘法，复杂度 \(O(k E)\)；
  - ChebNet：通过递归公式计算 \(T_k(\tilde{L}) f\)，仅需 \(O(K E)\) 复杂度，且 K 小（3-5），大规模图可高效处理；
- **无需特征分解**：无论是 U 还是 \(L^k\)，均无需直接计算，仅需拉普拉斯矩阵 L 和最大特征值 \(\lambda_{max}\)（快速估算）；
- **滤波器的局部性**：ChebNet 的滤波器具有 K-hop 局部性，即节点 i 的卷积结果仅依赖其 K 跳邻居，符合 CNN 的局部感受野特性，提升模型解释性；
- **参数效率高**：仅需 K 个参数（\(\theta_0 \dots \theta_{K-1}\)），远少于传统谱网络的 N 个参数，降低过拟合风险。

#### 3. ChebNet 的应用与影响

- **节点分类任务**：在 Cora、Citeseer 等引文网络数据集上，ChebNet 的分类准确率远超传统 GNN（如 GCN 的前身），验证了谱图 CNN 的有效性；
- **后续模型基础**：ChebNet 的一阶近似（\(K=1\)）即为 GCN（Graph Convolutional Network）的核心公式，GCN 通过进一步简化 ChebNet，成为最流行的谱图 CNN 模型之一；
- **工业界应用**：ChebNet 的高效计算特性使其适用于大规模图任务（如社交网络节点分类、推荐系统），为 GNN 的工业化应用奠定基础。

**专业术语解释**

- **切比雪夫多项式（Chebyshev Polynomial）**：一类正交多项式，具有递归性和良好的逼近性，在数值分析、信号处理中广泛应用，ChebNet 将其引入谱图卷积，是降低计算复杂度的关键创新。
- **K-hop 局部性（K-hop Locality）**：滤波器仅依赖节点的 K 跳邻居信息，不涉及更远的节点，类比 CNN 的局部感受野（如 3×3 卷积仅依赖 3×3 邻域），使模型计算高效且具有局部解释性。

### 页码 56：Analyzing GCN in spatial & spectral aspects（从空间和谱域视角分析 GCN）

- **GCN 的核心定位**：Graph Convolutional Network（GCN）是 ChebNet 的一阶近似（\(K=1\)），同时具备谱域解释性和空间域直观性，是目前应用最广泛的谱图 CNN 模型。

#### 1. GCN 的谱域视角（基于 ChebNet 简化）

- **ChebNet 的一阶近似**：当 \(K=1\) 时，ChebNet 的谱图卷积简化为：

  

  \(h *_{\mathcal{G}} f \approx \theta_0 T_0(\tilde{L}) f + \theta_1 T_1(\tilde{L}) f = \theta_0 f + \theta_1 \tilde{L} f\)

  

  - 代入 \(\tilde{L} = \frac{2}{\lambda_{max}} L - I\)，并合并参数（\(\theta = \theta_1 \cdot \frac{2}{\lambda_{max}}\)，\(\beta = \theta_0 - \theta_1\)），可得：

    

    \(h *_{\mathcal{G}} f \approx \theta L f + \beta f\)

- **GCN 的最终公式**：进一步简化拉普拉斯矩阵为归一化形式 \(\hat{L} = \tilde{D}^{-1/2} (D - A) \tilde{D}^{-1/2}\)（\(\tilde{D}[i][i] = D[i][i] + 1\) 为带自环的度矩阵），GCN 的卷积公式为：

  

  \(X' = \hat{A} X W\)

  

  - \(\hat{A} = \tilde{D}^{-1/2} (A + I) \tilde{D}^{-1/2}\)（带自环的归一化邻接矩阵）；
  - X：输入节点特征矩阵；
  - W：可学习权重矩阵；

- **谱域解释**：GCN 的卷积本质是对图信号进行一阶低通滤波，保留低频全局特征，抑制高频噪声，适用于节点分类、图分类等任务。

#### 2. GCN 的空间域视角（邻居聚合）

- **空间域解释**：GCN 的卷积操作等价于 “节点特征的邻居聚合”，对每个节点 i，其输出特征为：

  

  \(x'_i = \sum_{j \in \mathcal{N}(i) \cup \{i\}} \hat{A}[i][j] x_j W\)

  

  - \(\hat{A}[i][j]\)：归一化的邻接权重，确保不同度的节点聚合后特征尺度一致（如度大的节点权重小，避免特征被主导）；
  - \(\mathcal{N}(i) \cup \{i\}\)：包含节点 i 自身的邻居集合（自环），避免自身特征丢失；

- **示例**：社交网络中，用户 i 的输出特征为自身兴趣特征与好友兴趣特征的加权和（权重由归一化邻接矩阵决定），整合局部社交关系信息。

#### 3. GCN 的局限性

- **空间域局限**：
  - 平等对待邻居：\(\hat{A}[i][j]\) 仅由节点度决定，无法区分邻居的重要性（如好友与陌生人的权重相同）；
  - 无向图限制：归一化邻接矩阵 \(\hat{A}\) 需对称（无向图），无法处理有向图；
- **谱域局限**：
  - 固定低通滤波：仅能进行一阶低通滤波，无法灵活调整频率范围（如无法增强高频局部特征）；
  - 动态图适配差：动态图（如时序社交网络）的 \(\hat{A}\) 随时间变化，GCN 需重新训练，泛化能力差；
- **改进方向**：
  - 注意力机制：如 GAT（Graph Attention Network），通过注意力权重区分邻居重要性；
  - 动态 GCN：设计时变邻接矩阵，适配动态图场景；
  - 高阶 GCN：堆叠多层 GCN，模拟高阶多项式近似，增强频率捕捉能力。

**专业术语解释**

- **归一化邻接矩阵（Normalized Adjacency Matrix）**：GCN 中为避免节点度差异导致的特征尺度不一致而设计的矩阵，\(\hat{A} = \tilde{D}^{-1/2} (A + I) \tilde{D}^{-1/2}\) 确保每行元素和为 1，使邻居聚合后的特征尺度稳定，是 GCN 训练稳定的关键。
- **自环（Self-loop）**：在邻接矩阵中添加 \(A[i][i] = 1\)（节点连接自身），确保节点在聚合时保留自身特征，避免自身信息被邻居特征覆盖，GCN 中自环是必要的设计（否则孤立节点的特征无法更新）。

### 页码 57：Datasets for Graph Neural Networks（图神经网络的数据集）

- **GNN 数据集的核心分类**：根据任务类型（节点级、边级、图级）和应用领域，GNN 数据集可分为多个类别，为不同场景的模型训练和评估提供基准。

#### 1. 分子与生物领域数据集

##### （1）ATOM3D

- **任务类型**：边分类、图回归、节点属性预测；
- **数据内容**：包含蛋白质 - 蛋白质相互作用（PPI）、配体结合亲和力（LBA）等子数据集，输入为三维空间中的原子结构（节点为原子，边为化学键或相互作用）；
- **核心任务**：
  - PPI：预测蛋白质之间的相互作用边（边分类）；
  - LBA：预测配体与蛋白质的结合亲和力（图回归）；
- **数据规模**：包含数百万个原子级图样本，支持大规模 GNN 训练；
- **获取链接**：https://zenodo.org/records/4911102。

##### （2）ENZYMES

- **任务类型**：图分类；
- **数据内容**：蛋白质三级结构数据集，每个图代表一个酶分子（节点为氨基酸残基，边为残基间的空间距离）；
- **核心任务**：将酶分子分类到 6 个不同的酶家族（多分类）；
- **数据规模**：包含 600 个图样本，每个图平均含 32 个节点；
- **获取链接**：http://quantum-machine.org/datasets/。

##### （3）QM7b

- **任务类型**：图回归；
- **数据内容**：有机分子生成数据集，每个图代表一个有机分子（节点为原子，边为化学键），标签为分子的量子化学性质（如原子化能）；
- **核心任务**：预测分子的量子化学性质（连续值回归）；
- **数据规模**：包含 7211 个分子图样本；
- **应用场景**：药物分子设计、材料科学中的分子性质预测。

#### 2. 社交与文本领域数据集

##### （1）Reddit

- **任务类型**：节点分类；
- **数据内容**：社交网络数据集，节点为 Reddit 用户，边为用户间的互动关系（如评论、点赞），节点标签为用户所属的兴趣社区（如 “gaming”“worldnews”）；
- **核心任务**：预测用户的兴趣社区类别（多分类）；
- **数据规模**：包含 232965 个节点、114615892 条边，属于大规模图数据集；
- **获取链接**：https://snap.stanford.edu/graphsage/。

##### （2）PubMed

- **任务类型**：节点分类、边预测；
- **数据内容**：学术论文引文网络，节点为论文，边为论文间的引用关系，节点属性为论文的关键词或摘要词嵌入；
- **核心任务**：
  - 节点分类：预测论文的研究领域（如 “医学”“计算机科学”）；
  - 边预测：预测两篇论文是否存在引用关系；
- **数据规模**：包含 19717 个节点、44338 条边；
- **获取链接**：https://doi.org/10.1609/aimag.v29i3.2157。

#### 3. 计算机视觉领域数据集

##### （1）3D Scene Graph Dataset

- **任务类型**：节点分类、边预测、关系预测；
- **数据内容**：3D 场景图数据集，节点为场景中的物体（如 “chair”“table”“person”），边为物体间的空间关系（如 “on”“behind”“next to”）；
- **核心任务**：
  - 节点分类：识别物体类别；
  - 关系预测：预测物体间的空间关系（如 “person is behind table”）；
- **数据规模**：包含数千个 3D 场景，每个场景含数十个物体节点；
- **获取链接**：https://3dscenegraph.stanford.edu/database.html。

##### （2）Visual Genome

- **任务类型**：场景图生成（节点 + 边预测）；
- **数据内容**：包含 108077 张图像的场景图标注，节点为图像中的物体，边为物体间的语义关系（如 “cat sitting on mat”）；
- **核心任务**：根据图像生成场景图（同时预测节点类别和边关系）；
- **应用场景**：图像理解、视觉问答（VQA）、图像检索。

#### 4. 数据集选择原则

- **适配任务类型**：节点级任务选 Reddit、PubMed，图级任务选 ENZYMES、QM7b，边级任务选 ATOM3D、PubMed；
- **考虑数据规模**：小规模数据集（如 ENZYMES）适合模型验证，大规模数据集（如 Reddit、ATOM3D）适合复杂 GNN（如 Graph Transformer）训练；
- **匹配领域特性**：分子任务选 ATOM3D、QM7b，社交任务选 Reddit，视觉任务选 3D Scene Graph Dataset。

**专业术语解释**

- **图回归（Graph Regression）**：图级任务的一种，预测目标为连续值（如分子的结合亲和力、量子化学性质），与图分类（离散标签）的核心区别在于输出类型，常用均方误差（MSE）作为损失函数。
- **场景图（Scene Graph）**：计算机视觉中的一种图结构表示，节点为图像中的物体，边为物体间的空间或语义关系（如 “on”“behind”），是图像理解、视觉问答的核心数据形式。

### 页码 58：Reference（参考文献）

- **核心作用**：汇总本讲涉及的关键研究论文，为深入学习 RNN、GNN 提供理论依据和扩展阅读资源，涵盖序列建模、谱图卷积、图 Transformer 等核心方向。

#### 1. 循环神经网络与序列建模相关

- **Diffusion-Convolutional Neural Networks（NIPS 2016）**

  作者：James Atwood, Don Towsley

  核心贡献：提出扩散卷积神经网络（DCNN），将图卷积视为节点特征的扩散过程，为后续图卷积模型提供了新思路；

  链接：https://proceedings.neurips.cc/paper/2016/file/390e982518a50e280d8e2b535462ec1f-Paper.pdf。

- **Recurrent Space-time Graph Neural Networks（NIPS 2019）**

  作者：Andrei Nicolicioiu, Iulia Duta, Marius Leordeanu

  核心贡献：提出时空循环图神经网络，将 RNN 的时序建模能力与 GNN 的空间建模能力结合，适用于动态图（如 3D 骨架序列、视频帧序列）任务；

  链接：https://proceedings.neurips.cc/paper/2019/file/383beaea4aa57dd8202dbff464fee3af-Paper.pdf。

#### 2. 谱图卷积与 GCN 相关

- **Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering（NIPS 2016）**

  作者：Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst

  核心贡献：提出基于多项式近似的谱图 CNN，首次实现谱图卷积的高效计算，为 ChebNet、GCN 奠定基础；

  链接：https://papers.nips.cc/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html。

- **Can GCNs Go as Deep as CNNs?（ICCV 2019）**

  作者：Guohao Li, Matthias Müller, Ali Thabet, Bernard Ghanem

  核心贡献：分析 GCN 的深度限制，提出 DeepGCNs 架构，通过残差连接、密集连接等技术支持深层 GCN 训练，突破传统 GCN 的层数局限；

  链接：https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_DeepGCNs_Can_GCNs_Go_As_Deep_As_CNN s_ICCV_2019_paper.pdf。

#### 3. 图 Transformer 与注意力机制相关

- **Graph Transformer Networks（NIPS 2019）**

  作者：Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, Hyunwoo Kim

  核心贡献：将 Transformer 的自注意力机制引入 GNN，提出图 Transformer 网络，通过注意力权重区分邻居重要性，提升 GNN 的表达能力；

  链接：https://proceedings.neurips.cc/paper/2019/file/9d63484abb477c97640154d40595a3bb-Paper.pdf。

- **Understanding Attention and Generalization in Graph Neural Networks（NIPS 2019）**

  作者：Boris Knyazev, Graham W. Taylor, Mohamed R. Amer

  核心贡献：深入分析 GNN 中注意力机制的泛化能力，提出注意力权重的正则化方法，避免过拟合，提升模型在未知图上的性能；

  链接：https://proceedings.neurips.cc/paper/2019/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf。

#### 4. 中文参考资料

- **《深入浅出图神经网络 GNN 原理解析》**

  

  核心内容：系统讲解 GNN 的基础理论、经典模型（如 GCN、GAT、GraphSAGE）及应用场景，包含代码实现案例，适合入门学习；

  

  下载链接：

  https://cloud.tsinghua.edu.cn/f/e95d877b8d2d4a4399d1/?dl=1

  。

#### 5. 参考文献使用建议

- **按主题学习**：序列建模方向优先阅读 RNN、时空 GNN 相关论文，谱图卷积方向优先阅读 Defferrard 等人（2016）、ChebNet 相关论文；
- **结合代码实践**：多数论文提供开源代码（如 GitHub 仓库），可结合代码理解模型细节（如 GCN 的归一化邻接矩阵实现）；
- **跟踪最新进展**：NIPS（NeurIPS）、ICCV、CVPR、ICML 等顶会每年发布 GNN 领域的最新研究，可通过会议官网或 arXiv 跟踪前沿方向。

**专业术语解释**

- **顶会（Top Conferences）**：计算机科学领域的高水平学术会议，GNN 相关研究主要发表于 NeurIPS（神经信息处理系统大会）、ICCV（国际计算机视觉大会）、CVPR（计算机视觉与模式识别大会）、ICML（国际机器学习大会），这些会议的论文代表领域前沿水平。
- **arXiv**：免费的预印本平台（https://arxiv.org/），许多 GNN 论文在正式发表前会上传至 arXiv，可提前获取最新研究成果，是跟踪领域进展的重要工具。

### 页码 59：课程总结（隐含页面，基于全文梳理）

- **本讲核心内容回顾**：第 9 讲围绕 “循环神经网络（RNN）” 与 “图神经网络（GNN）” 展开，覆盖序列建模与非结构化数据建模的核心技术，形成完整的知识体系。

#### 1. 循环神经网络（RNN）部分

- **核心问题**：传统 MLP 无法建模序列的时序依赖，RNN 通过隐藏状态传递历史信息，解决该问题；
- **关键模型**：
  - 基础 RNN：通过\(h^{(t)} = \sigma(W_h h^{(t-1)} + W_x x^{(t)} + b)\)实现时序建模，但存在梯度消失 / 爆炸问题；
  - LSTM：引入细胞状态和门控机制（遗忘门、输入门、输出门），解决梯度消失，支持长序列建模，在手写识别、机器翻译等领域取得成功；
- **局限性**：循环计算无法并行，效率低，后续被 Transformer 取代，但仍是理解序列建模的基础。

#### 2. 图神经网络（GNN）部分

- **核心问题**：传统 CNN/RNN 无法适配图的非规则拓扑，GNN 通过消息传递、谱图卷积等机制，实现图数据的特征学习；
- **关键模型与技术**：
  - 基础 GNN：通过邻居聚合更新节点特征，支持节点级、边级、图级任务；
  - 谱图卷积：基于拉普拉斯矩阵的特征分解，实现图信号的频率域滤波，代表模型有谱网络、ChebNet、GCN；
  - 实用优化：通过多项式近似、切比雪夫多项式降低计算复杂度，GCN（ChebNet 一阶近似）成为最流行的谱图 CNN 模型；
- **应用场景**：分子性质预测、社交网络节点分类、图像场景图生成等。

#### 3. 学习建议

- **基础巩固**：掌握 RNN 的隐藏状态传递、LSTM 的门控机制、GCN 的谱域与空间域解释；
- **实践重点**：
  - RNN：实现文本分类、语言建模，观察梯度消失现象，对比 LSTM 与基础 RNN 的性能差异；
  - GCN：使用 PyTorch Geometric 或 DGL 框架实现 GCN，在 Cora、PubMed 数据集上完成节点分类任务；
- **进阶方向**：学习 GraphSAGE（归纳式 GNN）、GAT（注意力机制 GNN）、Graph Transformer 等进阶模型，探索动态图、异构图等复杂场景。

**专业术语解释**

- **归纳式 GNN（Inductive GNN）**：能处理训练时未见过的新图的 GNN 模型（如 GraphSAGE），通过采样邻居、聚合函数泛化到新图，区别于仅能处理固定图的直推式 GNN（如早期 GCN），更适用于实际应用场景。
- **异构图（Heterogeneous Graph）**：包含多种类型节点和边的图（如社交网络中包含 “用户”“帖子”“评论” 等节点，“关注”“发布”“评论” 等边），异构图 GNN（如 HAN）需处理不同类型的节点 / 边特征，是 GNN 的重要研究方向。