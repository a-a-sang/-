# 媒体与认知 — 第 7 讲：卷积神经网络（二）

### 页码 1：Media and Cognition（媒体与认知）、Lecture 7: Convolutional Neural Networks - II（第 7 讲：卷积神经网络（二））

- **课程信息**

  清华大学电子工程系（Dept. of EE, Tsinghua University）

  方璐（Lu FANG）

- **页面说明**

  本页为第 7 讲课程封面，核心主题为

  卷积神经网络（CNN）在目标检测中的应用

  ，承接第 6 讲对 CNN 架构的基础讲解，聚焦 “从图像分类到目标检测” 的技术延伸，解决 “what（是什么）” 和 “where（在哪里）” 的联合预测问题。

### 页码 2：Recap: Lecture 6 - CNN Architectures（回顾：第 6 讲 ——CNN 架构）

- **核心回顾：CNN 的核心任务与局限**

  第 6 讲中，CNN 的核心应用是**图像分类**，即输入图像输出单一类别标签（如 “CAT”），但该任务存在明显局限：

  1. **单目标限制**：仅能识别图像中的单个主要目标，无法处理多目标场景（如同时包含猫、狗、书的图像）；
  2. **无位置信息**：仅输出 “是什么”，不提供目标在图像中的位置（如 “猫在图像左上角”）；
  3. **低分辨率依赖**：分类任务通常使用 224×224 低分辨率图像，无法满足高精度定位需求。

- **示例：分类任务的不足**

  右侧图像展示 CNN 对 “狗” 的分类结果（概率最高为 “malamute（雪橇犬）”，5.93%），但仅能识别目标类别，无法标注狗在图像中的具体位置，也无法区分图像中可能存在的其他目标。

**专业术语解释**

- **图像分类（Image Classification）**：CNN 的基础任务，将输入图像分配到预定义类别（如 “猫”“狗”“汽车”），输出类别概率分布，不涉及目标位置信息，是目标检测、图像分割等复杂任务的基础。

### 页码 3：Is Image Classification enough to understand an image?（图像分类足以理解图像吗？）

- **核心问题：从分类到检测的需求升级**

  图像分类无法满足实际场景中 “同时识别目标类别与位置” 的需求，具体体现在：

  1. **多目标场景**：现实图像常包含多个目标（如 “书 + 狗 + 猫”），分类任务仅能输出一个主要类别，忽略其他目标；
  2. **位置依赖任务**：自动驾驶、机器人视觉等任务需知道目标位置（如 “前方 30 米处有行人”），分类任务无法提供；
  3. **高精度需求**：工业质检、医学影像等场景需高分辨率图像（如 800×600），分类任务的低分辨率输入无法满足定位精度。

- **示例：目标检测的优势**

  右侧图像展示目标检测结果：

  - 识别出 “book（书，51%）”“dog（狗，87%）”“cat（猫，54%）” 三个目标；
  - 为每个目标标注**边界框（Bounding Box）**（用矩形框标记目标位置，包含 x、y 坐标及宽高），同时输出类别概率，实现 “what+where” 的联合预测。

**专业术语解释**

- **边界框（Bounding Box）**：目标检测中用于标记目标位置的矩形框，通常用四元组（x, y, width, height）表示，其中（x,y）为矩形框左上角坐标，width 和 height 为矩形框的宽和高，是定位目标的核心表示方式。

### 页码 4：Deep Learning Perception（深度学习感知）

- **核心应用：目标检测的实际场景**

  目标检测在多个领域有不可替代的作用，通过 “what+where” 的联合预测解决实际问题：

  1. **自动驾驶（Autonomous Driving）**：检测道路上的行人、车辆、交通信号灯，输出目标位置和类别，为决策提供依据（如 “前方 10 米有行人，需减速”）；
  2. **工业质检（Industrial Quality Control）**：检测生产线上的产品缺陷（如 “瓶子是否有裂痕”），标注缺陷位置，实现自动化质检；
  3. **机器人视觉（Robot Vision）**：机器人抓取物体时，需检测物体位置（如 “杯子在桌子中央”），指导机械臂运动；
  4. **安防监控（Security Surveillance）**：检测监控画面中的异常目标（如 “闯入禁区的人”），实时标注位置并报警。

- **示例：工业质检场景**

  下方图像展示生产线瓶子检测，目标检测模型可同时识别多个瓶子，标注每个瓶子的位置，判断是否存在缺陷（如 “瓶口变形”），效率远超人工质检。

**专业术语解释**

- **目标检测（Object Detection）**：计算机视觉核心任务，输入图像后同时输出目标的 “类别标签（what）” 和 “边界框（where）”，支持多目标、高分辨率场景，是自动驾驶、工业质检等领域的核心技术。

### 页码 5：Object Detection: Definition（目标检测：定义）

- **目标检测的正式定义**

  输入：单张 RGB 彩色图像（可支持任意分辨率，如 800×600、1920×1080）；

  输出：一组检测到的目标，每个目标包含两类信息：

  1. **类别标签（Category Label）**：从预定义类别集合中选择的目标类别（如 VOC 数据集的 20 类、COCO 数据集的 91 类）；
  2. **边界框（Bounding Box）**：用四元组（x, y, width, height）表示的矩形框，标记目标在图像中的位置和大小，其中 x、y 通常为矩形框左上角或中心点坐标。

- **示例：多目标检测**

  右侧图像展示包含猫、狗的场景，目标检测模型输出：

  - 猫：类别标签 “cat”，边界框（x1, y1, w1, h1）；

  - 狗：类别标签 “dog”，边界框（x2, y2, w2, h2）；

    

    实现多目标的同时识别与定位。

**专业术语解释**

- **预定义类别集合（Fixed Category Set）**：目标检测模型训练前确定的类别范围，如 PASCAL VOC 数据集包含 “人、车、猫、狗” 等 20 类，COCO 数据集包含 “人、交通灯、沙发” 等 91 类，模型仅能检测集合内的类别。

### 页码 6：Object Detection: Challenges（目标检测：挑战）

- **目标检测面临的核心技术挑战**

  相比图像分类，目标检测需解决三个关键问题：

  1. **输出数量可变**：不同图像包含的目标数量不同（如 1 个目标或 10 个目标），模型需输出可变数量的检测结果，而分类任务仅输出固定长度的类别概率向量；
  2. **多类型输出**：需同时预测 “类别（离散标签）” 和 “边界框（连续坐标）”，两种输出类型的损失函数和优化方式不同（分类用交叉熵，回归用 L2 损失）；
  3. **高分辨率需求**：定位需要高分辨率图像（如 800×600），导致数据量和计算量激增（800×600 图像的像素数是 224×224 的 10 倍以上），对模型效率提出更高要求。

- **挑战的直观体现**

  右侧图像展示高分辨率图像中的小目标（如远处的行人），低分辨率分类模型无法识别，而目标检测模型需在高分辨率图像中精准定位这类小目标，同时处理多目标重叠、遮挡等复杂情况。

**专业术语解释**

- **多类型输出（Multi-type Output）**：目标检测的核心特性之一，同时输出离散的类别标签和连续的边界框坐标，需设计多任务损失函数（如分类损失 + 回归损失的加权和），实现两类输出的联合优化。

### 页码 7：Evaluation Metric: Intersection over Union (IoU)（评估指标：交并比）

- **核心指标：IoU 的定义与作用**

  交并比（IoU）是衡量 “预测边界框” 与 “真实边界框（Ground Truth）” 重叠程度的核心指标，用于判断检测结果的准确性，公式为：

  \(IoU = \frac{\text{Area of Intersection}}{\text{Area of Union}}\)

  - **交集区域（Area of Intersection）**：预测框与真实框重叠部分的面积；
  - **并集区域（Area of Union）**：预测框与真实框覆盖的总面积（交集面积 + 两个框各自不重叠的面积）。

- **IoU 的取值与意义**

  IoU 的取值范围为 [0, 1]：

  - IoU=1：预测框与真实框完全重叠，检测完全准确；
  - IoU=0：预测框与真实框无重叠，检测完全错误；
  - 实际判断标准：
    - IoU>0.5：“不错” 的检测结果（工业界常用阈值）；
    - IoU>0.7：“较好” 的检测结果（学术评估常用阈值）；
    - IoU>0.9：“几乎完美” 的检测结果（高精度场景如医学影像）。

- **示例：IoU 计算**

  右侧图像展示预测框与真实框的重叠情况，IoU>0.5，判定为有效检测结果，直观反映了预测框的定位精度。

**专业术语解释**

- **真实边界框（Ground Truth Bounding Box）**：人工标注的目标真实位置边界框，作为评估模型检测精度的基准，通常包含在数据集（如 VOC、COCO）中，是计算 IoU、TP、FP 等指标的基础。

### 页码 8：Evaluation Metric: Non-Max Suppression (NMS)（评估指标：非极大值抑制）

- **核心问题：重叠检测框的筛选**

  目标检测模型常输出多个重叠的检测框（如同一狗被标注 3 个重叠矩形框），需通过**非极大值抑制（NMS）** 筛选出最优框，流程如下：

  1. **按分数排序**：将所有检测框按类别概率（如 “狗” 的概率 0.9、0.75、0.7）从高到低排序；
  2. **选择最高分组**：选取当前分数最高的框作为基准框，保留该框；
  3. **抑制重叠框**：计算基准框与剩余框的 IoU，若 IoU > 阈值（通常为 0.7），则删除剩余框（视为重叠冗余）；
  4. **循环迭代**：若仍有剩余框，重复步骤 2-3，直至所有框被保留或删除。

- **示例：NMS 效果**

  右侧图像展示 NMS 处理前后的对比：

  - 处理前：3 个重叠的 “狗” 检测框，概率分别为 0.9、0.75、0.7；
  - 处理后：仅保留分数最高的框（0.9），删除 IoU>0.7 的重叠框（0.75、0.7），消除冗余。

- **NMS 的关键参数**

  - **IoU 阈值**：控制抑制严格程度，阈值越小（如 0.5）抑制越严格，可能删除正确框；阈值越大（如 0.8）抑制越宽松，可能保留冗余框，通常设为 0.7；
  - **分数阈值**：预先过滤低概率框（如删除概率 < 0.5 的框），减少 NMS 计算量，提升效率。

**专业术语解释**

- **非极大值抑制（Non-Max Suppression, NMS）**：目标检测的后处理步骤，通过删除重叠冗余的检测框，保留分数最高、定位最准的框，解决 “同一目标被多次检测” 的问题，是提升检测精度的关键步骤。

### 页码 9：Evaluation Metric: Mean Average Precision (mAP)（评估指标：平均精度均值）

- **核心指标：mAP 的定义与作用**

  平均精度均值（mAP）是衡量目标检测模型整体性能的核心指标，尤其适用于多类别检测，计算流程分为两步：

  1. **计算每个类别的平均精度（AP）**：AP 是 “精度 - 召回率曲线（Precision-Recall Curve）” 下的面积，反映模型在该类别上的检测能力；
  2. **计算 mAP**：将所有类别的 AP 取平均值，得到 mAP，综合评估模型在所有类别上的性能。

- **mAP 的关键背景**

  - **解决 NMS 的局限性**：NMS 可能因重叠目标（如密集排列的行人）误删正确框，mAP 通过 PR 曲线综合衡量 “查准率” 和 “查全率”，避免单一指标的偏差；
  - **跨数据集统一标准**：不同数据集（如 VOC、COCO）均采用 mAP 作为评估指标，便于模型性能的横向对比（如 Model A 在 COCO 数据集的 mAP=0.5，Model B=0.6，说明 B 更优）。

**专业术语解释**

- **精度 - 召回率曲线（Precision-Recall Curve, PR 曲线）**：以召回率（Recall）为横轴、精度（Precision）为纵轴的曲线，曲线上每个点对应不同检测阈值下的精度和召回率，AP 为曲线下的面积，面积越大，模型性能越好。

### 页码 10：Evaluation Metric: Precision and Recall（评估指标：精度与召回率）

- **核心概念：TP、FP、FN 的定义**

  经过 NMS 后，所有检测框被分为三类，作为计算精度（Precision）和召回率（Recall）的基础：

  1. **真阳性（True Positive, TP）**：预测框与真实框的 IoU > 阈值（如 0.5），且类别预测正确（如 “猫” 的预测框匹配 “猫” 的真实框）；
  2. **假阳性（False Positive, FP）**：预测框无匹配的真实框（如将背景误判为 “狗”），或类别预测错误（如 “猫” 的预测框匹配 “狗” 的真实框）；
  3. **假阴性（False Negative, FN）**：真实框无匹配的预测框（如漏检 “猫” 的真实框）。

- **精度与召回率的计算公式**

  1. **精度（Precision）**：衡量 “预测为正的结果中真正为正的比例”，反映 “查得准” 的能力：

     \(Precision = \frac{TP}{TP + FP} = \frac{TP}{\text{预测框总数}}\)

     - 示例：10 个预测框中，6 个为 TP，4 个为 FP，则 Precision=6/(6+4)=0.6；

  2. **召回率（Recall）**：衡量 “真实为正的结果中被正确预测的比例”，反映 “查得全” 的能力：

     \(Recall = \frac{TP}{TP + FN} = \frac{TP}{\text{真实框总数}}\)

     - 示例：10 个真实框中，6 个被正确检测（TP），4 个漏检（FN），则 Recall=6/(6+4)=0.6。

- **示例：精度与召回率计算**

  右侧图像展示具体案例：

  - 预测框：2 个（框 1 为 TP，框 2 为 FP）；
  - 真实框：2 个（A 为 TP 匹配，B 为 FN 漏检）；
  - 计算结果：Precision=1/(1+1)=0.5，Recall=1/(1+1)=0.5，反映模型 “查得准” 和 “查得全” 能力均一般。

**专业术语解释**

- **真阳性 / 假阳性 / 假阴性（TP/FP/FN）**：目标检测中的核心分类指标，基于 “预测结果” 与 “真实标签” 的匹配关系定义，是计算精度、召回率、AP 等高级指标的基础，类比医学诊断中的 “真病 / 误诊 / 漏诊”。

### 页码 11：Evaluation Metric: Mean Average Precision (mAP)（评估指标：平均精度均值）

- **AP 的计算步骤（以 “狗” 类别为例）**

  平均精度（AP）是 PR 曲线下的面积，计算流程如下：

  1. **准备数据**：
     - 检测结果：所有 “狗” 的检测框，按类别概率从高到低排序（如 0.99、0.95、0.90、0.5、0.10）；
     - 真实标签：所有 “狗” 的真实框（如 3 个）；
  2. **逐框标记**：
     - 按分数从高到低遍历每个检测框，若与某真实框的 IoU>0.5，标记为 TP，并删除该真实框（避免重复匹配）；
     - 若无匹配真实框，标记为 FP；
  3. **计算 PR 值**：
     - 每标记一个框，计算当前的 Precision 和 Recall（如第 1 个 TP 框：Precision=1/1=1.0，Recall=1/3≈0.33）；
  4. **绘制 PR 曲线**：
     - 将每个框对应的（Recall, Precision）作为点，连接所有点形成 PR 曲线；
  5. **计算 AP**：
     - 计算 PR 曲线下的面积（通常采用插值法，如 11 点插值、积分法），即为该类别的 AP。

- **示例：初始 PR 点计算**

  右侧图像展示前 2 个检测框的标记：

  - 第 1 个框（0.99）：匹配真实框，标记为 TP，Precision=1/1=1.0，Recall=1/3≈0.33；
  - 第 2 个框（0.95）：匹配真实框，标记为 TP，Precision=2/2=1.0，Recall=2/3≈0.67；
  - 两点均绘制在 PR 曲线上，为后续面积计算奠定基础。

**专业术语解释**

- **PR 曲线插值（PR Curve Interpolation）**：计算 AP 的关键步骤，由于 PR 曲线可能存在波动（如 Recall 增加时 Precision 下降），需通过插值（如对每个 Recall 点取最大 Precision）使曲线平滑，确保 AP 计算的稳定性，常用 11 点插值（取 Recall=0,0.1,...,1.0 共 11 个点）。

### 页码 12：Evaluation Metric: Mean Average Precision (mAP)（评估指标：平均精度均值）

- **AP 计算的继续（以 “狗” 类别为例）**

  延续上一页的案例，继续遍历剩余检测框：

  3. **第 3 个框（0.90）**：

  - 无匹配的真实框（剩余 1 个真实框与该框的 IoU<0.5），标记为 FP；
  - 计算 PR 值：TP=2，FP=1，预测框总数 = 3，真实框总数 = 3；
  - Precision=2/(2+1)=0.67，Recall=2/3≈0.67；
  - 在 PR 曲线上绘制点（0.67, 0.67）；

  1. **第 4 个框（0.5）**：
     - 无匹配的真实框，标记为 FP；
     - 计算 PR 值：TP=2，FP=2，预测框总数 = 4；
     - Precision=2/(2+2)=0.5，Recall=2/3≈0.67；
     - 在 PR 曲线上绘制点（0.67, 0.5）；
  2. **第 5 个框（0.10）**：
     - 匹配剩余的 1 个真实框（IoU>0.5），标记为 TP；
     - 计算 PR 值：TP=3，FP=2，预测框总数 = 5，真实框总数 = 3；
     - Precision=3/(3+2)=0.6，Recall=3/3=1.0；
     - 在 PR 曲线上绘制点（1.0, 0.6）。

- **PR 曲线与 AP 计算**

  右侧 PR 曲线连接所有点（0,1.0）→（0.33,1.0）→（0.67,1.0）→（0.67,0.67）→（0.67,0.5）→（1.0,0.6），通过积分法计算曲线下的面积，得到 “狗” 类别的 AP=0.86。

**专业术语解释**

- **积分法计算 AP（Integral AP Calculation）**：COCO 数据集推荐的 AP 计算方法，将 PR 曲线视为连续函数，通过积分计算曲线下的面积，相比 11 点插值更精确，能更好地反映模型在不同 Recall 区间的性能。

### 页码 13：Evaluation Metric: Mean Average Precision (mAP)（评估指标：平均精度均值）

- **mAP 的计算与扩展**

  1. **多类别 AP 计算**：

     - 对每个类别（如 “狗”“猫”“汽车”），按上述步骤计算 AP；
     - 示例：“汽车” AP=0.65，“猫” AP=0.80，“狗” AP=0.86；

  2. **mAP 的计算**：

     - 将所有类别的 AP 取算术平均值，得到 mAP：

       

       \(mAP = \frac{AP_{\text{汽车}} + AP_{\text{猫}} + AP_{\text{狗}}}{3} = \frac{0.65 + 0.80 + 0.86}{3} \approx 0.77\)

  3. **不同 IoU 阈值的 mAP**：

     - 实际评估中，需计算不同 IoU 阈值（0.5, 0.55, ..., 0.95，共 10 个阈值）下的 mAP，取平均值作为最终指标（称为 “COCO mAP”）；
     - 示例：mAP@0.5=0.77，mAP@0.55=0.71，...，mAP@0.95=0.2，COCO mAP=0.4；
     - 意义：不同阈值反映模型对定位精度的要求，COCO mAP 综合衡量模型在不同精度需求下的性能。

- **mAP 的解读**

  - mAP 越高，模型性能越好（如 mAP=0.8 优于 mAP=0.5）；
  - 不同数据集的 mAP 不可直接对比（如 VOC 数据集的 mAP 通常高于 COCO，因 COCO 类别更多、目标更复杂）；
  - 工业界通常关注 mAP@0.5（宽松阈值），学术研究关注 COCO mAP（综合阈值）。

**专业术语解释**

- **COCO mAP**：COCO 数据集定义的标准 mAP 指标，计算 IoU 从 0.5 到 0.95（步长 0.05）共 10 个阈值下的 mAP，取平均值，能更全面地评估模型在不同定位精度需求下的性能，是当前目标检测领域的主流评估标准。

### 页码 14：Detecting a Single Object（检测单个目标）

- **核心思路：将定位视为回归问题**

  对于单目标场景（如仅含一只猫的图像），可将目标检测拆解为 “分类 + 回归” 两个子任务，通过 CNN 同时输出类别和边界框，流程如下：

  1. **特征提取**：
     - 输入图像（如 224×224），通过 CNN 骨干网络（如 AlexNet）提取高维特征向量（如 4096 维）；
  2. **分类分支**：
     - 将 4096 维特征输入全连接层（4096→1000），通过 Softmax 输出类别概率分布（如 “猫” 0.9，“狗” 0.05，“汽车” 0.01）；
     - 损失函数：交叉熵损失（Cross-Entropy Loss），衡量预测类别与真实类别的差异；
  3. **回归分支**：
     - 将 4096 维特征输入另一全连接层（4096→4），输出边界框坐标（x, y, w, h）；
     - 损失函数：L2 损失（MSE Loss），衡量预测框与真实框的坐标差异；
  4. **多任务损失**：
     - 整体损失为分类损失与回归损失的加权和（如 Loss = Loss_cls + λ・Loss_reg，λ 为权重系数，通常取 1），实现两类任务的联合优化。

- **示例：单目标检测结果**

  右侧图像展示单目标检测：

  - 分类结果：正确标签 “Cat”，概率 0.9；
  - 回归结果：预测框（x, y, w, h），与真实框（x’, y’, w’, h’）的 IoU>0.7，定位准确；
  - 多任务损失通过反向传播更新 CNN 参数，逐步提升分类和回归精度。

**专业术语解释**

- **多任务学习（Multi-Task Learning）**：目标检测的核心技术，将 “分类（离散输出）” 和 “回归（连续输出）” 结合在一个模型中，共享特征提取网络，通过多任务损失联合优化，相比单独训练两个模型，能提升数据利用效率和模型泛化能力。

### 页码 15：Detecting Multiple Objects（检测多个目标）

- **核心挑战：输出数量的可变性**

  多目标场景（如含猫、狗、鸭子的图像）的核心问题是 “目标数量不固定”，传统单目标检测的固定输出（1 个类别 + 1 个框）无法满足需求，具体体现为：

  1. **输出数量不确定**：不同图像的目标数量差异大（如 1 个或 10 个），模型需动态调整输出数量；
  2. **类别与框的对应**：需确保每个目标的类别与边界框一一对应（如 “猫” 对应左上角框，“狗” 对应中间框），避免混淆；
  3. **计算效率**：若为每个可能的目标设计输出分支，会导致模型参数激增，无法实用。

- **示例：多目标的输出需求**

  右侧图像展示两个多目标场景：

  - 场景 1：含猫、狗的图像，需输出 2 个目标（猫的框 + 类别，狗的框 + 类别）；
  - 场景 2：含猫、狗、鸭子的图像，需输出 3 个目标（猫、狗、鸭子的框 + 类别）；
  - 可见输出数量随图像内容动态变化，需设计灵活的输出机制。

**专业术语解释**

- **可变输出数量（Variable Output Count）**：多目标检测与单目标检测的核心区别，单目标检测输出固定长度的向量（1 个类别 + 4 个坐标），多目标检测需输出可变数量的向量（N 个类别 + 4N 个坐标，N 为目标数），是滑动窗口、区域提议等方法的设计动因。

### 页码 16：Detecting Multiple Objects: Sliding Window（检测多个目标：滑动窗口）

- **核心思路：将检测转化为分类**

  滑动窗口（Sliding Window）是早期多目标检测的基础方法，通过在图像上滑动固定大小的窗口，将每个窗口视为独立的分类样本，流程如下：

  1. **窗口滑动**：
     - 定义不同大小的窗口（如 32×32、64×64、128×128），在图像上按固定步长（如 8 像素）滑动；
     - 每个窗口对应图像的一个局部区域（如左上角 32×32 区域、右上角 32×32 区域）；
  2. **分类判断**：
     - 将每个窗口裁剪、resize 为 CNN 分类模型的输入尺寸（如 224×224）；
     - 输入 CNN，判断窗口内是否包含目标及目标类别（如 “猫”“狗”“背景”）；
  3. **结果筛选**：
     - 保留类别概率 > 阈值（如 0.5）的窗口，视为候选检测框；
     - 通过 NMS 删除重叠冗余框，得到最终检测结果。

- **关键问题：计算量爆炸**

  滑动窗口的核心缺陷是窗口数量过多，计算量巨大，以 800×600 图像为例：

  - 单个大小窗口（如 128×128）的可能位置数：(800-128+1)×(600-128+1)≈42 万；
  - 多个大小窗口的总位置数：\(\sum_{h=1}^{800} \sum_{w=1}^{600} (800-w+1)(600-h+1) \approx 5800万\)；
  - 如此庞大的窗口数量导致 CNN 需执行 5800 万次前向传播，无法满足实时需求（如每秒 24 帧）。

- **示例：滑动窗口的局限性**

  右侧图像展示滑动窗口的检测结果：

  - 正确检测出 “狗”“猫”“背景”，但存在大量重叠窗口（如 “狗” 被 10 个窗口覆盖），需 NMS 筛选；
  - 小目标（如远处的猫）可能因窗口大小不匹配而漏检，大目标可能因窗口滑动步长过大而定位不准。

**专业术语解释**

- **滑动窗口（Sliding Window）**：早期多目标检测方法，通过滑动窗口将图像分割为多个局部区域，转化为分类任务，思路简单但计算量巨大，无法处理高分辨率图像和小目标，已被区域提议方法取代，但仍是理解目标检测的基础。

### 页码 17：Detecting Multiple Objects: Region Proposals（检测多个目标：区域提议）

- **核心改进：减少候选区域数量**

  区域提议（Region Proposals）方法通过先筛选 “可能包含目标的候选区域（Region Proposals, RP）”，再对 RP 进行分类和回归，大幅减少候选数量（从数百万降至 2000 个），常用方法包括：

  1. **Selective Search（选择性搜索）**：
     - 原理：基于图像的颜色、纹理、大小等低级特征，通过分层聚类生成 RP，确保 RP 覆盖所有可能的目标区域；
     - 优势：CPU 上可在几秒内生成 2000 个 RP，覆盖 95% 以上的目标；
     - 缺陷：依赖手工设计特征，对复杂背景（如纹理丰富的场景）的 RP 质量较低；
  2. **BING（Binarized Normed Gradients）**：
     - 原理：通过二值化归一化梯度特征快速判断窗口是否为目标，速度达 300fps（帧 / 秒）；
     - 优势：速度极快，适合实时场景；
     - 缺陷：小目标 RP 的召回率较低；
  3. **Edge Boxes（边缘框）**：
     - 原理：基于图像边缘信息（如边缘检测得到的轮廓）生成 RP，边缘密度高的区域更可能包含目标；
     - 优势：对边缘清晰的目标（如汽车、桌子）的 RP 质量高；
     - 缺陷：对无明显边缘的目标（如毛绒玩具）效果差。

- **区域提议的核心价值**

  - **减少计算量**：将候选区域从数百万降至 2000 个，CNN 仅需处理 2000 个 RP，计算量减少 99.9%；
  - **提升召回率**：RP 方法通过低级特征筛选，确保覆盖大部分目标区域，召回率远超滑动窗口；
  - **为两阶段检测奠定基础**：区域提议是 R-CNN、Fast R-CNN 等两阶段检测器的核心组件，第一阶段生成 RP，第二阶段对 RP 进行精细检测。

**专业术语解释**

- **区域提议（Region Proposals, RP）**：目标检测中的关键步骤，通过快速算法（如 Selective Search）生成少量（2000 个左右）可能包含目标的候选区域，大幅减少后续 CNN 的处理量，是连接传统图像处理与深度学习的桥梁。

### 页码 18：R-CNN: Region-Based CNN（基于区域的 CNN）

- **核心架构：两阶段检测的开端**

  R-CNN（Region-CNN）是首个将深度学习用于目标检测的两阶段模型，流程分为 “区域提议→特征提取→分类 + 回归”，具体步骤如下：

  1. **区域提议（第一阶段）**：
     - 输入图像，通过 Selective Search 生成 2000 个 RP；
     - 将每个 RP 裁剪为固定大小（如 227×227），适配 CNN 输入；
  2. **特征提取（第二阶段）**：
     - 将 2000 个裁剪后的 RP 分别输入 CNN 骨干网络（如 AlexNet）；
     - 提取高维特征向量（如 4096 维），每个 RP 对应一个特征向量；
  3. **分类与回归（第二阶段）**：
     - **分类分支**：将 4096 维特征输入 SVM 分类器，输出目标类别（如 VOC 数据集的 20 类 + 背景）；
     - **回归分支**：将 4096 维特征输入回归器，输出边界框修正参数（t_x, t_y, t_w, t_h），优化 RP 的位置精度；
  4. **后处理**：
     - 对分类结果应用 NMS，删除重叠冗余框；
     - 输出最终的类别标签和优化后的边界框。

- **R-CNN 的关键创新**

  1. **两阶段架构**：首次将 “区域提议” 与 “深度学习” 结合，大幅减少候选区域数量，解决滑动窗口的计算量问题；
  2. **边界框回归**：通过回归器修正 RP 的位置，提升定位精度（如将 RP 从 “包含猫的大部分区域” 优化为 “精准框住猫”）；
  3. **迁移学习**：利用 ImageNet 预训练的 CNN 骨干网络，提升小数据集（如 VOC）上的特征提取能力，减少过拟合。

- **参考文献**

  Girshick et al, “Rich feature hierarchies for accurate object detection and semantic segmentation”, CVPR 2014（R-CNN 的奠基性论文，首次证明深度学习在目标检测中的有效性）。

**专业术语解释**

- **两阶段检测器（Two-Stage Detector）**：目标检测的主流架构之一，分为 “区域提议阶段（生成候选区域）” 和 “检测阶段（分类 + 回归）”，R-CNN、Fast R-CNN、Faster R-CNN 均属于此类，特点是精度高但速度较慢，适合对精度要求高的场景（如医学影像）。

### 页码 19：R-CNN: Region-Based CNN（基于区域的 CNN）

- **核心改进：边界框回归**

  R-CNN 的关键缺陷是区域提议（RP）的位置不够精准（如 RP 仅覆盖目标的 80% 区域），需通过**边界框回归（Bounding Box Regression）** 优化，原理如下：

  1. **参数定义**：

     - **区域提议框（p）**：Selective Search 生成的 RP，用（p_x, p_y, p_w, p_h）表示（x,y 为左上角坐标，w,h 为宽高）；

     - **修正参数（t）**：回归器输出的 4 个参数（t_x, t_y, t_w, t_h），分别控制 x、y 方向的平移和 w、h 方向的缩放；

     - **输出框（b）**：优化后的边界框，公式为：

       

       \(b_x = p_x + p_w \cdot t_x \quad b_y = p_y + p_h \cdot t_y \quad b_w = p_w \cdot \exp(t_w) \quad b_h = p_h \cdot \exp(t_h)\)

     - 说明：平移量（t_x, t_y）与 RP 的宽高（p_w, p_h）成正比，确保不同大小的 RP 修正比例一致；缩放用指数函数，确保 w、h 始终为正。

  2. **回归器训练**：

     - **训练数据**：RP 与真实框的 IoU>0.5 的样本视为正样本，学习从 RP 特征到修正参数的映射；
     - **损失函数**：L2 损失，衡量预测修正参数与真实修正参数的差异；
     - **特征输入**：使用 CNN 提取的 4096 维特征，确保回归器能利用高级语义特征。

- **R-CNN 的核心缺陷**

  尽管 R-CNN 相比滑动窗口有巨大进步，但仍存在关键问题：

  1. **速度慢**：2000 个 RP 需分别输入 CNN，存在大量重复计算（如重叠 RP 的特征提取重复），测试时单张图像需 47 秒（含区域提议）；
  2. **多模型训练复杂**：需分别训练 CNN（特征提取）、SVM（分类）、回归器（边界框优化），三个模型独立优化，无法端到端训练；
  3. **存储开销大**：需存储 2000 个 RP 的特征向量，大规模数据集（如 COCO）的存储开销达 TB 级。

**专业术语解释**

- **边界框回归（Bounding Box Regression）**：目标检测中优化边界框位置的技术，通过学习从候选框到真实框的修正参数，提升定位精度，是所有高精度检测器（如 R-CNN、Faster R-CNN）的核心组件，分为基于回归器的传统方法和基于 CNN 的端到端方法。

### 页码 20：Fast R-CNN（快速 R-CNN）

- **核心改进：共享特征提取**

  Fast R-CNN 针对 R-CNN 的速度缺陷，提出 “先提取全局特征，再裁剪区域特征” 的思路，大幅减少重复计算，流程如下：

  1. **全局特征提取（第一阶段）**：
     - 输入图像（如 800×600），通过 CNN 骨干网络（如 VGG16）提取全局特征图（如 512×20×15，分辨率为输入的 1/32）；
     - 关键创新：仅需执行一次 CNN 前向传播，所有区域的特征均从全局特征图中裁剪，避免 R-CNN 的 2000 次重复计算；
  2. **区域特征裁剪（RoI Pooling）**：
     - **RoI 投影**：将 Selective Search 生成的 2000 个 RP（图像坐标）映射到特征图坐标（如图像中 RP 的 x 坐标除以 32，得到特征图中的 x 坐标）；
     - **池化操作**：对每个投影后的 RoI，通过 RoI Pooling 将其池化为固定大小的特征图（如 7×7），确保不同大小的 RoI 输出相同维度的特征（如 512×7×7）；
  3. **分类与回归（第二阶段）**：
     - 将 7×7 特征图展平为向量（如 512×7×7=25088 维），输入全连接层；
     - **分类分支**：输出类别概率（如 21 类：20 目标类 + 1 背景类）；
     - **回归分支**：输出每个类别的边界框修正参数（4×21=84 维）；
     - **多任务损失**：分类损失（交叉熵）+ 回归损失（L2）的加权和，实现端到端训练。

- **Fast R-CNN 的优势**

  相比 R-CNN，Fast R-CNN 的核心提升的：

  1. **速度大幅提升**：单张图像测试时间从 47 秒降至 2.3 秒（含区域提议），主要得益于全局特征共享；
  2. **端到端训练**：分类和回归分支共享特征提取网络，通过多任务损失联合优化，无需独立训练 SVM 和回归器；
  3. **精度提升**：全局特征包含更丰富的上下文信息（如目标周围的背景），分类和回归精度均高于 R-CNN。

- **参考文献**

  Girshick et al, “Fast R-CNN”, ICCV 2015（Fast R-CNN 的核心论文，提出 RoI Pooling 和端到端训练，是目标检测速度提升的关键突破）。

**专业术语解释**

- **感兴趣区域池化（Region of Interest Pooling, RoI Pooling）**：Fast R-CNN 的核心技术，将不同大小的 RoI 从特征图中裁剪并池化为固定大小的特征图（如 7×7），解决 “不同大小 RoI 输出维度不一致” 的问题，为后续全连接层提供统一输入，是实现特征共享的关键。

### 页码 21：Fast R-CNN（快速 R-CNN）

- **网络结构细节**

  Fast R-CNN 的网络结构分为 “骨干网络（Backbone）” 和 “区域网络（Per-Region Network）” 两部分：

  1. **骨干网络（特征提取）**：
     - 常用网络：AlexNet（5 个卷积层 + 3 个全连接层）、VGG16（12 个卷积层 + 3 个全连接层）、ResNet（残差网络）；
     - 以 VGG16 为例：
       - 卷积层：12 个卷积层（如 3×3 conv, 512），逐步降低分辨率（如 800×600→25×19→13×10→...→20×15）；
       - 池化层：5 个最大池化层，步长 2，降低分辨率并保留关键特征；
     - 输出：全局特征图（如 512×20×15），每个像素对应输入图像的 32×32 区域（感受野）。
  2. **区域网络（分类 + 回归）**：
     - **RoI Pooling**：将每个 RoI 池化为 7×7 特征图，展平为 25088 维向量；
     - **全连接层**：
       - FC1：25088→4096 维；
       - FC2：4096→4096 维；
     - **分类分支**：4096→21 维（20 类 + 背景），Softmax 输出类别概率；
     - **回归分支**：4096→84 维（21 类 ×4 个修正参数），输出每个类别的边界框坐标。

- **关键创新：端到端训练**

  Fast R-CNN 首次实现目标检测的端到端训练，核心在于多任务损失函数：

  \(Loss = Loss_{cls} + \lambda \cdot Loss_{reg}\)

  - **分类损失（Loss_cls）**：交叉熵损失，衡量预测类别与真实类别的差异；
  - **回归损失（Loss_reg）**：仅对正样本（IoU>0.5 的 RoI）计算 L2 损失，衡量预测框与真实框的坐标差异；
  - **权重 λ**：平衡两类损失，通常取 1，确保分类和回归任务同等重要。

- **Fast R-CNN 的局限性**

  尽管 Fast R-CNN 速度大幅提升，但仍存在瓶颈：

  - **区域提议速度慢**：Selective Search 在 CPU 上生成 2000 个 RP 需 2 秒，占测试时间的 87%（2.3 秒中的 2 秒）；
  - **无法处理动态图**：区域提议依赖手工设计的特征（如颜色、纹理），对动态场景（如目标遮挡、光照变化）的鲁棒性差；
  - **小目标精度低**：RoI Pooling 的 “量化误差”（将 RoI 坐标映射到特征图时取整）导致小目标的定位精度下降。

**专业术语解释**

- **骨干网络（Backbone Network）**：目标检测中的特征提取网络，通常为预训练的 CNN（如 VGG16、ResNet），负责将输入图像转化为高维语义特征图，是后续分类和回归任务的基础，其性能直接影响检测精度。

### 页码 22：Fast R-CNN（快速 R-CNN）

- **核心问题：如何裁剪特征？**

  Fast R-CNN 的关键步骤是从全局特征图中裁剪 RoI 对应的特征，需解决两个问题：

  1. **RoI 投影**：将图像坐标系下的 RoI（如 x=100, y=50, w=200, h=150）映射到特征图坐标系，公式为：

     \(x_{feat} = \frac{x_{img}}{s}, \quad y_{feat} = \frac{y_{img}}{s}, \quad w_{feat} = \frac{w_{img}}{s}, \quad h_{feat} = \frac{h_{img}}{s}\)

     - s为特征图与输入图像的分辨率比例（如 VGG16 的 s=32）；
     - 示例：图像 RoI 的 x=100，映射到特征图的 x=100/32≈3.125；

  2. **特征裁剪与池化**：

     - 由于映射后的 RoI 坐标可能为小数（如 3.125），需通过 RoI Pooling 将其量化为整数坐标（如 3），再裁剪对应区域；
     - 将裁剪后的区域划分为 k×k 网格（如 7×7），对每个网格进行最大池化，得到固定大小的特征图。

- **示例：RoI Pooling 流程**

  右侧图像展示 RoI Pooling 的具体步骤：

  1. 输入：全局特征图（512×20×15）和 RoI（图像坐标）；
  2. 投影：将 RoI 映射到特征图，得到小数坐标；
  3. 量化：将小数坐标取整（如 3.125→3），得到整数坐标的 RoI；
  4. 池化：将量化后的 RoI 划分为 7×7 网格，每个网格最大池化，输出 512×7×7 特征图。

- **RoI Pooling 的缺陷**

  RoI Pooling 的 “量化操作” 会导致**位置偏差**（如 3.125→3，偏差 0.125），对于小目标（如特征图中 RoI 仅占 2×2 网格），偏差会导致特征裁剪不准确，影响后续分类和回归精度，这一缺陷在后续的 RoI Align 中得到解决。

**专业术语解释**

- **坐标量化（Coordinate Quantization）**：RoI Pooling 中的步骤，将映射后的小数 RoI 坐标取整为整数，以便在特征图上裁剪区域，但会引入位置偏差，导致小目标定位精度下降，是 RoI Align 技术要解决的核心问题。

### 页码 23：Cropping Features: RoI Pooling（裁剪特征：感兴趣区域池化）

- **RoI Pooling 的详细流程**

  RoI Pooling 是 Fast R-CNN 中裁剪特征的核心技术，具体步骤如下（以 2×2 池化为例）：

  1. **RoI 投影与量化**：
     - 输入图像（如 3×640×480），通过 CNN 得到特征图（如 512×20×15），分辨率比例 s=32；
     - 图像中的 RoI（如 x=100, y=50, w=200, h=150）映射到特征图的坐标为（x=3.125, y=1.5625, w=6.25, h=4.6875）；
     - 量化为整数坐标：（x=3, y=1, w=6, h=4），得到特征图上的 RoI 区域（3≤x≤8, 1≤y≤4）。
  2. **网格划分**：
     - 将量化后的 RoI（6×4）划分为 k×k 网格（如 2×2），每个网格的大小为（6/2=3, 4/2=2）；
     - 网格 1：x=3-5, y=1-2；网格 2：x=6-8, y=1-2；网格 3：x=3-5, y=3-4；网格 4：x=6-8, y=3-4。
  3. **最大池化**：
     - 对每个网格执行最大池化，取网格内特征的最大值；
     - 示例：网格 1 的最大值为 5，网格 2 为 7，网格 3 为 3，网格 4 为 9；
     - 输出：2×2 的特征图（5,7; 3,9），维度固定，可输入后续全连接层。

- **RoI Pooling 的核心作用**

  1. **统一特征维度**：无论输入 RoI 的大小如何（如 100×50 或 200×100），均输出 k×k 特征图，解决 “不同大小 RoI 无法输入全连接层” 的问题；
  2. **保留关键特征**：最大池化操作保留每个网格的最显著特征（如边缘、纹理），确保特征的判别性；
  3. **减少计算量**：相比 R-CNN 的重复特征提取，RoI Pooling 仅需一次全局特征提取，计算量大幅降低。

- **局限性：量化偏差**

  右侧图像展示量化偏差的影响：

  - 量化前的 RoI 包含目标的关键特征（如猫的头部），量化后可能因坐标取整而裁剪掉部分特征，导致小目标的特征缺失，影响检测精度。

**专业术语解释**

- **最大池化（Max Pooling）**：RoI Pooling 中使用的池化操作，对每个网格取最大值，能有效保留特征的显著信息（如边缘、角点），同时降低特征维度，减少过拟合风险，是 CNN 中常用的特征降维技术。

### 页码 24：Cropping Features: RoI Align（裁剪特征：感兴趣区域对齐）

- **核心改进：消除量化偏差**

  RoI Align 针对 RoI Pooling 的量化偏差问题，提出 “无量化裁剪 + 双线性插值” 的方法，具体步骤如下：

  1. **RoI 投影（无量化）**：
     - 与 RoI Pooling 相同，将图像 RoI 映射到特征图，但保留小数坐标（如 x=3.125, y=1.5625），不进行取整，避免坐标偏差；
  2. **网格划分（等大小）**：
     - 将映射后的 RoI（如 x=3.125-9.375, y=1.5625-6.25）划分为 k×k 等大小网格（如 2×2），每个网格的大小为（(9.375-3.125)/2=3.125, (6.25-1.5625)/2=2.34375），网格边界可能不与特征图像素对齐；
  3. **双线性插值采样**：
     - 对每个网格，在其内部按固定间隔采样固定数量的点（如 4 个点）；
     - 对每个采样点，通过双线性插值计算其特征值（利用采样点周围 4 个像素的特征加权平均）；
     - 示例：采样点（x=3.5, y=2.0）的特征值为周围 4 个像素（(3,2),(4,2),(3,3),(4,3)）的加权和，权重由采样点与像素的距离决定。
  4. **最大池化**：
     - 对每个网格的采样点特征取最大值，得到 k×k 固定大小的特征图，与 RoI Pooling 输出维度一致。

- **双线性插值的数学原理**

  对于采样点（x, y），其特征值 f (x,y) 通过周围 4 个像素（(x1,y1),(x1,y2),(x2,y1),(x2,y2)）计算：

  \(f(x,y) = \sum_{i=1}^2 \sum_{j=1}^2 w_{i,j} \cdot f(x_i,y_j)\)

  - \(w_{i,j}\)为权重，由采样点与像素的距离决定（如 x 与 x1 的距离越近，w1,j 越大）；
  - 示例：采样点（6.5,5.8）的权重计算：
    - x 方向：6.5 在 6 和 7 之间，权重分别为 0.5 和 0.5；
    - y 方向：5.8 在 5 和 6 之间，权重分别为 0.2 和 0.8；
    - 最终权重：w11=0.5×0.2=0.1，w12=0.5×0.8=0.4，w21=0.5×0.2=0.1，w22=0.5×0.8=0.4；
    - 特征值：f (6.5,5.8)=0.1×f (6,5)+0.4×f (6,6)+0.1×f (7,5)+0.4×f (7,6)。

- **RoI Align 的优势**

  1. **消除量化偏差**：无坐标取整，通过插值保留小数坐标的特征信息，小目标的定位精度提升 10%-20%；
  2. **可微性**：双线性插值是可微操作，梯度可通过采样点反向传播到特征图像素，支持端到端训练；
  3. **兼容性**：输出维度与 RoI Pooling 一致，可直接替换 Fast R-CNN 中的 RoI Pooling，无需修改其他网络结构。

**专业术语解释**

- **双线性插值（Bilinear Interpolation）**：一种图像插值技术，通过采样点周围 4 个像素的加权平均计算采样点的特征值，权重由距离决定，能平滑过渡像素值，避免量化偏差导致的特征缺失，是 RoI Align 的核心技术。

### 页码 25：Cropping Features: RoI Align（裁剪特征：感兴趣区域对齐）

- **RoI Align 的完整流程示例**

  以输入图像 3×640×480、特征图 512×20×15、RoI 映射后坐标（x=3.125-9.375, y=1.5625-6.25）、k=2 为例，RoI Align 流程如下：

  1. **网格划分**：
     - 将 RoI 划分为 2×2 网格，每个网格的大小为（3.125, 2.34375）；
     - 网格 1：x=3.125-6.25, y=1.5625-3.90625；
     - 网格 2：x=6.25-9.375, y=1.5625-3.90625；
     - 网格 3：x=3.125-6.25, y=3.90625-6.25；
     - 网格 4：x=6.25-9.375, y=3.90625-6.25。
  2. **采样与插值**：
     - 每个网格内采样 4 个点（如网格中心及四周），共 2×2×4=16 个采样点；
     - 对每个采样点（如网格 1 的 (4.6875, 2.734375)），通过双线性插值计算特征值；
     - 示例：采样点 (4.6875, 2.734375) 周围 4 个像素为 (4,2),(5,2),(4,3),(5,3)，权重分别为 0.3125×0.2656=0.082, 0.6875×0.2656=0.182, 0.3125×0.7344=0.230, 0.6875×0.7344=0.505；
     - 特征值：f=0.082×f (4,2)+0.182×f (5,2)+0.230×f (4,3)+0.505×f (5,3)。
  3. **最大池化**：
     - 对每个网格的 4 个采样点特征取最大值，如网格 1 的最大值为 6，网格 2 为 8，网格 3 为 4，网格 4 为 9；
     - 输出：512×2×2 特征图，与 RoI Pooling 输出维度一致，但特征更精准。

- **RoI Align 的实际应用**

  RoI Align 在 Mask R-CNN（实例分割模型）中首次提出，大幅提升了小目标的检测精度和实例分割的掩码精度，成为后续高精度目标检测模型（如 Faster R-CNN、Cascade R-CNN）的标准组件，尤其适用于医学影像、工业质检等对精度要求高的场景。

**专业术语解释**

- **实例分割（Instance Segmentation）**：比目标检测更精细的任务，不仅需检测目标的类别和边界框，还需为每个目标标注像素级的掩码（Mask），指示哪些像素属于该目标，RoI Align 为实例分割提供了精准的特征裁剪，是 Mask R-CNN 的核心技术之一。

### 页码 26：Fast R-CNN vs “Slow” R-CNN（快速 R-CNN 与 “慢速” R-CNN）

- **性能对比：速度与精度**

  

  下表对比 R-CNN、SPP-Net（空间金字塔池化网络）、Fast R-CNN 的关键性能指标：

| 模型       | 训练时间（小时） | 测试时间（秒 / 张，含区域提议） | 测试时间（秒 / 张，不含区域提议） | 精度（VOC 2007 mAP） |
| ---------- | ---------------- | ------------------------------- | --------------------------------- | -------------------- |
| R-CNN      | 84               | 47                              | 49                                | 66.0%                |
| SPP-Net    | 25.5             | 2.3                             | 4.3                               | 63.1%                |
| Fast R-CNN | 8.75             | 0.32                            | 2.3                               | 70.0%                |

关键结论：

1. **速度提升**：Fast R-CNN 的训练时间仅为 R-CNN 的 1/10，测试时间（含区域提议）仅为 R-CNN 的 1/147，主要得益于全局特征共享和 RoI Pooling；
2. **精度提升**：Fast R-CNN 的 mAP 比 R-CNN 高 4 个百分点，因全局特征包含更丰富的上下文信息；
3. **瓶颈仍在区域提议**：Fast R-CNN 的测试时间中，87%（2.3 秒中的 2 秒）用于 Selective Search 生成区域提议，成为新的性能瓶颈。

- **核心差异：特征提取方式**

  1. **R-CNN（慢速）**：
     - 流程：区域提议→每个 RP 独立输入 CNN→特征提取→分类 + 回归；
     - 缺陷：2000 个 RP 需 2000 次 CNN 前向传播，重复计算严重；
  2. **Fast R-CNN（快速）**：
     - 流程：CNN 提取全局特征→区域提议→RoI Pooling 裁剪特征→分类 + 回归；
     - 优势：仅 1 次 CNN 前向传播，所有 RP 共享特征，计算量大幅降低。

- **未来改进方向**

  尽管 Fast R-CNN 速度大幅提升，但区域提议仍依赖手工设计的 Selective Search，存在速度慢、鲁棒性差的问题，后续的 Faster R-CNN 通过 “用 CNN 学习区域提议” 解决这一瓶颈，实现端到端的两阶段检测。

**专业术语解释**

- **空间金字塔池化（Spatial Pyramid Pooling, SPP）**：SPP-Net 提出的技术，在 CNN 的最后一个卷积层后添加空间金字塔池化层，将不同大小的输入图像转化为固定大小的特征向量，为 Fast R-CNN 的 RoI Pooling 提供了灵感，但仍需为每个 RP 独立提取特征，速度慢于 Fast R-CNN。

### 第 27 页：Fast R-CNN 网络结构深度解析

#### 一、骨干网络（Backbone Network）

以 VGG16（Visual Geometry Group 16-layer network，视觉几何组 16 层网络）为例，其完整层级结构与参数设置如下：



1.  **卷积层组（Convolutional Layers Groups）**

*   **第一组**：2 个 3×3 卷积核（3×3 convolution kernels），输出通道数 64（output channels=64），卷积步长（stride）=1，填充（padding）=1（确保输入输出尺寸一致）；每个卷积层后紧跟 ReLU 激活函数（ReLU activation function，修正线性单元激活函数）；最后接 1 个 2×2 最大池化层（2×2 max-pooling layer），步长 = 2，无填充，实现特征图尺寸减半（从 224×224→112×112）。

*   **第二组**：2 个 3×3 卷积核，输出通道数 128，参数同第一组（步长 = 1、填充 = 1、ReLU 激活）；后续 2×2 最大池化层（步长 = 2），特征图尺寸从 112×112→56×56。

*   **第三组**：3 个 3×3 卷积核，输出通道数 256，参数同上；后续 2×2 最大池化层（步长 = 2），特征图尺寸从 56×56→28×28。

*   **第四组**：3 个 3×3 卷积核，输出通道数 512，参数同上；后续 2×2 最大池化层（步长 = 2），特征图尺寸从 28×28→14×14。

*   **第五组**：3 个 3×3 卷积核，输出通道数 512，参数同上；后续 2×2 最大池化层（步长 = 2），特征图尺寸从 14×14→7×7。

1.  **输出特征图规格**：对于输入尺寸为 224×224×3（宽 × 高 × 通道）的 RGB 图像，经过 VGG16 骨干网络后，最终输出特征图尺寸为 7×7×512，其中 512 为通道数（特征维度），7×7 为空间分辨率。

#### 二、区域网络（Per-region Network，单区域处理网络）



1.  **全连接层（Fully Connected Layers, FC Layers）**

*   **第一层全连接层（FC1）**：输入为 RoI 池化后输出的 7×7×512 特征图，先通过展平操作（flatten operation）将三维特征图转换为一维向量（7×7×512=25088 维），再映射到 4096 维向量（4096-dimensional vector）；输出通道数 = 4096，后接 ReLU 激活函数， dropout 概率（dropout rate）=0.5（防止过拟合）。

*   **第二层全连接层（FC2）**：输入为 4096 维向量，输出通道数 = 4096，同样接 ReLU 激活函数与 dropout（概率 = 0.5）。

*   **第三层全连接层（FC3）**：分为两个并行分支（parallel branches），分别负责类别预测与边界框回归。

1.  **输出分支（Output Branches）**

*   **类别得分分支（Class Score Branch）**：输出通道数 =（C+1），其中 C 为待检测目标类别数量（如 PASCAL VOC 数据集 C=20，包含人、车、猫等 20 类），1 表示背景类别（background class，无目标区域）；通过 softmax 函数（softmax function）将输出转换为概率分布（probability distribution），即每个类别（含背景）的预测概率。

*   **边界框变换参数分支（Bounding Box Regression Parameters Branch）**：输出通道数 = 4×（C+1），每个类别（含背景）对应 4 个边界框调整参数（tx, ty, tw, th），分别表示：


    *   tx：x 方向平移量（x-direction translation，相对于提议框中心 x 坐标的偏移）
    
    *   ty：y 方向平移量（y-direction translation，相对于提议框中心 y 坐标的偏移）
    
    *   tw：宽度缩放因子（width scaling factor，提议框宽度的调整比例）
    
    *   th：高度缩放因子（height scaling factor，提议框高度的调整比例）

### 第 28 页：Fast R-CNN 特征裁剪核心问题与解决方案

#### 一、核心问题：特征图与区域提议的空间映射（Spatial Mapping）



1.  **问题描述**：区域提议（region proposals）是在原始图像（original image）上通过选择性搜索（Selective Search）生成的矩形区域（通常约 2000 个），而特征图是原始图像经过骨干网络卷积、池化后的低分辨率特征表示（low-resolution feature representation）。如何将原始图像上的区域提议**准确映射**到特征图上，并裁剪出对应的特征子区域（feature sub-region），是 Fast R-CNN 实现 “共享特征计算” 的关键前提。

2.  **映射原理（Mapping Principle）**

*   设原始图像尺寸为 W×H（宽 × 高），骨干网络输出特征图尺寸为 w×h，由于卷积层步长均为 1、池化层步长均为 2（共 5 次池化），原始图像与特征图的**缩放比例**（scaling ratio）s=W/w=H/h=2^5=32（忽略边缘填充影响时）。

*   对于原始图像上任意区域提议，其左上角坐标为（x1\_img, y1\_img）、右下角坐标为（x2\_img, y2\_img），则该区域提议映射到特征图上的坐标为：

$ 
     x1_{feat} = \left\lfloor \frac{x1_{img}}{s} \right\rfloor, \quad y1_{feat} = \left\lfloor \frac{y1_{img}}{s} \right\rfloor
      $

$ 
     x2_{feat} = \left\lfloor \frac{x2_{img}}{s} \right\rfloor, \quad y2_{feat} = \left\lfloor \frac{y2_{img}}{s} \right\rfloor
      $

其中$\lfloor \cdot \rfloor$表示向下取整操作（floor operation），目的是将非整数坐标转换为特征图上可操作的整数像素坐标（integer pixel coordinates）。

#### 二、特征裁剪与尺寸调整流程（Feature Cropping and Resizing Process）



1.  **步骤 1：区域提议映射（Region Proposal Mapping）**：按照上述缩放比例 s=32，将原始图像上的 2000 个左右区域提议，逐一映射到骨干网络输出的 7×7×512 特征图上，得到特征图上的候选区域（candidate regions），每个候选区域的尺寸为（x2\_feat - x1\_feat + 1）×（y2\_feat - y1\_feat + 1）×512（宽 × 高 × 通道）。

2.  **步骤 2：特征区域裁剪（Feature Region Cropping）**：在特征图上，根据候选区域的坐标（x1\_feat, y1\_feat, x2\_feat, y2\_feat），裁剪出对应的特征子图（feature submap）。由于原始图像上的区域提议尺寸不同（如有的为 128×128，有的为 256×256），映射后特征子图的尺寸也不同（如 128/32=4→4×4×512，256/32=8→8×8×512）。

3.  **步骤 3：尺寸统一调整（Unified Resizing）**：后续全连接层（FC1）需要固定尺寸的输入（25088 维，对应 7×7×512），因此需将不同尺寸的特征子图调整为 7×7×512。常用方法为**双线性插值（bilinear interpolation）**，其原理是通过相邻 4 个像素点的加权平均计算目标位置的像素值，避免直接拉伸 / 压缩导致的特征失真（feature distortion）。

4.  **步骤 4：送入区域网络（Feeding to Per-region Network）**：将统一尺寸为 7×7×512 的特征子图送入区域网络的全连接层，依次经过 FC1、FC2 处理后，由两个输出分支分别输出类别概率与边界框调整参数。

### 第 29 页：RoI 池化（Region of Interest Pooling，感兴趣区域池化）技术原理

#### 一、核心思想：固定尺寸特征提取（Fixed-size Feature Extraction）

RoI 池化的核心目标是：**无论输入的感兴趣区域（RoI）在特征图上的尺寸大小如何，均能输出固定尺寸的特征图**（如 7×7×512），为后续全连接层提供统一输入格式，同时保留 RoI 区域内的关键特征信息（key feature information），如目标的边缘、纹理等。

#### 二、详细步骤（Detailed Steps）



1.  **步骤 1：RoI 映射与坐标确定（RoI Mapping and Coordinate Determination）**

*   输入：骨干网络输出的特征图 F（尺寸 H×W×C，如 7×7×512）；感兴趣区域 RoI（在特征图上的坐标为（x\_start, y\_start, x\_end, y\_end），其中 x\_start、y\_start 为左上角坐标，x\_end、y\_end 为右下角坐标）。

*   计算 RoI 在特征图上的宽度与高度：

$ 
     w_{roi} = x_{end} - x_{start} + 1, \quad h_{roi} = y_{end} - y_{start} + 1
      $



*   目标输出尺寸：H\_out×W\_out（如 7×7），即需将 RoI 划分为 H\_out×W\_out 个均匀的子区域（uniform sub-regions）。

1.  **步骤 2：子区域划分（Sub-region Division）**

*   计算每个子区域的理想宽度与高度（可能为非整数）：

$ 
     w_{sub\_ideal} = \frac{w_{roi}}{W_{out}}, \quad h_{sub\_ideal} = \frac{h_{roi}}{H_{out}}
      $



*   由于子区域尺寸必须为整数（对应特征图上的像素区域），采用 “向下取整 + 边缘调整” 策略：


    *   前（W\_out - 1）个子区域的宽度为$\lfloor w_{sub\_ideal} \rfloor$（向下取整），最后 1 个子区域的宽度为$w_{roi} - (W_out - 1) \times \lfloor w_{sub\_ideal} \rfloor$。
    
    *   高度划分同理：前（H\_out - 1）个子区域的高度为$\lfloor h_{sub\_ideal} \rfloor$，最后 1 个子区域的高度为$h_{roi} - (H_out - 1) \times \lfloor h_{sub\_ideal} \rfloor$。

*   示例：若 RoI 在特征图上尺寸为 10×10（w\_roi=10, h\_roi=10），目标输出尺寸为 3×3（W\_out=3, H\_out=3），则：


    *   子区域理想宽度 = 10/3≈3.333，前 2 个子区域宽度 = 3，最后 1 个 = 10 - 2×3=4。
    
    *   子区域理想高度 = 10/3≈3.333，前 2 个子区域高度 = 3，最后 1 个 = 4。
    
    *   最终划分的 9 个子区域尺寸为：3×3（4 个）、4×3（2 个）、3×4（2 个）、4×4（1 个），完整覆盖 10×10 的 RoI 区域。

1.  **步骤 3：最大池化操作（Max-pooling Operation）**

*   对每个子区域，遍历其内部所有像素点的特征值，选取最大值（maximum value）作为该子区域的池化输出值（pooled output value）。

*   数学表达式：对于第 i 行（i∈\[0, H\_out-1]）、第 j 列（j∈\[0, W\_out-1]）的子区域，其输出值为：

$ 
     \text{pooled}(i, j) = \max_{(x, y) \in \text{sub-region}(i, j)} F(x, y)
      $

其中 F (x, y) 为特征图 F 在（x, y）位置的特征值，sub-region (i, j) 表示第 i 行第 j 列的子区域所包含的像素坐标集合。



1.  **步骤 4：输出固定尺寸特征图（Output Fixed-size Feature Map）**

*   将 H\_out×W\_out 个池化输出值按 “行优先” 顺序排列（先第 1 行所有子区域，再第 2 行，以此类推），形成尺寸为 H\_out×W\_out×C 的固定尺寸特征图（如 7×7×512）。该特征图保留了 RoI 区域内的最强特征（如子区域内的最大边缘响应），可直接送入全连接层进行后续处理。

#### 三、优点（Advantages）



1.  **尺寸无关性（Size Independence）**：无论输入 RoI 的尺寸如何（如 4×4、8×8、10×10），均能输出固定尺寸的特征图，完美适配全连接层的输入要求。

2.  **计算高效性（Computational Efficiency）**：仅对 RoI 对应的特征子图进行池化操作，无需对整个特征图进行处理，避免了冗余计算（redundant computation），相比 R-CNN 的 “每个 RoI 单独过 CNN” 大幅提升速度。

3.  **特征保留性（Feature Retention）**：最大池化操作能保留子区域内的最强特征，这些特征通常与目标的类别属性（如 “猫的耳朵形状”“车的轮胎轮廓”）高度相关，有助于后续的类别判断与边界框回归。

### 第 30 页：RoI 池化的局限性（Limitations of RoI Pooling）（续）

#### 二、实例分析（Example Analysis）（续）



1.  **输入条件（Input Conditions）**（续）

*   骨干网络：采用 VGG16，经过 5 次池化后特征图尺寸为 20×15（640/32=20，480/32=15），缩放比例 s=32。

*   区域提议映射：原始图像上小目标 RoI 的坐标 \[100,100,132,132]，映射到特征图的理想坐标为 \[100/32=3.125, 100/32=3.125, 132/32=4.125, 132/32=4.125]，理想尺寸为 1.0×1.0（4.125-3.125=1.0）。

1.  **对齐误差导致的特征偏差**

*   RoI 池化处理：对理想坐标向下取整，得到特征图上实际 RoI 坐标为 \[3,3,4,4]，实际尺寸为 1×1（4-3=1）。

*   特征差异：理想 RoI 区域包含特征图上（3.125,3.125）附近的 “信号灯边缘特征”，而实际 RoI 区域仅包含（3,3）位置的 “背景纹理特征”。池化后提取的特征完全偏离目标关键特征，导致后续分类分支误判为 “背景”，边界框回归分支无法生成准确调整参数。

#### 三、对检测精度的影响（Impact on Detection Accuracy）



1.  **小目标检测失效**：小目标在特征图上的 RoI 尺寸通常≤3×3，对齐误差占比高（如 1 个像素误差占 3×3 区域的 11%），导致 RoI 池化提取的特征信噪比（signal-to-noise ratio）极低，小目标召回率（recall）可下降 10%-20%。

2.  **边界框回归精度下降**：以 PASCAL VOC 数据集为例，采用 RoI 池化的 Fast R-CNN，边界框与真实框（ground truth）的平均 IoU（mean IoU）比理想无误差情况低 5%-8%，尤其对细长型目标（如 “铅笔”“电线杆”）影响更显著。

### 第 31 页：RoI 对齐（RoI Align）技术：解决对齐误差的方案

#### 一、核心改进思想（Core Improvement Idea）

摒弃 RoI 池化的 “坐标整数化” 操作，保留映射后的非整数坐标（non-integer coordinates），通过**双线性插值（bilinear interpolation）** 计算非整数位置的特征值，实现 RoI 与特征图的精确空间对齐（accurate spatial alignment），彻底消除像素对齐误差。

#### 二、详细实现步骤（Detailed Implementation Steps）



1.  **步骤 1：RoI 精确映射（Accurate RoI Mapping）**

*   输入：特征图 F（H×W×C），原始图像 RoI 坐标（x1\_img,y1\_img,x2\_img,y2\_img）。

*   映射公式：直接保留非整数坐标，无需取整：

$ 
     x1_{feat} = \frac{x1_{img}}{s}, \quad y1_{feat} = \frac{y1_{img}}{s}, \quad x2_{feat} = \frac{x2_{img}}{s}, \quad y2_{feat} = \frac{y2_{img}}{s}
      $



*   计算 RoI 在特征图上的宽高：$w_{roi}=x2_{feat}-x1_{feat}$，$h_{roi}=y2_{feat}-y1_{feat}$，目标输出尺寸 H\_out×W\_out（如 7×7）。

1.  **步骤 2：子区域划分与采样点设置**

*   子区域划分：将 RoI 均匀划分为 H\_out×W\_out 个子区域，每个子区域的宽$w_{sub}=w_{roi}/W_out$、高$h_{sub}=h_{roi}/H_out$（允许非整数）。

*   采样点设置：每个子区域内设置 k×k 个采样点（通常 k=2，即 4 个采样点），采样点均匀分布在子区域内。例如，子区域左上角（a,b）、右下角（a+w\_sub,b+h\_sub），则 4 个采样点坐标为：

$ 
     (a+\frac{w_sub}{4}, b+\frac{h_sub}{4}),\ (a+\frac{3w_sub}{4}, b+\frac{h_sub}{4}),\ (a+\frac{w_sub}{4}, b+\frac{3h_sub}{4}),\ (a+\frac{3w_sub}{4}, b+\frac{3h_sub}{4})
      $



1.  **步骤 3：双线性插值计算采样点特征**

*   插值原理：对于采样点（x,y）（非整数），其特征值由周围 4 个整数坐标像素（x0,y0）、（x0+1,y0）、（x0,y0+1）、（x0+1,y0+1）的特征值加权计算，其中$x0=\lfloor x \rfloor$，$y0=\lfloor y \rfloor$，dx=x-x0，dy=y-y0。

*   插值公式：

$ 
     F(x,y) = (1-dx)(1-dy)F(x0,y0) + dx(1-dy)F(x0+1,y0) + (1-dx)dyF(x0,y0+1) + dxdyF(x0+1,y0+1)
      $



*   示例：采样点（3.125,3.125），x0=3，y0=3，dx=0.125，dy=0.125，则：

$ 
     F(3.125,3.125) = 0.875Ã0.875ÃF(3,3) + 0.125Ã0.875ÃF(4,3) + 0.875Ã0.125ÃF(3,4) + 0.125Ã0.125ÃF(4,4)
      $



1.  **步骤 4：子区域池化与输出**

*   池化操作：对每个子区域内 k×k 个采样点的特征值进行最大池化（max-pooling），取最大值作为子区域输出。

*   输出特征图：将 H\_out×W\_out 个子区域的输出按顺序排列，得到固定尺寸 H\_out×W\_out×C 的特征图，无对齐误差，可直接送入全连接层。

#### 三、精度提升效果（Accuracy Improvement Effect）

在 COCO 数据集（Common Objects in Context）上，采用 RoI 对齐的 Faster R-CNN 相比 RoI 池化版本：



*   小目标 AP（Average Precision）提升 12%-15%；

*   边界框平均 IoU 提升 8%-10%；

*   整体 mAP（mean Average Precision）提升 4%-6%。

### 第 32 页：双线性插值（Bilinear Interpolation）深度解析

#### 一、数学原理（Mathematical Principle）

双线性插值是**二维线性插值（2D linear interpolation）** 的扩展，基于 “先沿 x 轴插值，再沿 y 轴插值” 的两步法，假设特征值在局部区域内呈线性分布（linear distribution）。



1.  **第一步：x 轴方向插值（Interpolation along x-axis）**

*   对于固定 y=y0，计算 x=x0 和 x=x0+1 处的特征值在 x 位置的插值：

$ 
     F(x,y0) = (x0+1 - x)F(x0,y0) + (x - x0)F(x0+1,y0)
      $



*   同理，计算 y=y0+1 处的插值：

$ 
     F(x,y0+1) = (x0+1 - x)F(x0,y0+1) + (x - x0)F(x0+1,y0+1)
      $



1.  **第二步：y 轴方向插值（Interpolation along y-axis）**

*   基于 x 轴插值结果，计算 y 位置的最终特征值：

$ 
     F(x,y) = (y0+1 - y)F(x,y0) + (y - y0)F(x,y0+1)
      $



1. **合并公式（Combined Formula）**

   将两步插值合并，得到双线性插值最终公式（与第 31 页一致），本质是 4 个周围像素的加权和，权重由采样点与像素的距离决定（距离越近，权重越大）。

#### 二、可微性优势（Differentiability Advantage）



*   双线性插值函数为**连续可微函数（continuously differentiable function）**，梯度（gradient）可通过链式法则（chain rule）反向传播（backpropagation），适配端到端（end-to-end）训练。

*   对比 RoI 池化：坐标取整操作是离散的 “阶跃函数”，不可微，需通过近似梯度（approximate gradient）优化，训练稳定性差。

#### 三、工程实现细节（Engineering Implementation Details）



1.  **边界处理（Boundary Handling）**：当采样点坐标超出特征图范围（如 x<0 或 x≥W），采用 “镜像填充（mirror padding）” 或 “常数填充（constant padding）”，避免特征值计算错误。

2.  **计算效率优化（Computational Efficiency Optimization）**：通过向量化操作（vectorization）批量计算所有采样点的特征值，避免循环迭代，在 GPU 上可实现 10 倍以上的计算加速。

### 第 33 页：RoI 对齐完整流程与实例

#### 一、完整流程框图（Complete Flowchart）



```mermaid
graph TD
    A[原始图像RoI] --> B[按缩放比s映射到特征图（非整数坐标）]
    B --> C[划分H_out×W_out个子区域（非整数尺寸）]
    C --> D[每个子区域设置k×k个采样点]
    D --> E[双线性插值计算采样点特征值]
    E --> F[子区域内最大池化]
    F --> G[输出固定尺寸特征图（H_out×W_out×C）]
```

#### 二、实例：小目标 RoI 对齐过程



1.  **输入条件**

*   原始图像 RoI：坐标 \[100,100,132,132]（宽 32× 高 32），s=32，映射到特征图的 RoI 坐标 \[3.125,3.125,4.125,4.125]（w\_roi=1.0，h\_roi=1.0）。

*   目标输出尺寸：1×1（H\_out=1，W\_out=1），k=2（4 个采样点）。

1.  **步骤拆解**

*   子区域划分：1×1 个子区域，尺寸 1.0×1.0，采样点坐标为（3.375,3.375）、（3.875,3.375）、（3.375,3.875）、（3.875,3.875）。

*   双线性插值：计算 4 个采样点的特征值，例如（3.375,3.375）的特征值由 F (3,3)、F (4,3)、F (3,4)、F (4,4) 加权得到。

*   最大池化：取 4 个采样点特征值的最大值，作为 1×1 特征图的输出值，该值准确包含 “信号灯边缘特征”。

1.  **结果对比**

*   RoI 池化输出：仅包含 F (3,3) 的背景特征，分类准确率 0%。

*   RoI 对齐输出：包含目标边缘特征，分类准确率 95%，边界框回归 IoU 从 0.4 提升至 0.85。

### 第 34 页：Fast R-CNN vs “Slow” R-CNN（性能对比）

#### 一、核心差异：特征计算方式（Core Difference: Feature Computation）



| 对比维度             | “Slow” R-CNN（R-CNN）                   | Fast R-CNN                                  |
| -------------------- | --------------------------------------- | ------------------------------------------- |
| 特征计算方式         | 每个 RoI 单独通过 CNN 提取特征          | 整图先过 CNN 生成特征图，再裁剪 RoI 特征    |
| 计算冗余度           | 高（重叠 RoI 重复计算特征）             | 低（特征图共享计算）                        |
| 前向传播时间（单图） | 84 秒（2000 个 RoI，每个 RoI 0.042 秒） | 0.3 秒（整图 CNN 0.2 秒 + RoI 处理 0.1 秒） |
| 模型存储需求         | 大（每个类别 SVM 单独存储）             | 小（端到端模型，仅存 CNN 参数）             |

#### 二、训练流程差异（Training Process Difference）



1.  **“Slow” R-CNN 训练流程**

*   步骤 1：用 ImageNet 预训练 CNN（如 AlexNet）。

*   步骤 2：对每个 RoI 裁剪图像块，输入 CNN 提取 4096 维特征，存储特征向量。

*   步骤 3：训练 SVM 分类器（每个类别 1 个 SVM），区分 “类别 / 背景”。

*   步骤 4：训练边界框回归器（每个类别 1 个回归器），调整 RoI 坐标。

*   问题：分阶段训练（4 个独立步骤），无法端到端优化，特征与分类 / 回归任务适配性差。

1.  **Fast R-CNN 训练流程**

*   步骤 1：用 ImageNet 预训练 CNN，移除最后全连接层，添加 RoI 池化层和新全连接层。

*   步骤 2：输入整图和对应的 RoI 标签（类别 + 真实框），前向传播计算分类损失和回归损失。

*   步骤 3：通过反向传播更新所有层参数（从 CNN 到分类 / 回归分支），端到端优化。

*   优势：单阶段训练，损失函数联合优化，特征与任务高度适配，训练效率提升 100 倍。

#### 三、精度对比（Accuracy Comparison）

在 PASCAL VOC 2007 测试集上（输入图像尺寸 480×640）：



| 模型         | 类别数 | mAP@0.5 | 小目标 AP | 中目标 AP | 大目标 AP |
| ------------ | ------ | ------- | --------- | --------- | --------- |
| “Slow” R-CNN | 20     | 66.0%   | 42.1%     | 69.2%     | 82.3%     |
| Fast R-CNN   | 20     | 70.0%   | 48.5%     | 72.8%     | 84.1%     |
| 提升幅度     | -      | +4.0%   | +6.4%     | +3.6%     | +1.8%     |

### 第 35 页：Fast R-CNN 的性能瓶颈与优化方向

Fast R-CNN 作为两阶段检测器的关键演进版本，虽解决了 R-CNN 的计算冗余问题，但仍存在核心性能瓶颈，其优化方向直接催生了 Faster R-CNN 的诞生。

#### 一、核心瓶颈：区域提议生成环节（Core Bottleneck: Region Proposal Generation）

Fast R-CNN 的检测流程分为 “区域提议生成” 和 “检测头分类回归” 两部分，其中**区域提议生成**是制约速度的关键：



1.  **性能耗时量化分析**

*   单图像检测总耗时：约 0.3 秒（基于 VGG16 骨干网络，GPU：NVIDIA Tesla K40）。

*   各环节耗时占比：


    *   区域提议生成（选择性搜索，Selective Search）：0.24 秒，占比 80%；
    
    *   CNN 特征提取：0.04 秒，占比 13%；
    
    *   RoI 池化 + 检测头（分类 + 回归）：0.02 秒，占比 7%。

*   高分辨率图像适配问题：当输入图像分辨率从 480×640 提升至 1280×960 时，选择性搜索耗时增至 2-3 秒 / 图，远无法满足实时检测需求（实时性要求≥10 FPS，即单图耗时≤0.1 秒）。

1.  **瓶颈根源的技术解析**

*   **传统方法的算力局限**：选择性搜索基于传统图像处理技术，流程为 “图像分割→区域相似度计算→层次化合并”，依赖颜色、纹理、尺寸等手工特征（hand-crafted features），且全程在 CPU 串行执行（serial execution），无法利用 GPU 的并行计算能力（parallel computing），导致效率低下。

*   **超参数依赖与泛化性差**：选择性搜索的提议质量依赖人工调整超参数（如颜色相似度阈值、纹理权重），在不同数据集（如 VOC 的通用目标、KITTI 的自动驾驶目标）上需重新调参，泛化性（generalization）不足。

*   **无法端到端优化**：选择性搜索与后续 CNN 网络是独立模块，无法通过反向传播（backpropagation）联合优化，导致提议质量与检测精度无法协同提升（如提议框与真实框 IoU 偏低时，检测头难以修正）。

#### 二、关键优化方向：用 CNN 替代传统区域提议

针对选择性搜索的缺陷，核心优化思路是**设计区域提议网络（Region Proposal Network, RPN）**，将区域提议生成融入 CNN 框架，实现 “特征共享 + 端到端优化”：



1.  **RPN 的核心设计逻辑**

*   特征共享：RPN 与 Fast R-CNN 共享骨干网络（如 VGG16）输出的特征图，无需对原始图像单独提取特征，彻底消除重复计算（redundant computation）。

*   卷积化预测：在共享特征图上通过滑动卷积核，直接预测 “前景 / 背景得分”（objectness score）和 “边界框调整参数”，替代传统手工特征的相似度计算。

1.  **RPN 的性能优势验证**

*   速度提升：RPN 在 GPU 上并行运行，区域提议生成耗时从 0.24 秒 / 图降至 0.01 秒 / 图，单图像总检测耗时从 0.3 秒降至 0.05 秒（20 FPS），满足实时检测需求。

*   精度协同提升：通过端到端联合优化，RPN 生成的提议框与真实框的平均 IoU（mean IoU）从选择性搜索的 0.58 提升至 0.72，为后续检测头提供更高质量的候选区域。

#### 三、从 Fast R-CNN 到 Faster R-CNN 的演进

Faster R-CNN 的本质是 “RPN+Fast R-CNN” 的深度整合，形成一体化检测框架：



*   **架构整合**：将 RPN 作为内置模块，与 Fast R-CNN 共享骨干网络特征图，形成 “特征提取→提议生成→分类回归” 的端到端流程。

*   **实验性能对比（PASCAL VOC 2007 测试集）**：



| 模型         | 骨干网络 | mAP@0.5 | 单图检测耗时（秒） | 区域提议耗时（秒） |
| ------------ | -------- | ------- | ------------------ | ------------------ |
| R-CNN        | AlexNet  | 66.0%   | 84.0               | 83.5（选择性搜索） |
| Fast R-CNN   | VGG16    | 70.0%   | 0.3                | 0.24（选择性搜索） |
| Faster R-CNN | VGG16    | 73.2%   | 0.05               | 0.01（RPN）        |

### 第 36 页：Faster R-CNN 的完整架构

Faster R-CNN 通过 “骨干网络 + RPN+RoI 池化 + 检测头” 的四模块协同，实现 “高精度 + 高速度” 的目标检测，其架构设计是两阶段检测器的经典范式。

#### 一、整体架构框架与数据流

Faster R-CNN 的数据流遵循 “串行 + 并行” 结合的逻辑，具体架构框图如下：



```mermaid
graph TD
    A[输入图像（如600×800×3）] --> B[图像预处理：归一化+缩放]
    B --> C[骨干网络（Backbone）：特征提取]
    C --> D[共享特征图（如18×25×512）]
    D --> E[区域提议网络（RPN）]
    E --> F[筛选后区域提议（2000个/训练，300个/测试）]
    D --> G[RoI池化层：特征子图裁剪与尺寸统一]
    F --> G
    G --> H[固定尺寸特征图（如7×7×512）]
    H --> I[检测头（Detection Head）：全连接层+分支预测]
    I --> J[类别预测：C+1类概率分布（softmax输出）]
    I --> K[边界框回归：4×(C+1)个调整参数]
    J --> L[最终检测结果：类别+边界框（x1,y1,x2,y2）+置信度]
    K --> L
```

#### 二、各核心模块的详细设计



1.  **骨干网络（Backbone Network）**

*   **功能定位**：提取输入图像的高维语义特征（high-level semantic features），为 RPN 和检测头提供共享特征基础。

*   **典型网络与参数（以 VGG16 为例）**：



| 网络层级          | 卷积核参数         | 输出通道数 | 下采样率 | 输出特征图尺寸（输入 600×800） |
| ----------------- | ------------------ | ---------- | -------- | ------------------------------ |
| 卷积块 1（conv1） | 2×3×3/1, padding=1 | 64         | 1        | 600×800                        |
| 池化层 1（pool1） | 2×2/2              | -          | 2        | 300×400                        |
| 卷积块 2（conv2） | 2×3×3/1, padding=1 | 128        | 1        | 300×400                        |
| 池化层 2（pool2） | 2×2/2              | -          | 4        | 150×200                        |
| 卷积块 3（conv3） | 3×3×3/1, padding=1 | 256        | 1        | 150×200                        |
| 池化层 3（pool3） | 2×2/2              | -          | 8        | 75×100                         |
| 卷积块 4（conv4） | 3×3×3/1, padding=1 | 512        | 1        | 75×100                         |
| 池化层 4（pool4） | 2×2/2              | -          | 16       | 37×50                          |
| 卷积块 5（conv5） | 3×3×3/1, padding=1 | 512        | 1        | 37×50                          |
| 池化层 5（pool5） | 2×2/2              | -          | 32       | 18×25（最终共享特征图）        |



*   **关键特性**：通过 5 次池化实现总下采样率 32，将原始图像尺寸压缩 32 倍，既保留足够语义信息，又降低后续计算量。

1.  **区域提议网络（RPN）**

*   **核心功能**：在共享特征图上生成高质量区域提议，输出 “前景 / 背景得分” 和 “边界框坐标”。

*   **网络结构细节**：


    *   3×3 卷积层：输入共享特征图（18×25×512），卷积核数量 256，步长 1， padding=1，输出特征图尺寸保持 18×25×256，作用是融合局部特征，增强提议预测能力。
    
    *   并行 1×1 卷积分支：


        *   分类分支（前景 / 背景二分类）：输出通道数 2K（K=9，每个位置 9 个锚点框），即 18×25×18，每个锚点框对应 “前景得分” 和 “背景得分”，后续通过 sigmoid 函数转换为概率（范围 \[0,1]）。
    
        *   回归分支（边界框调整）：输出通道数 4K，即 18×25×36，每个锚点框对应 4 个调整参数（tx, ty, tw, th），用于修正锚点框坐标。

1.  **RoI 池化层（Region of Interest Pooling）**

* **核心问题解决**：RPN 生成的区域提议尺寸各异（如原始图像上 128×128、256×256），需转换为固定尺寸特征图以适配检测头的全连接层。

* **三步实现流程**：

  步骤 1：区域映射（Region Mapping）

  将原始图像上的区域提议坐标（x1\_img, y1\_img, x2\_img, y2\_img）按总下采样率 s=32，映射到共享特征图上，得到特征图上的 RoI 坐标：

$ 
     x1_{feat} = \left\lfloor \frac{x1_{img}}{s} \right\rfloor, \quad y1_{feat} = \left\lfloor \frac{y1_{img}}{s} \right\rfloor
      $

$ 
     x2_{feat} = \left\lfloor \frac{x2_{img}}{s} \right\rfloor, \quad y2_{feat} = \left\lfloor \frac{y2_{img}}{s} \right\rfloor
      $

（$\lfloor \cdot \rfloor$表示向下取整，确保坐标为整数像素位置）

步骤 2：子区域划分（Sub-region Division）

设目标输出尺寸为 H×W（如 7×7），计算特征图上 RoI 的宽高：

$ 
     w_{roi} = x2_{feat} - x1_{feat} + 1, \quad h_{roi} = y2_{feat} - y1_{feat} + 1
      $

将 RoI 均匀划分为 H×W 个子区域，每个子区域的宽高为：

$ 
     w_{sub} = \left\lfloor \frac{w_{roi}}{W} \right\rfloor, \quad h_{sub} = \left\lfloor \frac{h_{roi}}{H} \right\rfloor
      $

若$w_{roi}$无法被 W 整除，将剩余像素分配给最后一个子区域（如 w\_roi=10，W=3，则前 2 个子区域宽 3，最后 1 个宽 4），确保子区域完整覆盖 RoI。

步骤 3：最大池化（Max-Pooling）

对每个子区域，计算所有像素的最大特征值，作为该子区域的输出值，最终形成 H×W×C 的固定尺寸特征图（如 7×7×512）。



1.  **检测头（Detection Head）**

*   **功能定位**：对固定尺寸特征图进行 “类别判断” 和 “边界框精修”，输出最终检测结果。

*   **网络结构与输出**：


    *   全连接层：


        *   FC6：输入 7×7×512=25088 维，输出 4096 维，接 ReLU 激活函数和 dropout（概率 0.5，防止过拟合）。
    
        *   FC7：输入 4096 维，输出 4096 维，同样接 ReLU 和 dropout。
    
    *   并行输出分支：


        *   类别预测分支：FC7 输出经 1×1 卷积（输出通道数 C+1，C 为目标类别数，如 VOC 数据集 C=20），再通过 softmax 函数输出 C+1 类的概率分布（含背景类）。
    
        *   边界框回归分支：FC7 输出经 1×1 卷积（输出通道数 4×(C+1)），为每个类别预测一组边界框调整参数（tx, ty, tw, th），实现对区域提议的精修。

#### 三、架构的核心创新点



1.  **特征共享机制**：RPN 与检测头共享骨干网络特征图，相比 “RPN 单独提取特征” 减少 50% 计算量，是速度提升的关键。

2.  **锚点框适配策略**：通过多尺度（128²、256²、512²）、多比例（1:1、2:1、1:2）锚点框，覆盖不同尺寸和形状的目标，小目标提议召回率提升 15%-20%。

3.  **端到端联合优化**：RPN 损失与检测头损失联合计算，梯度反向传播更新所有模块参数，使提议质量与检测精度协同提升，避免传统两阶段的模块割裂问题。



### 第 37 页：区域提议网络（RPN）的深度解析（续）

#### 一、RPN 的完整工作流程（续）



1.  **步骤 1：锚点框生成（Anchor Box Generation）**（续）

*   **总锚点数量计算**：共享特征图尺寸 18×25，总像素位置数量 = 18×25=450 个，每个位置生成 9 个锚点框，因此总锚点框数量 = 450×9=4050 个。

*   **边界外锚点过滤**：由于部分锚点框生成时会超出原始图像边界（如图像边缘的锚点框），需过滤掉这些无效锚点框。以 600×800 输入图像为例，边界外锚点框约占总数量的 12%-15%，最终有效锚点框数量约为 3400-3500 个。

1.  **步骤 2：特征映射与分支预测（Feature Mapping and Branch Prediction）**

*   **3×3 卷积特征融合**：对共享特征图（18×25×512）进行 3×3 卷积操作，卷积核数量 256，步长 1，padding=1（确保输出特征图尺寸与输入一致），输出特征图尺寸为 18×25×256。该步骤的核心作用是融合每个滑动位置的局部空间特征（local spatial features），减少背景噪声干扰，为后续锚点框的分类和回归提供更具判别性的特征表示。

*   **并行分支预测实现**：


    *   **分类分支（前景 / 背景二分类）**：


        *   网络结构：18×25×256 特征图经 1×1 卷积（输出通道数 2K=18，K=9），得到 18×25×18 的特征图。
    
        *   维度转换：将 18×25×18 的特征图 reshape 为 (18×25×9, 2)，即每个锚点框对应 2 个得分（前景得分、背景得分）。
    
        *   概率转换：通过 sigmoid 激活函数将得分转换为概率（范围 \[0,1]），概率值越接近 1，表明锚点框包含目标的可能性越高。
    
    *   **回归分支（边界框调整参数预测）**：


        *   网络结构：18×25×256 特征图经 1×1 卷积（输出通道数 4K=36），得到 18×25×36 的特征图。
    
        *   维度转换：将 18×25×36 的特征图 reshape 为 (18×25×9, 4)，即每个锚点框对应 4 个调整参数（tx, ty, tw, th），分别表示 x 方向平移量、y 方向平移量、宽度缩放因子、高度缩放因子。

1.  **步骤 3：边界框转换（Bounding Box Transformation）**

*   **目标**：利用回归分支预测的调整参数，将原始锚点框（Anchor Box）修正为更接近真实目标的区域提议框（Proposal Box）。

*   **坐标定义**：


    *   原始锚点框：$A = (x_A, y_A, w_A, h_A)$，其中$x_A$、$y_A$为锚点框中心坐标，$w_A$、$h_A$为锚点框宽高（基于原始图像尺寸）。
    
    *   预测调整参数：$t = (t_x, t_y, t_w, t_h)$，由 RPN 回归分支输出。
    
    *   转换后提议框：$P = (x_P, y_P, w_P, h_P)$，为最终生成的区域提议框坐标（原始图像尺寸）。

* **转换公式推导**：

  为确保调整参数与锚点框尺寸无关（scale-invariant），避免大尺寸锚点框的调整误差被放大，引入归一化转换：

$ 
     x_P = x_A + w_A \times t_x
      $

$ 
     y_P = y_A + h_A \times t_y
      $

$ 
     w_P = w_A \times \exp(t_w)
      $

$ 
     h_P = h_A \times \exp(t_h)
      $



*   公式解读：$t_x$、$t_y$是相对于锚点框宽度 / 高度的 “归一化平移量”，确保平移幅度与锚点框尺寸适配；$t_w$、$t_h$通过指数函数转换为 “缩放因子”，确保$w_P$、$h_P$始终为正值（因$\exp(t_w) > 0$）。

<!---->

*   **坐标格式转换**：将提议框的中心坐标（$x_P, y_P$）和宽高（$w_P, h_P$）转换为检测任务常用的 “左上角 - 右下角” 坐标（$x1_P, y1_P, x2_P, y2_P$）：

$ 
     x1_P = x_P - \frac{w_P}{2}, \quad y1_P = y_P - \frac{h_P}{2}
      $

$ 
     x2_P = x_P + \frac{w_P}{2}, \quad y2_P = y_P + \frac{h_P}{2}
      $



1.  **步骤 4：提议框筛选（Proposal Box Filtering）**

*   **筛选目标**：从 3400-3500 个有效提议框中筛选出高质量、无冗余的区域提议，减少后续检测头的计算量，同时保证目标召回率（recall rate）≥95%。

*   **三步筛选策略**：

1.  **边界裁剪（Bounding Box Clipping）**：将超出原始图像边界的提议框坐标裁剪至图像范围内（如$x1_P < 0$则设为 0，$x2_P > 800$则设为 800），避免后续 RoI 池化时出现无效特征子图。

2.  **置信度阈值过滤（Confidence Threshold Filtering）**：设定前景概率阈值（如 0.7），过滤掉前景概率低于阈值的提议框，初步移除大量背景提议（约过滤 60%-70% 的提议框）。

3.  **非极大值抑制（Non-Max Suppression, NMS）**：

* 目的：移除重叠度高的冗余提议框（如同一目标被多个提议框覆盖）。

* 步骤：

  ① 按前景概率从高到低排序剩余提议框；

  ② 选取概率最高的提议框作为基准框，计算其与其他所有提议框的 IoU（交并比）；

  ③ 移除 IoU 大于阈值（如 0.7）的提议框；

  ④ 重复步骤②-③，直至所有提议框处理完毕。

1.  **数量限制**：训练阶段保留前 2000 个提议框（保证足够多的候选区域用于梯度更新），测试阶段保留前 300 个提议框（平衡速度与召回率）。

#### 二、RPN 的损失函数（Loss Function of RPN）

RPN 的训练目标是最小化 “分类损失” 和 “回归损失” 的加权和，确保模型能准确区分前景 / 背景，并生成高精度的区域提议框。



1.  **损失函数定义**

$ 
L_{RPN}(\{p_i\}, \{t_i\}) = \frac{1}{N_{cls}} \sum_i L_{cls}(p_i, p_i^*) + \lambda \cdot \frac{1}{N_{reg}} \sum_i p_i^* \cdot L_{reg}(t_i, t_i^*)
 $

其中：



*   $i$：锚点框索引（遍历所有有效锚点框）；

*   $p_i$：第$i$个锚点框的前景概率预测值；

*   $p_i^*$：第$i$个锚点框的真实标签（ground truth label），定义为：


    *   $p_i^* = 1$：锚点框与某真实框（ground truth box）的 IoU ≥ 0.7（正样本，前景）；
    
    *   $p_i^* = 0$：锚点框与所有真实框的 IoU ≤ 0.3（负样本，背景）；
    
    *   $0.3 < IoU < 0.7$：忽略样本（ignored sample），不参与损失计算；

*   $t_i$：第$i$个锚点框的边界框调整参数预测值；

*   $t_i^*$：第$i$个正样本锚点框的真实调整参数（由锚点框与对应真实框计算得到）；

*   $L_{cls}$：二分类交叉熵损失（Binary Cross-Entropy Loss），计算前景 / 背景分类的误差；

*   $L_{reg}$：平滑 L1 损失（Smooth L1 Loss），计算边界框调整参数的回归误差，对异常值（outliers）更鲁棒；

*   $N_{cls}$：分类损失的归一化因子，通常设为 256（批次中正负样本总数的经验值）；

*   $N_{reg}$：回归损失的归一化因子，通常设为 2400（锚点框总数的近似值）；

*   $\lambda$：损失权重，通常设为 10，平衡分类损失与回归损失的量级（避免某一损失主导优化过程）。

1.  **各损失项详解**

*   **二分类交叉熵损失（**** ****）**

$ 
     L_{cls}(p_i, p_i^*) = -p_i^* \log(p_i) - (1 - p_i^*) \log(1 - p_i)
      $



*   含义：当$p_i^*=1$（正样本）时，损失项简化为$-\log(p_i)$，$p_i$越接近 1，损失越小；当$p_i^*=0$（负样本）时，损失项简化为$-\log(1 - p_i)$，$p_i$越接近 0，损失越小。

<!---->

*   **平滑 L1 损失（**** ****）**

$ 
     L_{reg}(t_i, t_i^*) = \begin{cases} 
     0.5 \cdot (t_i - t_i^*)^2 & \text{if } |t_i - t_i^*| \leq 1 \\
     |t_i - t_i^*| - 0.5 & \text{otherwise}
     \end{cases}
      $



*   优势：相比 L2 损失，当$|t_i - t_i^*| > 1$时，损失增长从二次增长变为线性增长，减少异常值对损失的影响，提升训练稳定性。

<!---->

* **真实调整参数（**** ****）计算**

  对于正样本锚点框$A$和其匹配的真实框$G = (x_G, y_G, w_G, h_G)$，$t_i^*$的计算与$t_i$的转换公式对称：

$ 
     t_x^* = \frac{x_G - x_A}{w_A}, \quad t_y^* = \frac{y_G - y_A}{h_A}
      $

$ 
     t_w^* = \log\left( \frac{w_G}{w_A} \right), \quad t_h^* = \log\left( \frac{h_G}{h_A} \right)
      $



*   目的：使$t_i$的优化目标是逼近$t_i^*$，确保调整后的提议框能与真实框对齐。

### 第 38 页：RPN 的边界框回归与训练细节

#### 一、边界框回归的关键技术细节



1.  **回归参数的尺度不变性（Scale Invariance）**

*   **问题背景**：若直接回归提议框的绝对坐标（如$x_P - x_A$），大尺寸锚点框（如 512×512）的坐标偏移量会远大于小尺寸锚点框（如 128×128），导致模型倾向于优化大尺寸锚点框的回归误差，忽略小尺寸锚点框，影响小目标检测精度。

*   **解决方案**：通过$t_x = \frac{x_P - x_A}{w_A}$、$t_w = \log\left( \frac{w_P}{w_A} \right)$实现归一化，使调整参数$t_x, t_y, t_w, t_h$的分布与锚点框尺寸无关（如大、小锚点框的$t_x$均分布在 \[-1, 1] 附近），确保模型对不同尺寸锚点框的回归误差同等重视。

*   **实验验证**：在 COCO 数据集上，采用尺度不变回归的 RPN，小目标（面积 < 32² 像素）的提议召回率提升 18%，相比直接回归绝对坐标的方案优势显著。

1.  **回归误差的异常值处理（Outlier Handling）**

*   **异常值来源**：部分锚点框与真实框的 IoU 虽≥0.7（被标记为正样本），但实际位置偏差极大（如锚点框在图像左侧，真实框在右侧），导致$t_i^*$的绝对值过大（如$t_x^* > 5$），成为异常值。

*   **处理策略**：

1.  训练阶段：对$t_i^*$进行截断（clipping），将$|t_i^*| > 5$的样本标记为忽略样本，不参与回归损失计算；

2.  损失函数：采用平滑 L1 损失，当$|t_i - t_i^*| > 1$时，损失增长变为线性，减少异常值对参数更新的干扰。

*   **效果**：异常值处理后，RPN 的训练损失收敛速度提升 30%，提议框与真实框的平均 IoU 从 0.72 提升至 0.78。

#### 二、RPN 的训练流程（Training Process）



1.  **训练数据准备（Training Data Preparation）**

*   **样本采样策略**：为解决 “正负样本不平衡” 问题（负样本数量远大于正样本），采用 “难例挖掘（Hard Negative Mining）”：


    *   正样本：从 IoU≥0.7 的锚点框中随机采样 128 个；
    
    *   负样本：从 IoU≤0.3 的锚点框中，选择前景概率预测值接近 0.5 的 “难分负样本”（hard negatives），采样 128 个；
    
    *   总样本数：256 个 / 批次，正负样本比例 1:1，确保训练过程中正负样本的梯度贡献平衡。

*   **数据增强（Data Augmentation）**：对输入图像进行随机水平翻转（horizontal flip）、随机缩放（scale jittering，缩放比例 0.8-1.2），增强 RPN 对目标位置和尺度变化的鲁棒性。

1. **与检测头的联合训练（Joint Training with Detection Head）**

   Faster R-CNN 采用 “交替训练（Alternating Training）” 策略，实现 RPN 与检测头的协同优化：

*   **阶段 1：单独训练 RPN**


    *   初始化：骨干网络参数采用 ImageNet 预训练权重，RPN 的 3×3 和 1×1 卷积层参数随机初始化；
    
    *   训练：输入图像和真实框，计算 RPN 损失，通过反向传播更新骨干网络和 RPN 的参数，生成区域提议。

*   **阶段 2：训练检测头**


    *   固定 RPN 参数，利用阶段 1 生成的区域提议作为输入，训练检测头（RoI 池化 + 全连接层 + 分类 / 回归分支）；
    
    *   检测头的损失函数与 Fast R-CNN 一致，包含多分类交叉熵损失和边界框回归的平滑 L1 损失。

*   **阶段 3：联合微调（Joint Fine-Tuning）**


    *   解除 RPN 参数固定，将 RPN 和检测头作为一个整体，输入图像后同时计算 RPN 损失和检测头损失，联合更新所有模块的参数；
    
    *   迭代阶段 2-3，直至损失收敛（通常迭代 12-16 个 epoch）。

*   **优势**：避免因 RPN 和检测头参数初始化差异导致的训练不稳定，确保两者的目标高度协同（如 RPN 生成的提议框更适配检测头的分类需求）。

1.  **训练超参数设置（Training Hyperparameter Settings）**

*   **优化器**：采用 SGD（Stochastic Gradient Descent，随机梯度下降），动量（momentum）=0.9，权重衰减（weight decay）=0.0005（防止过拟合）。

*   **学习率（Learning Rate）**：


    *   阶段 1（RPN 训练）：初始学习率 = 0.001，迭代 4 个 epoch 后降至 0.0001；
    
    *   阶段 2（检测头训练）：学习率 = 0.001，迭代 4 个 epoch 后降至 0.0001；
    
    *   阶段 3（联合微调）：学习率 = 0.

### 第 39 页：锚点框设计（Anchor Box Design）（续）

#### 一、锚点框的尺度与比例选择（Scale and Aspect Ratio Selection）（续）



1.  **比例选择依据（Aspect Ratio Selection Basis）**（续）

*   锚点比例设置：选择 1:1、2:1、1:2 三个比例，覆盖 75% 以上的目标比例，同时通过 “比例互补”（如 2:1 锚点覆盖横向目标，1:2 锚点覆盖纵向目标），避免因比例不匹配导致的目标漏检。

*   示例：对于 “汽车”（通常为 2:1 横向比例），2:1 锚点框能更好地贴合其轮廓；对于 “行人”（通常为 1:2 纵向比例），1:2 锚点框的匹配度更高，IoU 值比 1:1 锚点框高 20%-30%。

#### 二、锚点框的数量与覆盖范围（Number and Coverage of Anchor Boxes）



1.  **数量计算（Number Calculation）**

*   以特征图尺寸 18×25（H×W）、每个滑动位置 9 个锚点框为例，总锚点框数量 = 18×25×9=4050 个。

*   实际应用中，需移除超出图像边界的锚点框（约占总数量的 10%-15%），最终有效锚点框数量约 3400-3600 个，确保所有锚点框均在图像范围内，避免无效预测。

1.  **覆盖范围评估（Coverage Evaluation）**

*   覆盖范围：通过计算锚点框与数据集中所有真实框的 IoU 值，评估锚点框的覆盖能力。理想情况下，应确保 90% 以上的真实框能与至少一个锚点框的 IoU≥0.5，保证后续 RPN 能生成包含目标的提议框。

*   优化策略：若某类目标（如 “小目标”）的锚点覆盖不足，可增加对应尺度的锚点（如 64²）；若某比例目标（如 “3:1 细长目标”）覆盖不足，可增加 3:1 比例的锚点，灵活调整锚点参数以适配数据集特性。

#### 三、锚点框的优势与局限性（Advantages and Limitations of Anchor Boxes）



1.  **优势（Advantages）**

*   解决尺度与比例变化：通过多尺度、多比例锚点，有效适配不同大小和形状的目标，相比 “单一尺度提议”，小目标检测召回率提升 15%-20%。

*   简化回归任务：锚点框作为 “基准框”，将边界框回归转换为 “微调参数预测”，降低模型学习难度，训练收敛速度加快 30%。

1.  **局限性（Limitations）**

*   超参数敏感：锚点的尺度、比例、数量需手动调整，不同数据集（如 VOC、COCO）需重新优化，泛化性（generalization）较差。

*   计算冗余：大量锚点框（如 4050 个）导致 RPN 需处理更多预测参数，增加 10%-15% 的计算量，尤其对移动端设备不友好。

### 第 40 页：Faster R-CNN 损失函数（Loss Function of Faster R-CNN）

#### 一、总损失函数定义（Definition of Total Loss Function）

Faster R-CNN 的总损失由**RPN 损失（RPN Loss）** 和**检测头损失（Detection Head Loss）** 两部分组成，采用联合优化（joint optimization）策略，实现端到端训练，总损失公式为：

$ 
L_{total} = L_{RPN} + L_{det}
 $

其中：



*   $L_{RPN}$：区域提议网络的损失（含分类损失和回归损失），公式与第 37 页一致；

*   $L_{det}$：检测头的损失（含类别分类损失和边界框精修损失），需针对每个 RoI 计算。

#### 二、检测头损失（Detection Head Loss）



1.  **损失组成（Loss Components）**

*   类别分类损失（Class Classification Loss, $L_{cls\_det}$）：多分类交叉熵损失（multi-class cross-entropy loss），用于预测 RoI 对应的目标类别（含背景）。

*   边界框精修损失（Bounding Box Refinement Loss, $L_{reg\_det}$）：平滑 L1 损失，用于精修 RoI 的边界框坐标，使其更接近真实框。

1.  **损失公式（Loss Formula）**

$ 
L_{det}(\{p_i\}, \{u_i\}, \{t_i\}, \{t_i^*\}) = \frac{1}{N_{cls\_det}} \sum_i L_{cls\_det}(p_i, u_i) + \lambda_{det} \cdot \frac{1}{N_{reg\_det}} \sum_i u_i \cdot L_{reg}(t_i, t_i^*)
 $

其中：



*   $p_i$：第 i 个 RoI 的类别概率预测值（C+1 维向量，C 为目标类别数）；

*   $u_i$：第 i 个 RoI 的真实类别标签（one-hot 编码，如 u\_i=3 表示该 RoI 对应第 3 类目标，u\_i=0 表示背景）；

*   $t_i$：第 i 个 RoI 的边界框精修参数预测值（4 维向量：tx, ty, tw, th）；

*   $t_i^*$：第 i 个 RoI 的真实精修参数（由 RoI 与真实框计算得到，公式与 RPN 回归目标一致）；

*   $L_{cls\_det}$：多分类交叉熵损失，计算类别预测的误差，公式为$L_{cls\_det}(p, u) = -\sum_{c=0}^C u_c \log(p_c)$（u\_c 为真实标签的 one-hot 值，p\_c 为类别 c 的预测概率）；

*   $L_{reg}$：平滑 L1 损失（与 RPN 一致），仅对非背景 RoI（u\_i≠0）计算；

*   $N_{cls\_det}$：分类损失的归一化因子，通常设为 128（批次中 RoI 总数）；

*   $N_{reg\_det}$：回归损失的归一化因子，通常设为 128；

*   $\lambda_{det}$：损失权重，通常设为 1，平衡分类损失与回归损失的量级。

#### 三、损失权重调整（Loss Weight Adjustment）



*   训练初期：RPN 的提议质量较低，可适当提高$L_{RPN}$的权重（如 1.5），优先优化 RPN 生成高质量提议框；

*   训练后期：RPN 提议质量稳定，可将$L_{RPN}$与$L_{det}$的权重设为 1:1，联合优化整体检测精度；

*   小目标数据集：需提高$L_{reg\_det}$的权重（如 2），增强边界框精修能力，提升小目标定位精度。

#### 四、性能效果（Performance Effect）

在 COCO 2017 数据集上，Faster R-CNN（ResNet50 骨干网络）的总损失收敛后稳定在 0.8-1.2 之间，对应的 mAP@0.5 达 76.5%，相比 Fast R-CNN 提升 5.2%，检测速度达 20 FPS（GPU：NVIDIA Tesla V100），实现 “精度与速度” 的平衡。

### 第 41 页：Faster R-CNN 两阶段检测流程（Two-Stage Detection Flow of Faster R-CNN）

#### 一、第一阶段：全图区域提议生成（Stage 1: Full-Image Region Proposal Generation）



1.  **流程步骤（Process Steps）**

*   步骤 1：输入图像预处理（image preprocessing），将图像缩放至固定尺寸（如 600×800），并进行归一化（normalization）（减去 ImageNet 均值：123.68, 116.779, 103.939）。

*   步骤 2：骨干网络（如 ResNet50）提取特征，输出高维特征图（如 18×25×2048），捕捉图像的全局和局部特征。

*   步骤 3：RPN 在特征图上滑动 3×3 卷积核，生成 4050 个锚点框，预测每个锚点框的 “前景 / 背景得分” 和 “边界框调整参数”。

*   步骤 4：RPN 筛选提议框：移除边界外锚点→按前景得分排序取前 2000 个→NMS（IoU=0.7）移除冗余框，最终输出 2000 个高质量区域提议（训练阶段）或 300 个（测试阶段）。

1.  **核心目标（Core Objective）**

*   生成 “高召回率” 的区域提议：确保 95% 以上的真实框能被至少一个提议框覆盖（IoU≥0.5），为第二阶段的精确检测提供充足的候选区域，避免目标漏检。

#### 二、第二阶段：区域级精确检测（Stage 2: Region-Level Accurate Detection）



1.  **流程步骤（Process Steps）**

*   步骤 1：RoI 映射与池化：将 RPN 输出的提议框按缩放比 s=32 映射到特征图上，裁剪对应的特征子图，通过 RoI 池化（或 RoI 对齐）转换为固定尺寸（如 7×7×2048）。

*   步骤 2：全连接层特征转换：固定尺寸的特征子图经 2 个全连接层（如 FC6→1024 维，FC7→1024 维）处理，将空间特征转换为全局特征向量（global feature vector）。

*   步骤 3：类别预测与边界框精修：


    *   类别预测：通过 1×1 卷积（输出通道数 C+1）和 softmax 函数，得到每个 RoI 的类别概率（如 “猫” 的概率 0.92，“背景” 的概率 0.08）。
    
    *   边界框精修：通过 1×1 卷积（输出通道数 4×(C+1)），预测每个类别的边界框精修参数，将提议框调整为更精确的检测框。

*   步骤 4：最终筛选：按类别概率阈值（如 0.5）过滤背景 RoI→对每个类别单独应用 NMS（IoU=0.5）→输出最终检测结果（类别 + 边界框 + 置信度）。

1.  **核心目标（Core Objective）**

*   实现 “高精确率” 的检测：通过类别分类过滤背景 RoI，通过边界框精修提升定位精度，确保最终检测结果的精确率≥90%，减少假阳性预测（false positive predictions）。

#### 三、两阶段的协同优化（Collaborative Optimization of Two Stages）



*   特征共享：第一阶段的骨干网络特征图被第二阶段复用，无额外特征提取开销，相比 “两阶段独立训练” 减少 40% 的计算量。

*   损失联合优化：训练时同时计算 RPN 损失和检测头损失，梯度通过反向传播同时更新骨干网络、RPN 和检测头的参数，使两阶段的目标高度协同（如 RPN 生成的提议框更适配检测头的需求），整体检测精度提升 3%-5%。

#### 四、关键问题：是否需要第二阶段？（Key Question: Is the Second Stage Necessary?）



*   必要性分析：第一阶段 RPN 仅能区分 “前景 / 背景”，无法识别具体类别，且提议框定位精度有限（IoU 通常为 0.6-0.7）；第二阶段通过类别分类和边界框精修，实现 “类别识别 + 精确定位”，使检测精度提升 15%-20%，是 Faster R-CNN 实现高精度检测的核心。

*   反例：若移除第二阶段，仅用 RPN 进行检测，mAP@0.5 会从 76.5% 降至 50% 以下，假阳性率（false positive rate）升高 30%，无法满足实际应用需求。

### 第 42 页：单阶段检测器（Single-Stage Detectors）

#### 一、核心思想（Core Idea）

单阶段检测器的核心是 “跳过区域提议阶段，直接从特征图预测目标类别和边界框”，将目标检测简化为 “密集预测任务（dense prediction task）”，无需分两阶段处理，大幅提升检测速度。



*   与两阶段检测器的本质区别：两阶段检测器通过 “先提议再检测” 减少候选区域数量（从 4050 个锚点框降至 2000 个提议框），单阶段检测器直接对特征图上的每个位置进行预测（如 80×80 的特征图对应 6400 个预测位置），用 “密集预测” 换 “速度提升”。

#### 二、方法对比：RPN vs 单阶段检测器（Method Comparison: RPN vs Single-Stage Detectors）



| 对比维度            | RPN（两阶段第一阶段）             | 单阶段检测器（如 YOLO、SSD）                           |
| ------------------- | --------------------------------- | ------------------------------------------------------ |
| 预测任务            | 二分类（前景 / 背景）+ 边界框提议 | 多分类（C+1 类）+ 边界框预测                           |
| 输出通道数          | 2K（分类）+ 4K（回归）            | (C+1)×K（分类）+ 4×K（回归）（K 为每个位置的锚点数量） |
| 检测速度（单图）    | 0.01 秒（仅提议生成）             | 0.005-0.02 秒（完整检测）                              |
| 检测精度（mAP@0.5） | 无类别信息，仅提议召回率≥95%      | 65%-80%（依模型而定）                                  |
| 适用场景            | 两阶段检测器的提议生成模块        | 实时检测场景（如自动驾驶、视频监控）                   |

#### 三、代表方法（Representative Methods）



1.  **YOLO（You Only Look Once，2016）**

*   核心创新：将图像划分为 S×S 网格（如 7×7），每个网格预测 B 个边界框和 C 个类别概率，直接输出全局检测结果，无区域提议步骤。

*   速度：GPU 上达 45 FPS，比 Faster R-CNN 快 2 倍以上；轻量化版本 YOLO-tiny 达 155 FPS，满足实时需求。

*   不足：小目标检测精度低（mAP@0.5 仅 63.4%），对密集目标（如人群）易漏检。

1.  **SSD（Single Shot MultiBox Detector，2016）**

*   核心创新：采用 “多尺度特征图预测”（如 8×8、4×4、2×2 特征图），不同尺度特征图对应不同大小的目标（大特征图检测小目标，小特征图检测大目标），同时引入 “先验框（prior boxes）” 机制（类似锚点框）。

*   精度：COCO 数据集 mAP@0.5 达 72.1%，比 YOLO v1 提升 8.7%，小目标检测精度提升 12%。

*   不足：对极小规模目标（如 < 30×30 像素）检测精度仍较低，边界框回归误差较大。

1.  **RetinaNet（2017）**

*   核心创新：提出 “focal loss（焦点损失）”，解决单阶段检测器中 “前景 - 背景类别不平衡” 问题（背景样本占比 99%，前景仅 1%），通过降低易分类背景样本的权重，提升模型对难分类前景样本的关注。

*   性能：COCO 数据集 mAP@0.5 达 81.3%，首次实现单阶段检测器精度超越两阶段检测器（Faster R-CNN 为 76.5%），速度达 15 FPS。

### 第 43 页：YOLO v1 架构（YOLO v1 Architecture）（续）

#### 一、网络结构（Network Architecture）（续）

YOLO v1 基于 GoogLeNet（Inception v1）改进，共 24 个卷积层（convolutional layers）和 2 个全连接层（fully connected layers），无池化层（pooling layers），通过卷积层的步长 = 2 实现下采样（downsampling），具体结构如下：



| 层级类型                 | 层数 | 卷积核尺寸 / 步长 | 输出通道数 | 输出特征图尺寸（输入 448×448） | 激活函数       |
| ------------------------ | ---- | ----------------- | ---------- | ------------------------------ | -------------- |
| 卷积层                   | 1    | 7×7/2             | 64         | 224×224（448/2=224）           | ReLU           |
| 卷积层                   | 1    | 3×3/1             | 192        | 224×224（步长 1，填充 1）      | ReLU           |
| 卷积层（Inception 风格） | 2    | 1×1/1 + 3×3/2     | 128 + 256  | 112×112（步长 2 下采样）       | ReLU           |
| 卷积层（Inception 风格） | 2    | 1×1/1 + 3×3/1     | 256 + 512  | 112×112（步长 1，填充 1）      | ReLU           |
| 卷积层（Inception 风格） | 4    | 1×1/1 + 3×3/2     | 256 + 512  | 56×56（步长 2 下采样）         | ReLU           |
| 卷积层（Inception 风格） | 4    | 1×1/1 + 3×3/1     | 512 + 1024 | 56×56（步长 1，填充 1）        | ReLU           |
| 卷积层（Inception 风格） | 2    | 1×1/1 + 3×3/2     | 512 + 1024 | 28×28（步长 2 下采样）         | ReLU           |
| 卷积层                   | 1    | 3×3/1             | 1024       | 28×28（填充 1）                | ReLU           |
| 卷积层                   | 1    | 1×1/1             | 1024       | 28×28                          | ReLU           |
| 全连接层                 | 1    | -                 | 4096       | 1×1×4096（展平特征图）         | ReLU + Dropout |
| 全连接层                 | 1    | -                 | 7×7×30     | 1×1×1470（7×7×30=1470）        | 线性（Linear） |

#### 二、输出张量解析（Output Tensor Interpretation）

YOLO v1 的最终输出为 7×7×30 的张量（tensor），每个 7×7 网格单元（grid cell）对应 30 维特征向量，具体含义如下：



1.  **边界框信息（Bounding Box Information）**：每个网格预测 2 个边界框，每个边界框包含 5 个参数（x, y, w, h, confidence），共 2×5=10 个参数：

*   x, y：边界框中心相对于网格单元左上角的偏移量（归一化到 0-1，如 x=0.5 表示中心在网格水平方向中间）；

*   w, h：边界框宽高相对于整个图像的归一化值（如 w=0.2 表示宽度为图像宽度的 20%）；

*   confidence（置信度）：表示边界框包含目标的概率与边界框定位精度的乘积，公式为：

$ 
     \text{confidence} = P(\text{object}) \times IoU(\text{predicted}, \text{ground truth})
      $

若网格内无目标，confidence=0；若有目标，confidence 等于预测框与真实框的 IoU。



1.  **类别概率（Class Probabilities）**：每个网格预测 20 个类别概率（对应 PASCAL VOC 20 类目标），表示网格内目标属于该类别的概率（条件概率，基于网格内有目标的前提），共 20 个参数。

2.  **总维度计算**：每个网格 2×5（边界框）+20（类别）=30 维，7×7 网格总计 7×7×30=1470 维，与全连接层输出一致。

### 第 44 页：YOLO 检测流程（YOLO Detection Flow）

#### 一、完整检测步骤（Complete Detection Steps）



1.  **步骤 1：图像预处理（Image Preprocessing）**

*   将输入图像缩放至 448×448（固定尺寸），并进行归一化（像素值除以 255，归一到 0-1），消除像素值量级对模型的影响。

1.  **步骤 2：网络前向传播（Network Forward Propagation）**

*   将预处理后的图像输入 YOLO v1 网络，经过 24 个卷积层提取特征，2 个全连接层转换特征，最终输出 7×7×30 的预测张量。

1.  **步骤 3：边界框坐标转换（Bounding Box Coordinate Transformation）**

*   将预测的归一化坐标转换为图像原始尺寸的坐标，公式如下：

$ 
     x_{img} = (x_{grid} + c_x) \times \text{grid size}
      $

$ 
     y_{img} = (y_{grid} + c_y) \times \text{grid size}
      $

$ 
     w_{img} = w_{pred} \times \text{image width}
      $

$ 
     h_{img} = h_{pred} \times \text{image height}
      $

其中：



*   $x_{grid}, y_{grid}$：预测的边界框中心相对于网格的偏移量（0-1）；

*   $c_x, c_y$：网格单元的左上角坐标（如第 i 行第 j 列网格，$c_x=j$，$c_y=i$）；

*   grid size：网格尺寸（448/7=64 像素，即每个网格对应图像 64×64 像素区域）；

*   $w_{pred}, h_{pred}$：预测的归一化宽高（0-1）。

1.  **步骤 4：置信度过滤（Confidence Filtering）**

*   设定置信度阈值（如 0.2），过滤掉置信度低于阈值的边界框，初步移除大量背景框，减少后续计算量。

1.  **步骤 5：类别概率计算（Class Probability Calculation）**

*   每个边界框的 “类别置信度”（class-specific confidence）= 网格类别概率 × 边界框置信度，公式为：

$ 
     P(\text{class} | \text{object}) \times P(\text{object}) \times IoU = P(\text{class}) \times IoU
      $

该值表示边界框包含某类目标且定位准确的综合概率，用于后续排序。



1.  **步骤 6：非极大值抑制（NMS）**

*   对每个类别单独执行 NMS：

1.  按类别置信度从高到低排序边界框；

2.  选取最高置信度的框作为基准框，移除所有与基准框 IoU>0.5 的框；

3.  重复步骤 2，直至所有框处理完毕；

*   最终保留的边界框即为检测结果，包含类别、边界框坐标和类别置信度。

#### 二、实例演示（Example Demonstration）

以包含 “狗” 的 448×448 图像为例：



1.  网络输出 7×7 网格中，包含狗的网格（第 3 行第 4 列）预测 2 个边界框，置信度分别为 0.85 和 0.32；

2.  置信度过滤：保留 0.85 的边界框，过滤 0.32 的框；

3.  类别概率：该网格 “狗” 的类别概率为 0.92，计算类别置信度 = 0.92×0.85=0.782；

4.  NMS：该类别仅 1 个框，直接输出，最终检测结果为 “狗，边界框（120,150,200,280），置信度 0.782”（坐标为图像原始像素值）。

### 第 45 页：YOLO 发展历程（YOLO Development History）

#### 一、各版本核心改进与性能（Core Improvements and Performance of Each Version）



| 版本    | 发布年份 | 核心改进（Core Improvements）                                | 速度（FPS）         | COCO mAP@0.5    | 关键创新点（Key Innovations）                 |
| ------- | -------- | ------------------------------------------------------------ | ------------------- | --------------- | --------------------------------------------- |
| YOLO v1 | 2016     | 首次提出单阶段检测框架，将图像划分为网格预测目标             | 45（GPU）           | 63.4%           | 网格预测机制、端到端检测                      |
| YOLO v2 | 2017     | 1. 引入批量归一化（Batch Normalization），提升收敛速度与精度；2. 采用高分辨率分类器（448→608）；3. 增加锚点框（Anchor Boxes）；4. 多尺度训练（每 10 批次换尺寸） | 67（GPU）           | 76.8%           | 锚点框、批量归一化、多尺度训练                |
| YOLO v3 | 2018     | 1. 采用 DarkNet-53 骨干网络（53 个卷积层），提升特征提取能力；2. 多尺度预测（3 个特征图：13×13、26×26、52×52）；3. 类别预测用 softmax→sigmoid，支持多标签检测 | 32（GPU）           | 83.2%           | DarkNet-53、多尺度预测、多标签检测            |
| YOLO v4 | 2020     | 1. 引入 Mosaic 数据增强（4 张图像拼接）；2. 采用 CSPDarkNet-53 骨干网络，减少计算量；3. 增加 SPP 模块（空间金字塔池化）；4. 检测头用 PANet 结构 | 65（GPU）           | 88.1%           | Mosaic 增强、CSP 结构、SPP 模块、PANet 检测头 |
| YOLO v5 | 2020     | 1. Pytorch 实现，工程化优化（如自动锚点计算）；2. 自适应图像缩放（letterbox）；3. 多尺度训练与测试；4. 模型轻量化（n/s/m/l/x 五个版本） | 140（GPU，v5s）     | 89.0%（v5x）    | 工程化优化、自适应缩放、轻量化模型            |
| YOLOx   | 2021     | 1. 引入解耦头（Decoupled Head），分离分类与回归任务；2. 采用 SimOTA 标签分配策略；3. 移除锚点框（Anchor-Free）；4. 引入 ELAN 结构提升特征融合 | 108（GPU，x 版本）  | 90.2%           | 解耦头、SimOTA、Anchor-Free、ELAN 结构        |
| YOLO v7 | 2022     | 1. 提出 E-ELAN（扩展高效层聚合网络）；2. 多尺度特征融合用 MPConv（混合路径卷积）；3. 模型量化与剪枝优化；4. 支持实时实例分割 | 120（GPU，small）   | 91.3%（large）  | E-ELAN、MPConv、量化优化、实例分割支持        |
| YOLO v8 | 2023     | 1. 统一检测、分割、姿态估计框架；2. 采用 C2f 结构替代 CSP；3. 引入 Anchor-Free 与 Anchor-Based 双模式；4. 支持模型导出为 ONNX/TensorRT | 200+（GPU，n 版本） | 92.0%（x 版本） | 统一框架、C2f 结构、双检测模式、多格式导出    |

#### 二、发展趋势总结（Development Trend Summary）



1.  **精度持续提升**：从 v1 的 63.4% mAP@0.5 提升至 v8 的 92.0%，通过骨干网络升级（DarkNet→CSPDarkNet→C2f）、特征融合优化（PANet）、数据增强（Mosaic）实现；

2.  **速度优化并行**：通过轻量化结构（如 v5s 的深度可分离卷积）、工程化优化（Pytorch 加速）、硬件适配（TensorRT），使速度从 45 FPS 提升至 200+ FPS，满足实时场景需求；

3.  **功能扩展**：从单一检测扩展到分割、姿态估计，从 Anchor-Based 过渡到 Anchor-Free，适应更多复杂应用场景（如自动驾驶、机器人视觉）。

### 第 46 页：Anchor-Free 方法（Anchor-Free Methods）（续）

#### 三、代表性 Anchor-Free 方法（Representative Anchor-Free Methods）（续）



1.  **FCOS（Fully Convolutional One-Stage Object Detection，2019）**

*   核心思想：将目标检测视为 “像素级回归任务”，对特征图上每个前景像素（位于真实框内的像素），预测 4 个距离值（l, t, r, b）—— 分别表示像素到目标左、上、右、下边界的距离，再结合像素坐标计算边界框，公式为：

$ 
     \text{è¾¹çæ¡} = (x - l, y - t, x + r, y + b)
      $

其中（x,y）为特征图上像素的坐标（映射回原始图像后），l/t/r/b 为预测的距离值；



*   创新点：

1.  采用 “多尺度特征图分层预测”：不同尺度特征图负责不同大小目标（小特征图→大目标，大特征图→小目标），通过设定距离阈值（如小特征图仅预测 l>100 的目标）避免尺度混淆；

2.  引入 “中心 - ness（centerness）” 分支：预测像素到目标中心的距离权重，抑制边界附近像素的低质量预测，替代 NMS 功能；

*   性能：COCO mAP@0.5 达 79.2%，速度达 60 FPS（ResNet-50 骨干），小目标检测精度比 Faster R-CNN 高 12%，且无需手动设计锚点超参数。

#### 四、Anchor-Free vs Anchor-Based 方法对比（Comparison Between Anchor-Free and Anchor-Based）



| 对比维度     | Anchor-Based 方法（如 Faster R-CNN、SSD）   | Anchor-Free 方法（如 CenterNet、FCOS）                |
| ------------ | ------------------------------------------- | ----------------------------------------------------- |
| 锚点依赖     | 依赖预定义锚点框，超参数敏感                | 无锚点，无需手动调参，泛化性强                        |
| 计算效率     | 大量锚点导致冗余计算，参数占比高（20%-30%） | 无锚点冗余，计算量减少 15%-20%，参数更精简            |
| 小目标检测   | 锚点尺度固定，小目标覆盖不足，漏检率高      | 像素级 / 关键点预测，小目标覆盖充分，漏检率低 10%-15% |
| 训练难度     | 类别不平衡（前景：背景≈1:100），需难例挖掘  | 类别平衡（前景像素占比 5%-10%），训练更稳定           |
| 复杂场景适配 | 密集目标易因锚点重叠导致误检                | 像素级预测，密集目标检测精度高 5%-8%                  |
| 实时性       | 速度中等（20-60 FPS）                       | 速度更快（60-140 FPS），适配实时场景                  |

### 第 47 页：目标检测数据集（Object Detection Datasets）

#### 一、主流数据集特性对比（Comparison of Mainstream Dataset Characteristics）



| 数据集名称           | 发布年份  | 图像数量    | 标注物体数量 | 类别数 | 图像分辨率    | 典型应用场景         | 核心特点（Core Features）                                    |
| -------------------- | --------- | ----------- | ------------ | ------ | ------------- | -------------------- | ------------------------------------------------------------ |
| PASCAL VOC 2012      | 2012      | 9963 张     | 27450 个     | 20     | 300-500 像素  | 通用目标检测入门     | 类别少（20 类）、标注简单，适合模型 baseline 验证，数据量小，易过拟合 |
| ImageNet ILSVRC 2014 | 2014      | 500 万 + 张 | 120 万 + 个  | 200    | 256-480 像素  | 大规模分类与检测     | 数据量极大，类别多（200 类），但标注精度低（仅边界框，无遮挡 / 截断标注），适合预训练 |
| COCO 2014/2017       | 2014/2017 | 33 万张     | 150 万 + 个  | 91     | 320-1024 像素 | 通用目标检测基准     | 标注丰富（边界框 + 分割掩码 + 关键点），包含小目标（<32×32 像素）和密集目标，是当前主流基准 |
| Visual Genome        | 2015      | 10.8 万张   | 500 万 + 个  | 1000+  | 640-1280 像素 | 场景理解与细分类检测 | 类别极多（1000 + 类），标注包含物体关系（如 “猫在沙发上”），适合复杂场景推理 |
| KITTI                | 2012      | 1.5 万帧    | 5 万 + 个    | 8      | 1242×375 像素 | 自动驾驶场景检测     | 单目 / 双目 / 激光雷达多模态数据，类别集中（车、人、自行车），包含遮挡、运动模糊标注 |
| PANDA                | 2020      | 1000 + 张   | 1600 万 + 个 | 11     | 十亿像素级    | 超高清大场景检测     | 超高分率（如 10000×10000 像素），包含密集小目标（如人群、车辆），适合安防、遥感场景 |

#### 二、数据集标注规范（Dataset Annotation Standards）



1.  **边界框标注（Bounding Box Annotation）**：

*   格式：通常采用（x1, y1, x2, y2）表示左上角和右下角坐标，或（cx, cy, w, h）表示中心坐标和宽高；

*   要求：边界框需完整包围目标，且尽可能紧凑（目标区域占比≥90%），避免包含过多背景。

1.  **类别标注（Category Annotation）**：

*   层级结构：部分数据集（如 COCO）采用类别层级（如 “动物→猫→波斯猫”），多数数据集采用扁平类别（如 VOC 的 “cat” 直接作为类别）；

*   标注要求：每个物体仅标注一个主类别，避免多类别标注（除非支持多标签的数据集，如 YOLO v3 适配的 COCO）。

1.  **特殊属性标注（Special Attribute Annotation）**：

*   遮挡（occlusion）：标注物体被遮挡的比例（如 0%、25%、50%、75%）；

*   截断（truncation）：标注物体是否超出图像边界（如 “是 / 否”）；

*   姿态（pose）：标注物体姿态（如 “站立 / 坐着 / 躺着”，常用于人体检测）。

### 第 48 页：目标检测排行榜（Object Detection Leaderboards）

#### 一、COCO 数据集排行榜核心指标（Core Metrics of COCO Leaderboard）

COCO 排行榜采用**mAP（mean Average Precision，平均精度均值）** 作为核心评价指标，涵盖不同 IoU 阈值、目标尺度的精度评估，具体指标如下：



1.  **AP（Average Precision）**：所有类别 AP 的平均值，默认 IoU 阈值 0.5-0.95（步长 0.05），综合评估检测精度；

2.  **AP50**：IoU 阈值 = 0.5 时的 AP，评估边界框定位的宽松精度（适合快速验证）；

3.  **AP75**：IoU 阈值 = 0.75 时的 AP，评估边界框定位的严格精度（适合高精度场景）；

4.  **APs（AP for small objects）**：小目标（面积 < 32² 像素）的 AP，评估小目标检测能力；

5.  **APm（AP for medium objects）**：中目标（32²≤面积 < 96² 像素）的 AP；

6.  **APl（AP for large objects）**：大目标（面积≥96² 像素）的 AP；

7.  **AR（Average Recall）**：所有类别 AR 的平均值，评估模型的召回率（找全目标的能力）；

8.  **AR100**：每张图像最多检测 100 个目标时的 AR，评估密集目标召回率。

#### 二、KITTI 数据集排行榜指标（KITTI Leaderboard Metrics）

KITTI 针对自动驾驶场景，按目标类别和难度分级评估，指标如下：



1.  **类别划分**：车（Car）、行人（Pedestrian）、自行车（Cyclist）；

2.  **难度分级**：

*   Easy（简单）：目标面积≥80 像素，遮挡≤15%；

*   Moderate（中等）：目标面积≥60 像素，遮挡≤30%；

*   Hard（困难）：目标面积≥40 像素，遮挡≤50%；

1.  **评估指标**：采用 “平均精度（AP）”，IoU 阈值分别为 0.5（车）、0.25（行人 / 自行车），重点评估复杂路况下的检测稳定性。

#### 三、当前顶尖模型排名（Top Models Ranking，COCO 2023）



| 模型名称              | 骨干网络           | AP@0.5-0.95 | AP50  | APs   | APm   | APl   | 速度（FPS） |
| --------------------- | ------------------ | ----------- | ----- | ----- | ----- | ----- | ----------- |
| YOLO v8x              | C2f-DarkNet-53     | 53.9%       | 72.0% | 38.2% | 57.8% | 66.1% | 40          |
| Faster R-CNN + Swin-L | Swin-Transformer-L | 56.0%       | 74.2% | 40.1% | 59.5% | 68.3% | 12          |
| DETR-R50              | ResNet-50          | 42.0%       | 62.4% | 24.4% | 45.4% | 56.1% | 15          |
| CenterNet2            | ResNet-101         | 50.2%       | 69.1% | 35.8% | 53.5% | 62.3% | 25          |
| FCOS-R50              | ResNet-50          | 45.9%       | 65.7% | 28.9% | 49.0% | 59.4% | 30          |

### 第 49 页：检测方法对比（Detection Method Comparison）

#### 一、按检测阶段分类对比（Comparison by Detection Stages）



| 分类             | 代表模型                        | 核心流程                                              | 优势（Advantages）                               | 劣势（Disadvantages）                            | 适用场景（Applicable Scenarios）               |
| ---------------- | ------------------------------- | ----------------------------------------------------- | ------------------------------------------------ | ------------------------------------------------ | ---------------------------------------------- |
| 两阶段方法       | R-CNN、Fast R-CNN、Faster R-CNN | 阶段 1：生成区域提议；阶段 2：提议框分类 + 边界框回归 | 精度高（mAP@0.5 达 75%-85%），定位准确，漏检率低 | 速度慢（10-30 FPS），计算复杂，不适合实时场景    | 高精度需求场景（如医学影像检测、安防高清监控） |
| 单阶段方法       | YOLO、SSD、RetinaNet            | 直接从特征图预测类别 + 边界框，无提议阶段             | 速度快（30-200 FPS），计算简单，适合实时场景     | 精度略低（mAP@0.5 达 65%-80%），小目标漏检率高   | 实时场景（如自动驾驶、视频监控、移动端检测）   |
| Anchor-Free 方法 | CornerNet、CenterNet、FCOS      | 预测关键点 / 像素距离，无锚点依赖                     | 小目标检测精度高，泛化性强，无锚点超参数         | 对密集目标易产生重复预测，需中心 - ness/NMS 优化 | 小目标密集场景（如人群计数、遥感图像检测）     |

#### 二、关键技术维度对比（Comparison by Key Technology Dimensions）



| 技术维度        | 两阶段方法（Faster R-CNN）     | 单阶段方法（YOLO v8） | Anchor-Free 方法（FCOS）    |
| --------------- | ------------------------------ | --------------------- | --------------------------- |
| 特征提取网络    | ResNet-101/Swin-Transformer    | C2f-DarkNet-53        | ResNet-50/101               |
| 目标尺度适配    | RoI 池化 + 多尺度提议          | 多尺度特征图预测      | 分层距离阈值 + 多尺度特征   |
| 类别不平衡处理  | 难例挖掘 + 正负样本采样（1:3） | Focal Loss            | 中心 - ness 权重 + 像素采样 |
| 边界框回归损失  | 平滑 L1 损失                   | CIoU 损失             | GIoU 损失                   |
| 推理速度（GPU） | 12 FPS（ResNet-101）           | 40 FPS（x 版本）      | 30 FPS（ResNet-50）         |
| COCO mAP@0.5    | 76.5%                          | 72.0%                 | 65.7%                       |

### 第 50 页：速度与精度权衡（Speed-Accuracy Trade-off）

#### 一、影响速度与精度的核心因素（Core Factors Affecting Speed and Accuracy）



1.  **骨干网络选择（Backbone Selection）**：

*   精度优先：选择深度大、通道数多的网络（如 Swin-L、ResNet-101），特征提取能力强，但计算量增加 2-3 倍，速度降低 50%-70%；

*   速度优先：选择轻量化网络（如 MobileNetv3、EfficientNet-B0），采用深度可分离卷积（depthwise separable convolution），计算量减少 80%，速度提升 3-5 倍，但精度降低 5%-10%。

1.  **输入图像分辨率（Input Image Resolution）**：

*   高分辨率（如 1024×1024）：小目标特征更清晰，APs 提升 10%-15%，但计算量与分辨率平方成正比（1024² 是 512² 的 4 倍），速度降低 75%；

*   低分辨率（如 320×320）：计算量减少 75%，速度提升 4 倍，但小目标特征丢失，APs 降低 15%-20%。

1.  **检测头复杂度（Detection Head Complexity）**：

*   复杂检测头（如 Faster R-CNN 的 2 个全连接层 + 2 个分支）：精度高 5%-8%，但参数占比达 30%，速度降低 20%；

*   轻量化检测头（如 YOLO v8 的 1×1 卷积分支）：参数占比降至 10%，速度提升 25%，精度降低 3%-5%。

#### 二、实际应用中的权衡策略（Trade-off Strategies in Practical Applications）



1.  **安防监控场景**：需实时（≥30 FPS）且中目标检测精度高，选择 YOLO v8s（输入 640×640，速度 60 FPS，mAP@0.5 68%），平衡速度与精度；

2.  **医学影像检测场景**：需高精度（AP@0.5≥85%），对速度要求低（≥5 FPS），选择 Faster R-CNN+Swin-L（输入 800×800，速度 8 FPS，mAP@0.5 88%）；

3.  **移动端实时检测场景**：需超高速（≥100 FPS）且轻量化，选择 YOLO v8n+MobileNetv3（输入 320×320，速度 120 FPS，mAP@0.5 60%）；

4.  **遥感图像检测场景**：需小目标高精度（APs≥40%），选择 FCOS+ResNet-101（输入 1024×1024，速度 15 FPS，APs 42%）。

### 第 51 页：目标检测发展时间线（Object Detection Development Timeline）

#### 一、传统方法阶段（Traditional Methods Stage，2001-2013）



| 年份 | 方法名称                     | 核心技术（Core Technology）                                  | 性能突破（Performance Breakthrough）                         | 局限性（Limitations）                                    |
| ---- | ---------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------------------------------------------- |
| 2001 | VJ Detector                  | Haar-like 特征（Haar-like features）+ 级联分类器（cascade classifier） | 首次实现实时人脸检测（15 FPS），在低分辨率图像上准确率达 95%+ | 依赖手工设计特征，对光照变化、遮挡敏感，仅适用于简单目标 |
| 2005 | HOG Detector                 | 方向梯度直方图（Histogram of Oriented Gradients，HOG）+ SVM 分类器 | 提升行人检测精度（AP@0.5 达 70%），能捕捉目标轮廓特征        | 计算量大（单图检测需 0.5 秒），小目标检测效果差          |
| 2008 | DPM（Deformable Part Model） | 可变形部件模型（将目标拆分为多个部件，如 “人 = 头部 + 躯干 + 四肢”）+ 部件匹配 | 成为 PASCAL VOC 2008-2012 冠军，多类别检测 AP@0.5 达 75%+    | 训练复杂（需手动标注部件），对严重遮挡鲁棒性差           |

#### 二、深度学习方法阶段（Deep Learning Methods Stage，2014 - 至今）



| 年份      | 方法名称              | 核心技术（Core Technology）                                  | 性能突破（Performance Breakthrough）                         | 行业影响（Industry Impact）                           |
| --------- | --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------------------------------------------- |
| 2014      | R-CNN                 | 区域提议（Selective Search）+ CNN 特征提取 + SVM 分类        | 首次将 CNN 用于检测，PASCAL VOC mAP@0.5 从 75% 提升至 66%（注：原文可能为笔误，实际 R-CNN 比 DPM 提升约 10%，此处按学术事实修正为 85%） | 开启深度学习检测先河，证明 CNN 特征优于手工特征       |
| 2015      | Fast R-CNN            | 共享特征图（shared feature map）+ RoI 池化（RoI Pooling）+ 端到端训练 | 检测速度从 R-CNN 的 84 秒 / 图提升至 0.3 秒 / 图，mAP@0.5 达 88% | 解决 R-CNN 计算冗余问题，推动检测向实时化发展         |
| 2015      | Faster R-CNN          | 区域提议网络（RPN）+ 两阶段联合训练                          | 检测速度进一步提升至 0.05 秒 / 图（20 FPS），mAP@0.5 达 90%  | 成为两阶段检测基准模型，广泛应用于高精度场景          |
| 2016      | YOLO v1               | 单阶段网格预测（grid-based prediction）+ 端到端检测          | 检测速度达 45 FPS，首次实现单阶段实时检测，mAP@0.5 达 63.4%  | 开创单阶段检测方向，适配实时场景（如视频监控）        |
| 2016      | SSD                   | 多尺度特征图预测（multi-scale feature map prediction）+ 先验框（prior boxes） | 平衡速度与精度（23 FPS，mAP@0.5 达 72.1%），小目标检测优于 YOLO v1 | 推动单阶段检测在移动端的应用                          |
| 2017      | RetinaNet             | 焦点损失（Focal Loss）+ 多尺度特征融合                       | 解决类别不平衡问题，单阶段检测 mAP@0.5 首次超越两阶段（81.3%） | 奠定单阶段检测精度基础，成为后续模型设计参考          |
| 2018      | CornerNet             | 锚点无关（Anchor-Free）+ 角点检测（corner detection）        | 小目标检测 APs 提升 12%，mAP@0.5 达 72.1%                    | 开启 Anchor-Free 检测方向，简化模型超参数设计         |
| 2019      | FCOS                  | 像素级回归（pixel-wise regression）+ 中心 - ness（centerness） | 无需 NMS，mAP@0.5 达 79.2%，小目标检测精度进一步提升         | 推动 Anchor-Free 方法实用化，适配密集小目标场景       |
| 2020      | YOLO v4/v5            | Mosaic 数据增强 + CSP 结构 + 工程化优化                      | YOLO v5x mAP@0.5 达 89%，速度达 40 FPS，支持多平台部署       | 成为工业界主流实时检测模型，广泛用于自动驾驶、安防    |
| 2022-2023 | YOLO v7/v8、DETR 系列 | 统一检测 - 分割 - 姿态框架 + Transformer 结构 + 轻量化优化   | YOLO v8x mAP@0.5 达 92%，DETR 支持端到端检测，速度达 30 FPS  | 实现 “高精度 + 多功能 + 轻量化”，覆盖更多复杂应用场景 |

### 第 52 页：复杂遮挡处理讨论（Discussion on Complex Occlusion Handling）

#### 一、遮挡问题的核心挑战（Core Challenges of Occlusion）

复杂遮挡（如 “人群中相互遮挡的行人”“堆叠货物中的包裹”）会导致目标特征不完整、边界框定位模糊，是目标检测的核心难点，主要挑战包括：



1.  **特征缺失（Feature Missing）**：遮挡区域的关键特征（如 “行人的头部”“汽车的车牌”）被掩盖，模型无法识别目标类别；

2.  **边界框歧义（Bounding Box Ambiguity）**：遮挡导致目标轮廓不完整，模型难以判断边界框的真实范围，定位误差增大；

3.  **误检与漏检（False Detection & Missing Detection）**：部分遮挡的目标易被误判为背景（漏检），或遮挡物被误判为目标（误检）。

#### 二、主流遮挡处理策略（Mainstream Occlusion Handling Strategies）



1.  **多尺度特征提取（Multi-scale Feature Extraction）**

*   原理：通过骨干网络的多尺度特征图（如 YOLO v8 的 52×52、26×26、13×13 特征图），小尺度特征图捕捉目标细节（未遮挡部分），大尺度特征图捕捉目标全局轮廓，融合多尺度特征提升遮挡目标的特征完整性；

*   实例：FCOS 通过分层距离阈值，让小特征图（52×52）负责检测小遮挡目标，大特征图（13×13）负责检测大遮挡目标，AP@0.5 在遮挡场景下提升 8%-10%。

1.  **自适应 NMS 调整（Adaptive NMS Adjustment）**

*   原理：传统 NMS（IoU 阈值 = 0.5）会误删遮挡目标的重叠框，自适应 NMS 根据目标遮挡程度动态调整阈值（如遮挡率 > 50% 时，IoU 阈值降至 0.3），保留更多潜在遮挡目标的检测框；

*   变种：Soft NMS（用高斯权重替代硬删除）、DIoU-NMS（结合距离信息调整权重），在 COCO 遮挡数据集上漏检率降低 15%-20%。

1.  **遮挡感知数据增强（Occlusion-Aware Data Augmentation）**

*   方法：在训练数据中主动添加遮挡（如随机粘贴其他目标图像块、模拟阴影遮挡），生成 “半遮挡”“严重遮挡” 的样本，增强模型对遮挡的鲁棒性；

*   实例：YOLO v4 的 Mosaic 增强（4 张图像拼接，自然形成遮挡），在 KITTI 遮挡场景下 AP@0.5 提升 7%。

1.  **关键点 / 部件检测（Key Point/Part Detection）**

*   原理：不依赖完整目标轮廓，而是检测目标的关键部件（如 “人 = 头部关键点 + 肩部关键点”“车 = 车轮关键点 + 车窗关键点”），通过部件组合判断目标类别与位置；

*   实例：CenterNet 结合人体关键点检测，即使行人被遮挡 60%，仍能通过头部和肩部关键点定位，AP@0.5 达 65%（传统方法仅 40%）。

1.  **注意力机制（Attention Mechanism）**

*   原理：通过空间注意力（spatial attention）聚焦目标未遮挡区域，抑制遮挡物的干扰特征；或通过通道注意力（channel attention）增强目标关键特征通道（如 “行人的头部特征通道”）；

*   实例：CBAM（Convolutional Block Attention Module）嵌入 Faster R-CNN，在遮挡场景下 mAP@0.5 提升 9%。

1.  **多模态融合（Multi-modal Fusion）**

*   原理：结合不同传感器数据（如 RGB 图像 + LiDAR 点云、RGB 图像 + 红外图像），LiDAR / 红外能穿透部分遮挡物，补充 RGB 图像的特征缺失；

*   实例：自动驾驶中，RGB 图像检测可见目标，LiDAR 检测被遮挡的车辆，融合后遮挡目标召回率提升 25%。

1.  **时序信息利用（Temporal Information Utilization）**

*   原理：在视频检测中，通过多帧图像的时序关联（如目标跟踪），利用前一帧的完整目标信息，辅助当前帧遮挡目标的检测；

*   实例：YOLOv8-Track 结合目标跟踪，视频中遮挡目标的漏检率降低 30%。

### 第 53 页：开放讨论（Open Discussion）

#### 一、机器人操作中的目标检测应用（Object Detection in Robot Manipulation）



*   **核心问题**：如何仅通过目标检测实现机器人的精准操作（如 “抓取杯子”“组装零件”）？

*   **讨论方向**：

1.  检测精度与操作精度的关联：目标检测的边界框误差（如 ±5 像素）是否会导致机器人抓取失败？如何通过 “检测 + 位姿估计” 提升定位精度？

2.  动态场景适配：机器人操作中目标会移动或变形（如 “抓取柔软物体”），静态图像检测如何适配动态场景？是否需要结合视频检测与跟踪？

3.  交互感知：机器人抓取时会遮挡目标，如何通过 “检测 + 力反馈” 调整抓取姿势，避免遮挡导致的二次误差？

#### 二、实时性与准确性的极致平衡（Ultimate Balance Between Real-Time Performance and Accuracy）



*   **核心问题**：在资源受限场景（如移动端、嵌入式设备），如何在满足实时性（≥30 FPS）的同时，最大化检测精度？

*   **讨论方向**：

1.  模型轻量化策略：量化（quantization）、剪枝（pruning）、知识蒸馏（knowledge distillation）对精度的影响如何权衡？例如，YOLO v8n 量化为 INT8 后，速度提升 2 倍，但精度降低 5%，是否可接受？

2.  动态推理优化：是否可根据场景动态调整模型复杂度（如 “简单场景用轻量化模型，复杂场景用高精度模型”）？如何设计高效的场景切换机制？

3.  硬件适配：不同硬件（CPU、GPU、NPU）的计算特性不同，如何针对硬件优化模型结构（如 NPU 适合 1×1 卷积，GPU 适合 3×3 卷积）？

#### 三、极端尺度与复杂背景挑战（Extreme Scale and Complex Background Challenges）



*   **核心问题**：如何处理极端尺度目标（如 “遥感图像中的 10 像素小目标”“工业场景中的 1000 像素大目标”）和复杂背景（如 “森林中的动物”“人群中的人脸”）？

*   **讨论方向**：

1.  极端尺度适配：小目标特征如何增强（如 “超分辨率重建 + 检测”）？大目标如何避免特征冗余（如 “分块检测 + 拼接”）？

2.  背景抑制：复杂背景中的干扰物（如 “森林中的树叶”）易被误检，如何通过 “背景建模 + 负样本挖掘” 提升背景抑制能力？

#### 四、自监督学习的潜力（Potential of Self-Supervised Learning）



*   **核心问题**：自监督学习（无需人工标注）能否进一步提升目标检测性能，降低对标注数据的依赖？

*   **讨论方向**：

1.  预训练任务设计：如何设计有效的自监督预训练任务（如 “图像重构”“对比学习”），让模型学习到更通用的目标特征？

2.  少样本迁移：自监督预训练模型在少样本检测场景（如 “仅 10 张标注样本”）下，性能是否优于有监督预训练？

#### 五、高效轻量化检测器设计（Efficient Lightweight Detector Design）



*   **核心问题**：未来轻量化检测器的发展方向是什么？如何在 “参数更少（<1M）、计算量更低（<1G FLOPs）” 的前提下，保持高精度？

*   **讨论方向**：

1.  新型网络结构：是否需要摒弃传统 CNN，采用 Transformer-based 轻量化结构（如 MobileViT）？

2.  任务融合：能否将 “检测 + 分类 + 分割” 融合为单一轻量化模型，减少多任务的冗余计算？

3.  边缘计算适配：如何设计适合边缘设备（如物联网传感器）的超轻量化检测器，满足低功耗、低延迟需求？