# 媒体与认知 — 特征工程 I 讲义（详细版）

## 2-1.pdf 第 1-20 页内容

### 第 1 页

媒体与认知（Media and Cognition）

讲座 2：特征工程 I（Feature Engineering-I）

清华大学电子工程系（Dept. of EE, Tsinghua University）

方璐（Lu FANG）

***

### 第 2 页

**原始媒体：大规模高维数据（Raw media: Big & High-Dimensional Data）**

*   **高维（High-Dimensions）**：意味着有大量特征（features）

*   **脑磁图（Magnetoencephalography, MEG）**：脑成像技术，120 个位置 × 500 个时间点 × 20 个对象

*   **文档分类（Document classification）**：数百万个单词、双词（bigrams）、上下文信息


    *   例如 Netflix 用户评分矩阵：480,189 用户 × 17,770 电影 → 80 亿个数据点

|        | 电影 1 | 电影 2 | 电影 3 | 电影 4 | 电影 5 | 电影 6 |
| ------ | ---- | ---- | ---- | ---- | ---- | ---- |
| Tom    | 5    | ?    | ?    | 1    | 3    | ?    |
| George | ?    | ?    | 3    | 1    | 2    | 5    |
| Susan  | 4    | 3    | 1    | ?    | 5    | 1    |
| Beth   | 4    | 3    | ?    | 2    | 4    | 2    |

**专业术语解释**：

*   **特征（Feature）**：数据中具有代表性的属性或模式

*   **双词（Bigrams）**：文本中连续出现的两个词，用于捕捉上下文关系

*   **高维数据（High-Dimensional Data）**：特征数量非常多的数据，通常面临维度灾难问题

### 第 5 页

**目标：数据背后的信息（Aiming for information behind data）**

*   **特征工程 I（Feature Engineering-I）**：学习原始数据的低维表示（讲座 2）

*   **特征工程 II（Feature Engineering-II）**：统计分析各种特征以实现机器智能（讲座 3）

1.  **通用特征（General features）**：普遍适用，如**PCA**

2.  **检测特征（Detecting features）**：以人类认知方式，如角点检测

3.  **描述特征（Describing features）**：寻找并量化数据间的表示，如**SIFT**

4.  **学习特征（Learnt features）**：基于学习的特征检测与描述方法

> [!NOTE]
>
> **测验（Quiz）**：使用 PCA 可视化数据集

**专业术语解释**：

*   **低维表示（Lower Dimensional Representation）**：用较少的特征来表示原始高维数据

*   **PCA（Principal Component Analysis）**：主成分分析，一种常用的数据降维方法

*   **SIFT（Scale-Invariant Feature Transform）**：尺度不变特征变换，一种图像局部特征描述算法

### 第 6 页

**主成分分析（Principal Component Analysis, PCA）**

*   **定义**：一种无监督技术（Unsupervised technique），用于从高维数据集中提取方差结构

*   **降维方法**：将数据正交投影（Orthogonal projection）或转换到低维子空间，使投影数据的方差最大化

*   **优势**：


    *   较小的数据集更易于探索和可视化
    *   使机器学习算法分析数据更快更高效

*   **应用**：模式识别和信号处理领域最流行的多元统计技术之一

**专业术语解释**：

*   **无监督学习（Unsupervised Learning）**：不需要人工标注数据的机器学习方法

*   **正交投影（Orthogonal Projection）**：将数据点投影到某个子空间，投影方向与子空间垂直

*   **方差（Variance）**：数据分布的离散程度，方差越大表示数据差异越明显



***

### 第 7 页

**最大化方差（Maximizing the Variance）**



*   考虑以下两种投影方式

*   哪一种最大化了方差？

(图片对比选项 A 和选项 B)

*图片来源：Andrew Ng（CS229 讲义）*

**专业术语解释**：



*   **投影（Projection）**：将高维数据映射到低维空间的过程

*   **方差最大化（Variance Maximization）**：PCA 的核心思想，选择能保留数据最大差异性的投影方向



***

### 第 8 页

**PCA 玩具示例（PCA Toy Example）**

考虑以下 3D 点，如果每个分量用 1 字节存储，需要 18 字节（3×6）：



```
1 2

3 4 8

12 9 10

6 12 9 12

18
```

仔细观察，所有点在几何上都相关：它们都是同一点的不同比例缩放：



```
\[1; 2; 3] = 1 \* \[1; 2; 3]

\[2; 4; 6] = 2 \* \[1; 2; 3]

\[4; 8; 12] = 4 \* \[1; 2; 3]

\[3; 6; 9] = 3 \* \[1; 2; 3]

\[5; 10; 15] = 5 \* \[1; 2; 3]

\[6; 12; 18] = 6 \* \[1; 2; 3]
```

它们只需 9 字节即可存储（节省 50%）：



*   存储一个点（3 字节）+ 缩放系数（6 字节）

**专业术语解释**：



*   **数据冗余（Data Redundancy）**：数据中存在的重复或可推导的信息

*   **数据压缩（Data Compression）**：减少数据存储空间的技术



***

### 第 9 页

**几何解释（Geometrical Interpretation）**



*   每个点在 3D 空间中

*   在这个例子中，所有点恰好都在一条直线上：


    *   原始 3D 空间中的 1D 子空间

**专业术语解释**：



*   **子空间（Subspace）**：一个向量空间中满足特定条件的子集，本身也是一个向量空间

*   **维度（Dimension）**：描述数据所需的最小参数数量



***

### 第 10 页

**几何解释**



*   考虑一个新的坐标系，其中一个轴沿着直线方向

*   每个点只有一个非零坐标：只需存储直线方向（3 字节）和每个点的非零坐标（6 字节）

**问题**：给定一组点，如何知道它们是否可以这样压缩？



*   答案是查看点之间的**相关性（Correlation）**

*   工具：PCA

**专业术语解释**：



*   **相关性（Correlation）**：变量之间的线性关系程度

*   **坐标系变换（Coordinate System Transformation）**：改变描述数据的参考框架



***

### 第 11 页

**PCA**



*   通过计算**协方差矩阵（Covariance Matrix）的特征值（Eigenvalues）和特征向量（Eigenvectors）**，特征值最大的特征向量对应数据集中相关性最强的维度（主成分）

*   将数据投影到多个正交子空间

*   PCA 在以下领域有广泛应用：


    *   人脸识别（Face Recognition）
    
    *   图像压缩（Image Compression）
    
    *   高维数据模式发现

**专业术语解释**：



*   **协方差矩阵（Covariance Matrix）**：描述变量之间协方差关系的矩阵

*   **特征值（Eigenvalue）**：线性变换中保持方向不变的缩放因子

*   **特征向量（Eigenvector）**：线性变换中仅被缩放而方向保持不变的向量

*   **主成分（Principal Component）**：数据方差最大的方向



***

### 第 12 页

**PCA 定理（PCA Theorem）**

设 x₁, x₂, ..., xₙ是 n 个 N×1 向量，x̄是它们的平均值：



```
xᵢ = \[xᵢ₁; xᵢ₂; ...; xᵢₙ]

x̄ = (1/n) \* Σᵢ₌₁ⁿ \[xᵢ₁; xᵢ₂; ...; xᵢₙ]
```

设 X 是 N×n 矩阵，其列是 x₁-x̄, x₂-x̄, ..., xₙ-x̄：



```
X = \[x₁-x̄, x₂-x̄, ..., xₙ-x̄]
```

**注意**：减去均值相当于将坐标系平移到均值位置（Mean Centering）

**专业术语解释**：



*   **均值中心化（Mean Centering）**：将每个数据点减去所有数据的均值，使数据围绕原点分布

*   **矩阵（Matrix）**：二维数据结构，由行和列组成



***

### 第 13 页

**PCA 定理**

设 M = XXᵀ是 N×N 矩阵：



```
M = XXᵀ = \[x₁-x̄, x₂-x̄, ..., xₙ-x̄] \* \[ (x₁-x̄)ᵀ; (x₂-x̄)ᵀ; ...; (xₙ-x̄)ᵀ ]
```

**特性**：



*   M 是方阵（Square Matrix）

*   M 是对称矩阵（Symmetric Matrix）

*   M 是协方差矩阵（Covariance Matrix）

*   M 可能非常大（例如图像数据中，N 通常是像素数量）

**专业术语解释**：



*   **方阵（Square Matrix）**：行数和列数相等的矩阵

*   **对称矩阵（Symmetric Matrix）**：矩阵等于其转置的矩阵

*   **协方差（Covariance）**：衡量两个变量变化趋势一致性的统计量



***

### 第 14 页

**PCA 定理**

计算 M 的特征值和特征向量：



```
Me = λe

(M-λI)e = 0
```

**计算步骤**：



1.  计算 M-λI 的行列式（得到多项式）

2.  求多项式的根（得到特征值）

3.  对每个特征值，解 (M-λI) e=0（得到特征向量）

**专业术语解释**：



*   **行列式（Determinant）**：一个标量值，可以从方阵的元素计算出来，反映了矩阵所代表的线性变换对体积的影响

*   **单位矩阵（Identity Matrix）**：主对角线为 1，其他元素为 0 的方阵



***

### 第 15 页

**PCA 定理**

每个 xⱼ可以表示为：



```
xⱼ = x̄ + Σᵢ₌₁ⁿ gⱼᵢ eᵢ
```

其中 eᵢ是 M 的非零特征值对应的特征向量

**特性**：



*   特征向量 e₁, e₂, ..., eₙ张成特征空间（Eigenspace）

*   特征向量是 N×1 的正交向量（Orthonormal Vectors）

*   标量 gⱼᵢ是 xⱼ在该空间中的坐标：gⱼᵢ = (xⱼ-x̄)・eᵢ

**专业术语解释**：



*   **特征空间（Eigenspace）**：由特征向量张成的向量空间

*   **正交向量（Orthonormal Vectors）**：相互垂直且模长为 1 的向量

*   **点积（Dot Product）**：两个向量的相似度度量



***

### 第 16 页

**使用 PCA 压缩数据（Using PCA to Compress Data）**



*   用 e₁...eₙ表示 X 并没有改变数据大小

*   但如果点高度相关，许多坐标将为零或接近零

*   按特征值大小排序特征向量：λ₁≥λ₂≥...≥λₙ

*   假设当 i>k 时，λᵢ≈0

*   则 xⱼ≈x̄+Σᵢ₌₁ᵏ gⱼᵢ eᵢ

*   **注意**：n>k 表示 xⱼ位于低维线性子空间中

**专业术语解释**：



*   **数据压缩（Data Compression）**：减少数据存储空间的技术

*   **线性子空间（Linear Subspace）**：向量空间中满足特定条件的子集



***

### 第 17 页

**PCA 示例 —— 步骤 1（PCA Example – STEP 1）**

**数据**：



| x   | y   |
| --- | --- |
| 2.5 | 2.4 |
| 0.5 | 0.7 |
| 2.2 | 2.9 |
| 1.9 | 2.2 |
| 3.1 | 3.0 |
| 2.3 | 2.7 |
| 2   | 1.6 |
| 1   | 1.1 |
| 1.5 | 1.6 |
| 1.1 | 0.9 |

(左侧为原始数据图，右侧为减去均值后的数据图)

**专业术语解释**：



*   **数据可视化（Data Visualization）**：将数据以图形方式展示的技术

*   **散点图（Scatter Plot）**：将数据点在二维坐标系中绘制的图表



***

### 第 18 页

**PCA 示例 —— 步骤 2（PCA Example – STEP 2）**



*   计算协方差矩阵：



```
cov = \[\[0.616555556, 0.615444444],

&#x20;      \[0.615444444, 0.716555556]]
```

协方差矩阵非对角线元素为正，表明 X 和 y 变量一起增加



*   计算协方差矩阵的特征值和特征向量：



```
特征值 = \[0.0490833989, 1.28402771]

特征向量 = \[\[-0.735178656, 0.677873399],

&#x20;          \[-0.677873399, -0.735178656]]
```

**专业术语解释**：



*   **正相关（Positive Correlation）**：一个变量增加时，另一个变量也增加的关系

*   **特征分解（Eigendecomposition）**：将矩阵分解为特征值和特征向量的过程



***

### 第 19 页

**PCA 示例 —— 步骤 3（PCA Example – STEP 3）**

(图中显示减去均值后的数据，叠加了特征向量)

**观察**：



*   特征向量绘制为对角虚线

*   它们互相垂直

*   其中一个特征向量穿过点的中间，类似最佳拟合线

*   第二个特征向量代表数据中次要的变化模式

**专业术语解释**：



*   **最佳拟合线（Line of Best Fit）**：最能代表数据点趋势的直线

*   **主要成分（Primary Component）**：数据方差最大的方向

*   **次要成分（Secondary Component）**：与主要成分正交的数据第二大方差方向



***

### 第 20 页

**PCA 示例 —— 步骤 4（PCA Example – STEP 4）**



*   特征向量矩阵：



```
FeatureVector = \[eig₁, eig₂, eig₃, ..., eigₙ]
```

可以使用全部特征向量：



```
\[\[-0.677873399, -0.735178656],

&#x20;\[-0.735178656,  0.677873399]]
```

或者只保留更重要的成分：



```
\[-0.677873399, -0.735178656]ᵀ
```

**专业术语解释**：



*   **特征选择（Feature Selection）**：选择最具代表性特征的过程

*   **维度选择（Dimension Selection）**：选择保留的主成分数量

### 第 21 页

**PCA 示例：最终近似（PCA Example : Final Approximation）**

（左侧为原始 PCA 数据图，标注 “Original PCA data”；右侧为仅用单个特征向量恢复的原始数据图，标注 “Original data restored using only a single eigenvector”）



*   左侧图下方标注：2D 点云（2D point cloud）

*   右侧图下方标注：使用单个特征向量基的近似（Approximation using one eigenvector basis）

**关键说明**：若进行降维处理，在重构数据时，显然会丢失那些被选择舍弃的维度信息。在本示例中，假设我们只考虑 x 维度……

**专业术语解释**：



*   **数据重构（Data Reconstruction）**：从降维后的低维数据恢复出近似原始高维数据的过程

*   **特征向量基（Eigenvector Basis）**：由特征向量构成的用于表示数据的坐标系

*   **2D 点云（2D Point Cloud）**：在二维空间中分布的离散点集合，每个点包含二维坐标信息



***

### 第 22 页

**需要多少个主成分？（How Many PCs?）**



*   对于 n 个原始维度，样本协方差矩阵（sample covariance matrix）为 n×n 矩阵，最多有 n 个特征向量，因此最多有 n 个主成分（PCs）。

*   **降维的来源**：可忽略那些重要性较低的成分。


    *   此过程会损失部分信息，但如果被忽略成分的特征值很小，损失的信息就很少。

**降维流程**：



1.  原始数据含 n 个维度

2.  计算 n 个特征向量和特征值

3.  根据特征值大小，仅选择前 p 个特征向量

4.  最终数据集仅含 p 个维度

**专业术语解释**：



*   **样本协方差矩阵（Sample Covariance Matrix）**：基于样本数据计算得到的协方差矩阵，用于描述样本变量间的协方差关系

*   **主成分（Principal Component, PC）**：PCA 中按特征值从大到小排序后的特征向量对应的维度，每个主成分代表数据的一个主要变化方向

*   **特征值重要性（Eigenvalue Importance）**：特征值的大小反映对应主成分所包含数据信息的多少，特征值越大，该主成分越重要



***

### 第 23 页

**PCA 示例 1：人脸识别（PCA Example 1: Facial Recognition）**



*   **核心任务**：基于面部图像识别特定人物

*   **关键需求**：需对眼镜、光线等干扰因素具有鲁棒性（robust）


    *   因此，不能直接使用原始的 256×256 像素数据

**专业术语解释**：



*   **人脸识别（Facial Recognition）**：通过分析面部图像特征，自动识别或验证个人身份的技术

*   **鲁棒性（Robustness）**：系统或算法在面对干扰、噪声、数据缺失等情况时，仍能保持稳定性能的能力

*   **像素数据（Pixel Data）**：图像的原始数据形式，每个像素代表图像中一个特定位置的亮度或颜色信息



***

### 第 24 页

**应用 PCA：特征脸（Applying PCA: Eigenfaces）**

**两种方法**：



*   **方法 A（Method A）**：为每个人构建一个 PCA 子空间，判断测试图像在哪个子空间中重构效果最佳

*   **方法 B（Method B）**：为整个数据集构建一个 PCA 数据库，然后基于权重进行分类

**示例数据集**：人脸图像集 —— 著名的 “特征脸” 方法（\[Turk & Pentland], \[Sirovich & Kirby]）



*   每张人脸图像 x 包含 256×256 个数值（对应每个位置的亮度，luminance）


    *   可将 x 视为ℜ²⁵⁶ˣ²⁵⁶空间中的向量（即 64K 维向量）

*   假设有 m 张人脸图像，记为 x₁, …, xₘ

*   构建中心化数据矩阵 X = \[x₁, ..., xₘ]

*   **计算问题**：协方差矩阵∑ = XXᵀ为 64K×64K 维度，规模极大（HUGE），计算难度高

**专业术语解释**：



*   **特征脸（Eigenfaces）**：通过对人脸图像数据集进行 PCA 处理，得到的特征向量对应的 “人脸” 图像，是人脸数据的主要特征表示

*   **PCA 子空间（PCA Subspace）**：由 PCA 主成分张成的低维向量空间，可用于数据的降维、重构与分类

*   **中心化数据矩阵（Centered Data Matrix）**：将每个数据样本减去样本均值后构成的矩阵，消除数据整体偏移对后续计算的影响

*   **亮度（Luminance）**：表示光的明暗程度，在灰度图像中，每个像素的亮度值反映该位置的明暗信息



***

### 第 25 页

**计算复杂度（Computational Complexity）**

**假设条件**：有 m 个样本，每个样本大小为 N



*   以特征脸为例：m=500 张人脸，每个样本大小 N=64K（即 256×256 像素）

**计算成本**：



*   对于 N×N 的协方差矩阵：


    *   计算所有 N 个特征向量和特征值，时间复杂度为 O (N³)
    
    *   计算前 k 个特征向量和特征值，时间复杂度为 O (kN²)

*   **关键问题**：当 N=64K 时，上述计算成本极高（EXPENSIVE）！

**疑问**：是否存在解决方案？

**专业术语解释**：



*   **计算复杂度（Computational Complexity）**：衡量算法执行所需计算资源（如时间、空间）与问题规模之间关系的指标，通常用大 O 符号（O-notation）表示

*   **时间复杂度（Time Complexity）**：算法执行所需时间随问题规模增长的变化趋势

*   **大 O 符号（O-notation）**：用于描述算法复杂度的数学符号，如 O (N³) 表示算法时间随问题规模 N 的三次方增长

### 第 28 页

**PCA 示例 2：图像压缩（PCA Example 2: Image Compression）**

（左侧为 L2 误差与 PCA 维度关系图，标注 “L2 error and PCA dim”；横轴为 PCA 维度（PCA dim），取值 10、25、50、100、150；纵轴为 L2 误差（L2 error），取值 0、0.05、0.1、0.15、0.2、0.25、0.3、0.35）

**压缩流程**：



1.  将 372×492 像素的图像划分为图像块（patches）

2.  每个图像块作为一个样本，包含 12×12 像素（可视为 144 维向量，144-D vector）

**压缩效果**：



*   PCA 压缩 1：144 维（144D）→ 60 维（60D）

*   PCA 压缩 2：144 维（144D）→ 16 维（16D）

**专业术语解释**：



*   **图像压缩（Image Compression）**：通过减少图像数据中的冗余信息，在保证一定图像质量的前提下，降低图像存储容量或传输带宽的技术

*   **L2 误差（L2 Error）**：也称为欧几里得误差（Euclidean Error），是衡量原始数据与重构数据之间差异的指标，计算方式为两者对应元素差值的平方和的平方根

*   **图像块（Patch）**：将图像分割成的若干个小尺寸子图像，每个图像块可作为独立的样本用于特征提取或处理

*   **144 维向量（144-D Vector）**：将 12×12 像素的图像块按行或列展开后得到的包含 144 个元素的向量，每个元素对应一个像素的亮度值



***

### 第 29 页

**16 个最重要的特征向量（16 most important eigenvectors）**

（展示 16 个特征向量对应的图像，每个图像下方标注 “2 4 6 8 10 12”，代表图像块的像素坐标范围）

**说明**：这些特征向量是通过对 12×12 像素的图像块样本集进行 PCA 计算，根据特征值从大到小排序后，选取前 16 个特征向量得到的。它们代表了图像块数据中最主要的 16 种变化模式，可用于图像压缩、特征提取等任务。

**专业术语解释**：



*   **重要特征向量（Important Eigenvector）**：在 PCA 中，特征值较大的特征向量对应的特征向量，其包含的数据信息更多，对数据的代表性更强

*   **变化模式（Variation Pattern）**：特征向量所反映的数据在不同维度上的变化规律，在图像领域，每个重要特征向量对应一种典型的图像纹理或结构模式



***

### 第 30 页

**PCA 压缩：144 维→1 维（PCA Compression: 144D à 1D）**

（左侧为原始图像块集合，右侧为仅用 1 个最重要特征向量（即 1 维 PCA）重构后的图像块集合）

**对比分析**：



*   原始图像块包含丰富的细节信息，不同图像块的纹理、明暗差异清晰

*   1D PCA 重构后的图像块仅保留了数据最核心的变化模式，细节信息损失较多，但仍能大致反映原始图像块的整体明暗分布趋势

**专业术语解释**：



*   **1 维 PCA（1D PCA）**：仅选择 PCA 中特征值最大的 1 个主成分进行降维，将原始高维数据映射到 1 维空间

*   **图像块重构（Patch Reconstruction）**：利用降维后的数据和对应的主成分，反向计算恢复出近似原始图像块的过程

*   **细节信息损失（Detail Information Loss）**：在降维过程中，由于舍弃了部分特征值较小的主成分，导致重构数据丢失原始数据中细节信息的现象



***

### 第 31 页

**PCA：结论（PCA: Conclusions）**



*   **核心特性**：PCA 是完全无先验知识（knowledge free）的方法，即一种无监督学习技术（unsupervised learning technique），用于从高维数据集中提取隐藏的（潜在低维的）结构。

*   **主要用途**：

1.  **可视化（Visualization）**：将高维数据降维到 2D 或 3D 空间，便于观察数据分布和模式

2.  **资源高效利用（More efficient use of resources）**：减少数据存储、计算时间和通信带宽等资源消耗

3.  **统计优势（Statistical Advantage）**：维度减少有助于提升机器学习算法的泛化能力（generalization）

4.  **噪声去除（Noise Removal）**：通过舍弃含噪声的次要主成分，提升数据质量

5.  **机器学习预处理（Further processing by machine learning algorithms）**：作为机器学习任务的前置步骤，降低数据维度，提高算法效率和性能

**专业术语解释**：



*   **无先验知识（Knowledge Free）**：算法在处理数据时，不需要依赖任何预先已知的关于数据的领域知识或人工设定的规则

*   **无监督学习技术（Unsupervised Learning Technique）**：不需要人工标注训练数据类别或标签，仅通过对数据本身的分析来挖掘数据内在规律和结构的学习方法

*   **泛化能力（Generalization）**：机器学习模型对未见过的新数据的适应能力，泛化能力越强，模型在新数据上的性能越好

*   **噪声去除（Noise Removal）**：消除或减弱数据中无关干扰信息（噪声）的过程，以提高数据的纯度和可用性



***

### 第 32 页

**PCA 足够吗？（Is PCA Enough?）**



*   **PCA 的优势**：作为一种通用特征分析方法（universal feature analysis method），适用于 1D 数据（如语音，voice）、2D 数据（如图像，image），甚至高维数据（如文档、统计数据，word files and statics）。

*   **PCA 的局限性**：无法检测人类真正关注的特征，如 “边缘（edges）”、“亮点（bright spot）” 和 “角点（corners）”。

*   **额外需求**：还需要一种方法来描述特征点，以便能在不同图像中区分同一个特征点。


    *   即需要 “检测器和描述符（detector and descriptor）”

**专业术语解释**：



*   **通用特征分析方法（Universal Feature Analysis Method）**：适用于多种不同类型、不同维度数据的特征提取和分析方法

*   **边缘（Edge）**：图像中灰度或颜色发生剧烈变化的区域，通常对应物体的轮廓或边界

*   **亮点（Bright Spot）**：图像中亮度明显高于周围区域的点或区域

*   **角点（Corner）**：图像中两个或多个边缘的交点，是图像中具有明显特征的关键位置

*   **检测器（Detector）**：用于在数据中检测出特定特征（如角点、边缘）位置的算法或模块

*   **描述符（Descriptor）**：用于对检测到的特征点进行量化描述，生成具有唯一性和区分性的特征向量的算法或模块



***

### 第 33 页

**两张图像之间有什么关系？（What is the relationship between 2 images?）**



*   **核心问题**：对应点（corresponding points）在哪里？

（图中展示 NASA 火星探测器（Mars Rover）拍摄的两张不同视角的火星表面图像，用于演示图像对应点匹配问题）

**后续问题**：你尝试匹配的是哪种类型的特征？

**专业术语解释**：



*   **对应点（Corresponding Points）**：在不同视角、不同时间或不同条件下拍摄的同一物体或场景的图像中，代表同一物理位置的点

*   **图像匹配（Image Matching）**：通过比较不同图像的特征，寻找图像间对应关系的过程，是立体视觉、图像拼接、目标跟踪等任务的基础

*   **火星探测器（Mars Rover）**：用于探测火星表面环境、地形和地质特征的无人航天器，其搭载的相机可拍摄火星表面图像



***

### 第 34 页



*   **操作任务**：在一张图像中选取一个点，在另一张图像中找到这个点。

*   **问题**：你会选择哪种类型的特征？

*   **答案**：角点（a corner）

**选择原因**：角点在图像中具有独特性和稳定性，其周围像素的灰度变化具有明显的各向异性（不同方向变化差异大），相比边缘（仅在垂直于边缘方向有变化）和平坦区域（无明显变化），更容易在不同图像中被准确识别和匹配。

**专业术语解释**：



*   **各向异性（Anisotropy）**：物体或现象在不同方向上表现出不同性质或特征的特性，在图像角点区域，灰度变化在不同方向上的强度和趋势差异显著

*   **特征独特性（Feature Uniqueness）**：特征在图像中具有与众不同的属性，能够与其他特征明确区分

*   **特征稳定性（Feature Stability）**：特征在不同拍摄条件（如视角、光照、尺度变化）下，仍能保持其核心属性和可识别性的能力



***

### 第 35 页

**如何找到角点？（How do you find a corner?）**



*   **识别依据**：通过小窗口（small window）观察即可轻松识别

*   **核心判断标准**：移动窗口时，亮度（intensity）应产生较大变化

**三种区域类型对比**：



1.  **平坦区域（flat region）**：在所有方向上移动窗口，亮度均无变化

2.  **边缘（edge）**：沿边缘方向移动窗口，亮度无变化；垂直于边缘方向移动，亮度变化明显

3.  **角点（corner）**：在所有方向上移动窗口，亮度均产生显著变化

**专业术语解释**：



*   **窗口（Window）**：在图像处理中，用于局部区域分析的矩形或其他形状的子区域，通过滑动窗口可对图像不同位置的局部特征进行提取和分析

*   **亮度（Intensity）**：图像中像素的明暗程度，在灰度图像中直接用像素值表示，在彩色图像中可通过亮度分量计算得到

*   **平坦区域（Flat Region）**：图像中灰度或亮度变化平缓、无明显结构特征的区域

*   **边缘（Edge）**：图像中灰度或亮度发生急剧变化的线状区域，通常对应物体的轮廓边界



***

### 第 36 页

**如何找到角点？（使用图像梯度）（How to find Corners? (using image gradient)）**

（图中左侧为原始图像，中间为 X 方向梯度图像（X derivative），右侧为 Y 方向梯度图像（Y derivative））

**步骤**：



1.  计算小区域内的图像梯度（compute image gradients over small region）

2.  对每个图像梯度减去均值（subtract mean from each image gradient）

3.  计算协方差矩阵（compute the covariance matrix）

4.  计算特征值和特征向量（compute eigenvectors and eigenvalues）

5.  基于特征值设定阈值检测角点（use threshold on eigenvalues to detect corners）

**专业术语解释**：



*   **图像梯度（Image Gradient）**：用于描述图像中像素灰度变化的速率和方向，通常包括 X 方向梯度（水平方向，X derivative）和 Y 方向梯度（垂直方向，Y derivative），是边缘检测、角点检测等任务的基础

*   **梯度均值减法（Gradient Mean Subtraction）**：将每个梯度值减去该区域内所有梯度的均值，消除梯度整体偏移的影响，使梯度数据围绕零点分布

*   **协方差矩阵（Covariance Matrix）**：在角点检测中，用于描述图像梯度在 X 和 Y 方向上的分布关系和相关性的矩阵

*   **阈值检测（Threshold Detection）**：设定一个阈值，将特征值大于该阈值的区域判定为角点的方法，阈值的选择会影响角点检测的数量和准确性



***

### 第 37 页

**1. 计算小区域内的图像梯度（非单个像素）（1. Compute image gradients over a small region (not just a single pixel)）**



*   **问题 1**：梯度分布（distribution）能告诉我们关于该区域的什么信息？


    *   答案：梯度分布可揭示边缘的方向（orientation）和幅度（magnitude）

*   **问题 2**：如何量化方向和幅度？

**专业术语解释**：



*   **梯度分布（Gradient Distribution）**：在图像的一个小区域内，所有像素的梯度值（包括 X 和 Y 方向）的整体分布情况

*   **边缘方向（Edge Orientation）**：边缘的走向，即图像灰度变化最剧烈的方向的垂直方向，通常用角度表示

*   **边缘幅度（Edge Magnitude）**：图像灰度变化的剧烈程度，幅度越大，边缘越明显

*   **量化（Quantization）**：将连续的数值（如梯度方向和幅度）转换为离散的、有限个数值的过程，便于后续的计算和分析



***

### 第 38 页

**2. 对每个图像梯度减去均值（2. Subtract the mean from each image gradient）**

（图中包含四组子图，分别为：恒定亮度图像（constant intensity）、梯度图像（gradient）、沿某条线的亮度值分布图（plot intensities along the line）、减去均值后的亮度值分布图（plot of image gradients after subtract mean））

**关键效果**：数据中心化（data is centered），即消除了 “直流偏移（DC offset）”

**专业术语解释**：



*   **恒定亮度图像（Constant Intensity Image）**：图像中所有像素的亮度值相同，无任何灰度变化的图像

*   **亮度值分布（Intensity Distribution）**：沿图像中某条直线或某个区域，像素亮度值的变化情况，通常用曲线图表示

*   **数据中心化（Data Centering）**：将数据减去其均值，使数据的平均值为零，消除数据整体偏移对后续统计分析（如协方差计算）的影响

*   **直流偏移（DC Offset）**：在信号或数据中，存在的一个恒定的、不随时间或空间变化的分量，在图像梯度中，直流偏移表现为梯度整体偏向某个非零值



***

### 第 39 页

**3. 计算协方差矩阵（3. Compute the covariance matrix）**

协方差矩阵 M 的计算公式：

$ 
M = \begin{bmatrix}
\sum_{p \in P} I_x I_x & \sum_{p \in P} I_x I_y \\
\sum_{p \in P} I_y I_x & \sum_{p \in P} I_y I_y
\end{bmatrix}
 $

**符号说明**：



*   $  P  $：图像中的小区域（patch）

*   $  I_x  $：X 方向梯度数组（array of x gradients）

*   $  I_y  $：Y 方向梯度数组（array of y gradients）

*   $  \sum_{p \in P} I_x I_x  $：区域 P 内所有像素 X 方向梯度的平方和（标量，scalar）

*   同理，$  \sum_{p \in P} I_x I_y  $、$  \sum_{p \in P} I_y I_x  $、$  \sum_{p \in P} I_y I_y  $分别为 X 与 Y 梯度的乘积和、Y 与 X 梯度的乘积和、Y 方向梯度的平方和

**核心作用**：通过协方差矩阵，对小图像区域内的梯度进行二次（平方项，quadratic）拟合，以描述梯度的分布特征。

**专业术语解释**：



*   **梯度平方和（Sum of Squared Gradients）**：对某个方向的梯度值进行平方后求和，反映该方向梯度的整体强度

*   **梯度乘积和（Sum of Gradient Products）**：X 方向梯度与 Y 方向梯度的乘积之和，反映 X 和 Y 方向梯度之间的相关性

*   **二次拟合（Quadratic Fitting）**：用二次函数来近似描述数据分布规律的方法，在角点检测中，通过对梯度的二次拟合，可更准确地分析区域的特征类型（平坦、边缘、角点）



***

### 第 40 页

$ 
M e = \lambda e
 $

（公式中，$  \lambda  $为特征值（eigenvalue），$  e  $为特征向量（eigenvector））

**4. 计算特征值和特征向量（我们在 PCA 部分已讨论过！）（4. Compute eigenvalues and eigenvectors (We have discussed in PCA section!)）**

$ 
(M - \lambda I) e = 0
 $

**计算步骤**：



1.  计算$  M - \lambda I  $的行列式（得到一个多项式）（Compute the determinant of $  M - \lambda I  $ (returns a polynomial)）

2.  求解多项式的根（得到特征值）（Find the roots of polynomial (returns eigenvalues)）

3.  对每个特征值，求解$  (M - \lambda I) e = 0  $（得到特征向量）（For each eigenvalue, solve $  (M - \lambda I) e = 0  $ (returns eigenvectors)）

**专业术语解释**：



*   **行列式（Determinant）**：一个与方阵相关的标量值，可通过特定公式计算得出，对于 2×2 矩阵$  \begin{bmatrix}a & b \\ c & d\end{bmatrix}  $，其行列式为$  ad - bc  $，行列式的值可用于判断矩阵是否可逆以及求解特征值等

*   **多项式根（Root of Polynomial）**：使多项式等于零的变量值，在特征值计算中，特征值就是特征方程（由行列式等于零得到的多项式方程）的根

*   **特征方程（Characteristic Equation）**：由$  \det(M - \lambda I) = 0  $得到的关于$  \lambda  $的多项式方程，用于求解矩阵 M 的特征值

### 第 41 页

**解读特征值（Interpreting Eigenvalues）**



*   设$\lambda_1$和$\lambda_2$为 2×2 矩阵$M$的两个特征值

*   **核心问题**：不同特征值组合对应哪种类型的图像块？

（图中以坐标系形式展示特征值分布，横轴为$\lambda_1$，纵轴为$\lambda_2$，标注 “$\lambda_1 \approx 0, \lambda_2 \approx 0$” 的区域代表平坦区域；标注 “$\lambda_1 \gg 0, \lambda_2 \approx 0$” 的区域代表边缘；标注 “$\lambda_1 \gg 0, \lambda_2 \gg 0$” 的区域代表角点）

**特征值与图像块类型的对应关系**：



1.  若$\lambda_1 \approx 0$且$\lambda_2 \approx 0$：区域内梯度变化极小，对应**平坦区域**

2.  若一个特征值远大于 0（如$\lambda_1 \gg 0$），另一个接近 0（如$\lambda_2 \approx 0$）：梯度变化主要集中在一个方向，对应**边缘**

3.  若$\lambda_1 \gg 0$且$\lambda_2 \gg 0$：梯度在两个正交方向均有显著变化，对应**角点**

**专业术语解释**：



*   **特征值分布（Eigenvalue Distribution）**：矩阵的特征值在数值空间中的分布情况，可反映矩阵所代表数据的内在结构

*   **梯度变化方向（Direction of Gradient Change）**：图像梯度变化最剧烈的方向，由特征向量方向决定，特征值大小反映该方向梯度变化的强度



***

### 第 42 页

**解读特征值（Interpreting Eigenvalues）**

（图中延续第 41 页的坐标系，进一步明确不同区域对应的图像块类型：平坦区域（flat）、边缘（edge）、角点（corner）的特征值范围）



*   **核心问题**：如何设计一个函数来量化 “角点响应（cornerness）”？


    *   即通过特征值计算一个数值，用于判断区域是否为角点，数值越大，角点特征越明显

**专业术语解释**：



*   **角点响应（Cornerness）**：衡量图像区域具备角点特征程度的量化指标，通过特征值或其他统计量计算得到

*   **量化函数（Quantization Function）**：将抽象的特征（如特征值组合）转换为具体数值的函数，便于后续的阈值判断和特征筛选



***

### 第 43 页

**5. 基于特征值阈值检测角点（5. Use threshold on eigenvalues to detect corners）**

**角点响应计算公式（Harris 角点响应函数）**：

$ 
R = \det(M) - \kappa \cdot trace^2(M)
 $

其中：



*   $\det(M)$：矩阵$M$的行列式（determinant），$\det(M) = \lambda_1 \cdot \lambda_2$

*   $trace(M)$：矩阵$M$的迹（trace），$trace(M) = \lambda_1 + \lambda_2$

*   $\kappa$：经验常数（empirical constant），通常取值范围为 0.04\~0.06，用于调整响应函数的灵敏度

**矩阵运算补充说明**：



*   对于 2×2 矩阵$\begin{bmatrix}a & b \\ c & d\end{bmatrix}$：


    *   行列式：$\det\begin{pmatrix}\begin{bmatrix}a & b \\ c & d\end{bmatrix}\end{pmatrix} = a \cdot d - b \cdot c$
    
    *   迹：$trace\begin{pmatrix}\begin{bmatrix}a & b \\ c & d\end{bmatrix}\end{pmatrix} = a + d$

**不同区域的角点响应值**：



*   平坦区域：$\lambda_1 \approx 0, \lambda_2 \approx 0$ → $R \approx 0$

*   边缘：一个特征值接近 0，另一个较大 → $R \approx -\kappa \cdot \lambda_1^2 < 0$（或$-\kappa \cdot \lambda_2^2 < 0$）

*   角点：两个特征值均较大 → $R = \lambda_1\lambda_2 - \kappa(\lambda_1+\lambda_2)^2 > 0$（且数值较大）

**专业术语解释**：



*   **行列式（Determinant）**：矩阵的一个标量属性，反映矩阵对向量的缩放和旋转效果，在角点检测中用于衡量两个特征值的乘积关系

*   **迹（Trace）**：矩阵主对角线元素之和，反映矩阵的整体 “能量”，在角点检测中用于衡量两个特征值的和关系

*   **经验常数（Empirical Constant）**：通过实验或经验总结得到的常数，用于调整算法性能，适应不同类型的图像数据



***

### 第 44 页

**Harris 角点检测器（Harris Detector）**



*   参考文献：C.Harris and M.Stephens. “A Combined Corner and Edge Detector.”1988.（经典角点检测算法论文）

**Harris 检测器步骤 1-3**：



1.  计算图像的 X 方向和 Y 方向导数（Compute X and y derivatives of image）：

$ 
   I_x = G_{\sigma}^x * I, \quad I_y = G_{\sigma}^y * I
    $

其中$G_{\sigma}^x$和$G_{\sigma}^y$分别为带高斯核的 X 方向和 Y 方向导数滤波器（Gaussian derivative filters），$*$表示卷积运算（convolution），$I$为原始图像。



1.  计算每个像素处导数的乘积（Compute products of derivatives at every pixel）：

$ 
   I_x^2 = I_x \cdot I_x, \quad I_y^2 = I_y \cdot I_y, \quad I_{xy} = I_x \cdot I_y
    $



1.  计算每个像素块内导数乘积的加权平均（Compute the weighted average inside the patch for each pixel）：

$ 
   S_{x^2} = w * I_{x^2}, \quad S_{y^2} = w * I_{y^2}, \quad S_{xy} = w * I_{xy}
    $

其中$w$为高斯窗口函数（Gaussian window function），用于对图像块内的像素进行加权，增强中心像素的影响，降低噪声干扰。

**专业术语解释**：



*   **高斯导数滤波器（Gaussian Derivative Filter）**：结合高斯平滑（Gaussian smoothing）和导数运算的滤波器，可在抑制噪声的同时计算图像梯度，$\sigma$为高斯核的标准差（standard deviation），控制平滑程度

*   **卷积运算（Convolution）**：图像处理中常用的线性运算，通过滤波器（卷积核）与图像的滑动相乘求和，实现特征提取（如边缘检测、梯度计算）

*   **高斯窗口函数（Gaussian Window Function）**：一种呈高斯分布的权重函数，窗口内像素的权重从中心向边缘逐渐减小，用于在局部区域分析中降低边缘像素的干扰，增强结果的平滑性



***

### 第 45 页

**Harris 角点检测器（Harris Detector）**

*   参考文献：C.Harris and M.Stephens. “A Combined Corner and Edge Detector.”1988.

**Harris 检测器步骤 4-6**：

4\. 定义每个像素处的矩阵$M$（Define the matrix at each pixel）：

$ 
   M(x, y) = \begin{pmatrix} S_{x^2}(x, y) & S_{xy}(x, y) \\ S_{xy}(x, y) & S_{y^2}(x, y) \end{pmatrix}
    $

其中$(x, y)$为像素坐标，$S_{x^2}$、$S_{y^2}$、$S_{xy}$分别为步骤 3 中得到的加权平均结果。

5\. 计算每个像素处的检测器响应值（Compute the response of the detector at each pixel）：

$ 
   R = \det(M) - k \cdot (trace(M))^2
    $

（公式中$k$与第 43 页的$\kappa$含义一致，均为经验常数）

6\. 对响应值$R$设定阈值并进行非极大值抑制（Threshold on value of R; compute non-max suppression）：



*   阈值筛选：保留$R$大于阈值的像素，初步判定为角点候选

*   非极大值抑制：在候选像素的局部邻域内，仅保留$R$值最大的像素，消除相邻重复的角点，得到最终的角点检测结果

**专业术语解释**：



*   **非极大值抑制（Non-Max Suppression, NMS）**：计算机视觉中常用的后处理技术，在特征检测（如角点、边缘、目标检测框）后，通过在局部区域内保留最大值、抑制非最大值，减少冗余特征，提高检测精度

*   **角点候选（Corner Candidates）**：通过阈值筛选得到的可能为角点的像素集合，需进一步处理（如非极大值抑制）以去除假阳性和重复项

*   **假阳性（False Positive）**：被错误判定为目标特征（如角点）的非目标像素，通常由噪声、纹理干扰等因素导致



***

### 第 51 页

**Harris 角点检测器的特性（Properties of the Harris Corner Detector）**



*   **问题 1**：是否具有旋转不变性？（Rotation invariant?）

（图中展示旋转前后的图像及角点检测结果：左侧为原始图像及检测到的角点（白色点），右侧为原始图像旋转一定角度后的图像及角点检测结果，两张图中角点的相对位置和数量一致）

**结论**：Harris 角点检测器具有旋转不变性。



*   原理：图像旋转时，局部区域的梯度分布模式不变，矩阵$M$的特征值大小仅与梯度强度相关，与方向无关，因此角点响应值$R$不变，角点检测结果不受旋转影响（椭圆旋转但形状（特征值）保持不变，角点响应$R$对图像旋转具有不变性）。

**专业术语解释**：



*   **旋转不变性（Rotation Invariance）**：算法或特征在图像旋转后，仍能保持检测结果（如角点位置、特征描述）一致性的特性，是计算机视觉中重要的特征鲁棒性指标

*   **梯度分布模式（Gradient Distribution Pattern）**：图像局部区域内梯度的方向和强度分布规律，旋转不会改变该模式的内在结构，仅改变整体方向



***

### 第 52 页

**Harris 角点检测器的特性（Properties of the Harris corner detector）**



*   **问题 1 回顾**：旋转不变性？（是，如第 51 页结论）

*   **问题 2**：是否具有尺度不变性？（Scale invariant?）

（图中展示不同尺度下的图像及 Harris 检测结果：上方为小尺度图像（物体较小），标注 “corner!”（角点），Harris 检测器能准确检测到角点；下方为同一物体的大尺度图像（物体放大），原角点区域在小窗口下呈现边缘特征，标注 “edge!”（边缘），Harris 检测器无法检测到角点）

**结论**：Harris 角点检测器不具有尺度不变性。



*   原理：尺度变化会改变图像局部区域的细节特征 —— 小尺度下的角点，在大尺度下可能表现为边缘（局部窗口内仅一个方向有梯度变化）；反之，大尺度下的角点，在小尺度下可能被噪声干扰无法检测。由于 Harris 检测器的窗口大小和阈值固定，无法适应不同尺度的图像，因此不具备尺度不变性。

**专业术语解释**：



*   **尺度不变性（Scale Invariance）**：算法或特征在图像尺度（物体大小）变化后，仍能准确检测或描述目标特征的特性，是解决多尺度目标检测、匹配的关键

*   **固定窗口（Fixed Window）**：Harris 检测器中用于计算梯度和协方差矩阵的窗口大小固定，无法根据图像尺度动态调整，导致对尺度变化敏感

*   **尺度变化（Scale Change）**：图像中物体的尺寸放大或缩小，会改变局部区域的像素分布和梯度变化范围



***

### 第 53 页

**如何实现具有尺度不变性的特征检测器？（How can we make a feature detector scale-invariant?）**



*   **核心问题**：如何自动选择尺度？（How can we automatically select the scale?）

（图中展示不同尺度下的花朵图像：左侧为小尺度花朵图像（Flowers in small scale），右侧为大尺度花朵图像（Flowers in large scale），标注 “但它们都是花朵！（But they are all flowers!）”，强调需在不同尺度下识别同一物体的特征）

**尺度选择的核心思路**：



*   为图像构建多尺度空间（multi-scale space），在不同尺度下分析特征

*   选择能最准确反映特征本质的 “特征尺度（characteristic scale）”，即在该尺度下特征响应值最大

*   常用方法：通过高斯金字塔（Gaussian pyramid）构建多尺度空间，结合拉普拉斯算子（Laplacian）或高斯差分（Difference of Gaussians, DoG）筛选特征尺度

**专业术语解释**：



*   **多尺度空间（Multi-scale Space）**：通过对图像进行不同程度的高斯平滑和下采样，构建一系列不同尺度的图像集合，用于在不同尺度下分析图像特征

*   **特征尺度（Characteristic Scale）**：最能体现目标特征（如角点、边缘）本质的尺度，在该尺度下特征响应值最大，可通过多尺度空间中的响应峰值确定

*   **高斯金字塔（Gaussian Pyramid）**：构建多尺度空间的常用结构，通过对图像反复进行高斯平滑和 2 倍下采样（宽高各减半），得到尺度逐渐减小的图像层，每层包含不同尺度的图像



***

### 第 54 页

**使用拉普拉斯滤波器进行尺度选择（Using Laplacian filter for scale selection）**

（图中展示原始信号与不同尺度拉普拉斯滤波后的响应：左侧为原始信号曲线，中间为标准差$\sigma=1$的拉普拉斯滤波器（Laplacian ($\sigma=1$)）与原始信号的卷积结果，右侧为其他尺度拉普拉斯滤波的响应对比）

**核心原理**：



*   拉普拉斯算子（Laplacian）是一种二阶导数算子，对图像中的局部极值（如角点、边缘）敏感，其响应值的绝对值越大，表明该位置的特征越显著

*   **尺度选择规则**：当滤波器的尺度（由高斯核标准差$\sigma$决定）与信号的特征尺度一致时，滤波响应值最大（Highest response when the signal has the same characteristic scale as the filter）


    *   小尺度拉普拉斯滤波器（小$\sigma$）：对小尺寸特征（如小角点、细边缘）响应强
    
    *   大尺度拉普拉斯滤波器（大$\sigma$）：对大尺寸特征（如大角点、粗边缘）响应强

**专业术语解释**：



*   **拉普拉斯算子（Laplacian Operator）**：一种二阶微分算子，在图像处理中用于计算图像的二阶导数，反映像素灰度的变化率的变化，可用于边缘检测和特征尺度选择，表达式为$\nabla^2 I = \frac{\partial^2 I}{\partial x^2} + \frac{\partial^2 I}{\partial y^2}$（$I$为图像）

*   **高斯核标准差（Standard Deviation of Gaussian Kernel）**：控制高斯平滑的程度，$\sigma$越小，平滑程度越弱，保留细节越多；$\sigma$越大，平滑程度越强，去除噪声越明显，同时也决定了滤波器的尺度

*   **滤波响应（Filter Response）**：图像与滤波器进行卷积运算后得到的结果，响应值的大小反映图像中该位置与滤波器特征的匹配程度



***

### 第 55 页

**特征尺度 —— 产生最大滤波响应的尺度（characteristic scale - the scale that produces peak filter response）**

（图中包含两张图：左侧为特征响应值与尺度（$\sigma$）的关系曲线，纵轴为响应值（如 2000、1500 等），横轴为尺度参数，曲线在某一尺度处达到峰值，标注 “characteristic scale”（特征尺度）；右侧为不同尺度的图像（Full size（原尺寸）、3/4 size（3/4 尺寸））及对应的滤波响应，峰值位置对应特征尺度）

**特征尺度选择流程**：



1.  构建多尺度空间：对原始图像使用不同$\sigma$的高斯核进行平滑，得到一系列不同尺度的图像

2.  对每个尺度的图像应用拉普拉斯滤波器，计算每个像素的滤波响应值

3.  对每个像素，在其对应的多尺度响应值中寻找峰值，峰值对应的尺度即为该像素的特征尺度

4.  保留特征尺度下响应值大于阈值的像素，作为多尺度特征点

**问题**：应用不同尺度的拉普拉斯滤波器后会发生什么？（What happened when you applied different Laplacian filters?）



*   答案：不同尺度的滤波器对不同尺寸的特征产生响应，只有当滤波器尺度与特征尺度匹配时，才会产生最大响应，据此可确定每个特征的最优尺度。

**专业术语解释**：



*   **响应峰值（Response Peak）**：在多尺度响应值中，数值最大的点，对应特征的最优尺度

*   **多尺度响应值（Multi-scale Response Values）**：同一像素在不同尺度图像上的滤波响应值集合，用于确定特征尺度

*   **阈值筛选（Threshold Screening）**：在多尺度特征点中，保留响应值大于阈值的点，去除噪声导致的假特征点



***

### 第 56 页

（无明确标题，为特征尺度选择的示例图）

（图中展示不同$\sigma$值（如$\sigma=4.2$、$\sigma=2.1$等）的拉普拉斯滤波器对图像的响应，每个尺度的图像上标注 “peak!”（峰值），指示该尺度下特征响应达到峰值，对应特征尺度）

**示例观察**：



*   图像中的同一特征（如某一角点）在不同$\sigma$的滤波器下，响应值不同

*   在$\sigma=2.1$和$\sigma=4.2$的尺度下，特征响应值达到峰值，表明这两个尺度为该特征的特征尺度（具体选择需结合图像整体尺度和特征大小）

*   过大或过小的$\sigma$（如$\sigma=15.5$、$\sigma=1$）会导致响应值降低，无法准确捕捉特征

**专业术语解释**：



*   **特征匹配度（Feature Matching Degree）**：滤波器尺度与特征尺度的匹配程度，匹配度越高，滤波响应值越大

*   **多尺度特征点（Multi-scale Feature Points）**：在多尺度空间中，通过特征尺度选择得到的、具有尺度不变性的特征点，可适应不同尺寸的目标



***

### 第 57 页

（无明确标题，为特征尺度选择的补充示例图）

（图中展示不同尺度的图像块及对应的拉普拉斯响应，每个图像块下方标注 “peak!”，表明在该尺度下特征响应达到峰值，进一步验证特征尺度选择的有效性）

**说明**：此页面通过多个局部图像块的示例，强化 “特征尺度对应最大响应” 的核心概念，无论图像块的内容（如纹理、角点）如何，只有在匹配的尺度下才会产生峰值响应，为后续尺度不变特征检测（如 SIFT）奠定基础。



***

### 第 58 页

（无明确标题，延续第 57 页的示例，展示更多不同内容的图像块及对应的峰值响应尺度，标注 “peak!”，进一步验证特征尺度选择的通用性）

**核心结论**：特征尺度选择方法具有通用性，适用于不同类型的图像特征（如角点、纹理、边缘交点），通过多尺度滤波和峰值检测，可在不同尺寸的特征上找到最优尺度，为实现尺度不变性提供关键支撑。



***

### 第 59 页

**最优尺度（Optimal Scale）**

（图中展示原尺寸图像（Full size image）和 3/4 尺寸图像（3/4 size image）的最优尺度选择结果：左侧为原尺寸图像，下方标注滤波器的$\sigma$值（2.1、4.2、6.0、9.8、15.5、17.0），其中$\sigma=4.2$和$\sigma=6.0$处标注 “maximum response”（最大响应）；右侧为 3/4 尺寸图像，同样标注$\sigma$值，$\sigma=2.1$和$\sigma=4.2$处标注 “maximum response”）

**最优尺度与图像尺寸的关系**：



*   原尺寸图像的最优尺度（$\sigma=4.2$、6.0）大于 3/4 尺寸图像的最优尺度（$\sigma=2.1$、4.2）

*   原理：图像尺寸缩小后，特征的物理尺寸也随之缩小，因此匹配的滤波器尺度（最优尺度）需相应减小，才能保持滤波器与特征的尺度匹配，产生最大响应

**专业术语解释**：



*   **最优尺度（Optimal Scale）**：即特征尺度，是使特征响应值最大的滤波器尺度，也是最能准确描述特征的尺度

*   **尺度一致性（Scale Consistency）**：图像尺寸与最优尺度之间的对应关系，图像尺寸缩小（或放大）时，最优尺度也随之缩小（或放大），以保持特征检测的准确性



***

### 第 60 页



*   **问题 1**：我们已经检测到了 “角点”，但这有什么用呢？（We have detected ‘corners’ but what is this useful for?）

*   **问题 2**：如果我们知道优质特征的位置，如何匹配它们？（If we know where the good features are, how to match them?）

*   **答案**：我们需要描述符（descriptors）

**核心逻辑**：



*   角点检测仅能确定特征的位置（和尺度），但无法区分不同角点的 “身份”—— 即无法判断图像 A 中的角点与图像 B 中的哪个角点对应同一物理位置

*   **描述符的作用**：对每个检测到的特征点（如角点），提取其局部邻域的特征信息（如梯度方向、灰度分布），并将其量化为一个固定长度的向量（描述子），通过比较不同特征点的描述子相似度，实现跨图像的特征匹配

**专业术语解释**：



*   **特征描述符（Feature Descriptor）**：用于量化描述特征点局部特征的向量，包含特征点邻域的灰度、梯度、纹理等信息，需具备唯一性（区分不同特征）和鲁棒性（抗噪声、光照变化）

*   **描述子相似度（Descriptor Similarity）**：衡量两个特征描述符之间相似程度的指标，常用欧氏距离（Euclidean distance）、汉明距离（Hamming distance）等计算，相似度越高，对应同一物理点的概率越大

*   **跨图像特征匹配（Cross-image Feature Matching）**：通过比较不同图像中特征点的描述符，寻找对应同一物理位置的特征点对，是立体视觉、图像拼接、目标跟踪等任务的核心步骤

### 第 61 页

**设计特征描述符（寻找图像不变量）（Designing a feature descriptor (the search for image invariants)）**



*   **核心问题 1**：什么是图像特征的最佳描述符？（What is the best descriptor for an image feature?）

*   **核心问题 2**：如何描述一个图像块？（How do we describe an image patch?）

*   **关键原则**：内容相似的图像块应具有相似的描述符（Patches with similar content should have similar descriptors）

**专业术语解释**：



*   **图像不变量（Image Invariants）**：图像中不随拍摄条件（如光照、视角、尺度、旋转）变化而改变的特征属性，是设计鲁棒特征描述符的核心依据

*   **图像块（Image Patch）**：以特征点为中心截取的固定大小的局部图像区域，是提取特征描述符的基本单位

*   **描述符相似性（Descriptor Similarity）**：衡量两个图像块描述符之间匹配程度的指标，相似性越高，表明两个图像块内容越接近



***

### 第 62 页

**设计特征描述符的挑战（Challenges of designing a feature descriptor）**



1.  **光度变换（Photometric transformations）**

*   包括光照强度变化（如明暗差异）、对比度变化、颜色通道偏移等

*   挑战：需使描述符对光照变化不敏感，例如通过归一化梯度、灰度均值减法等方式消除光度差异影响

1.  **几何变换（Geometric transformations）**

*   包括尺度变化（物体大小缩放）、平移（位置偏移）、旋转（角度转动）、透视变形（视角变化导致的形状扭曲）

*   挑战：需使描述符具备几何不变性，例如通过多尺度空间构建、方向归一化、透视校正等技术适应几何变换

（图中展示同一物体在不同光度和几何变换下的图像，标注 “objects will appear at different scales, translation and rotation”，说明物体在不同变换下的外观差异）

**专业术语解释**：



*   **光度变换（Photometric Transformations）**：影响图像像素亮度或颜色的变换，不改变物体的几何结构，仅改变外观明暗或色彩

*   **几何变换（Geometric Transformations）**：改变图像中物体位置、形状、尺寸或视角的变换，包括平移、旋转、缩放、剪切、透视等

*   **不变性（Invariance）**：描述符在特定变换（如光照、旋转）下保持数值稳定的特性，是判断描述符鲁棒性的关键指标



***

### 第 63 页

**图像块（Image Patch）**

**方法 1：直接使用图像块的像素值（Just use the pixel values of the patch）**

（图中展示 3×3 的图像块，像素值为 \[\[1,2,3],\[4,5,6],\[7,8,9]]，右侧为展开后的像素值向量 \[1,2,3,4,5,6,7,8,9]，标注 “vector of intensity values”）

**适用场景**：仅适用于几何形态（位置、尺度、旋转）和外观（光照、对比度）完全不变的图像

**局限性**：



*   对光照变化敏感：像素值随光照强度成比例变化，导致相同内容的图像块描述符差异大

*   对几何变换敏感：平移、旋转、缩放会改变像素的排列顺序和数值，无法匹配同一特征

*   别称：模板匹配（template matching）

**问题**：如何降低对绝对亮度值的敏感性？（How can you be less sensitive to absolute intensity values?）



*   解决思路：通过梯度计算、均值归一化等方式，消除绝对亮度的影响，保留相对亮度变化信息

**专业术语解释**：



*   **像素值向量（Vector of Intensity Values）**：将图像块的像素按行或列顺序展开得到的一维向量，直接作为描述符使用

*   **模板匹配（Template Matching）**：以某一图像块为模板，在另一图像中滑动模板并计算相似度，寻找最匹配区域的方法，对变换鲁棒性差

*   **绝对亮度值（Absolute Intensity Values）**：像素的实际亮度数值，受光照条件影响大，直接使用易导致匹配误差



***

### 第 64 页

**图像梯度（Image Gradients）**

**方法 2：使用像素差值（即图像梯度）（Use pixel differences）**

（图中展示 3×3 图像块，右侧为 X 方向梯度向量和 Y 方向梯度向量，标注 “vector of x derivatives” 和 “vector of y derivatives”）

**计算方式**：



*   X 方向梯度（水平梯度）：相邻像素在水平方向的差值，反映左右方向的亮度变化，如$I_x = I(x+1,y) - I(x,y)$

*   Y 方向梯度（垂直梯度）：相邻像素在垂直方向的差值，反映上下方向的亮度变化，如$I_y = I(x,y+1) - I(x,y)$

**核心优势**：对绝对亮度值具有不变性（Feature is invariant to absolute intensity values）



*   原理：若所有像素亮度增加或减少一个固定值，梯度值（差值）保持不变，因此不受光照整体偏移的影响

**问题**：如何降低对变形的敏感性？（How can you be less sensitive to deformations?）



*   解决思路：通过局部邻域梯度统计（如梯度直方图）、高斯加权等方式，平滑局部变形带来的梯度波动，增强描述符对小变形的鲁棒性

**专业术语解释**：



*   **图像梯度（Image Gradient）**：描述图像像素亮度在空间上变化率和方向的物理量，包括 X 和 Y 两个方向，是提取图像边缘、纹理等特征的基础

*   **梯度不变性（Gradient Invariance）**：梯度值不受绝对亮度偏移影响的特性，使描述符对光照整体变化具有鲁棒性

*   **局部变形（Local Deformation）**：图像局部区域因视角变化、物体形变等导致的像素位置轻微偏移或形状扭曲



***

### 第 65 页

**颜色直方图（Color Histogram）**

**方法 3：使用颜色直方图统计图像块的颜色分布（Count the colors in the image using a histogram）**

（图中展示图像块及对应的颜色直方图，横轴为颜色区间（colors），纵轴为该颜色区间的像素数量）

**计算方式**：



1.  将图像块的颜色空间（如 RGB）划分为若干个离散的颜色区间（ bins ）

2.  统计每个颜色区间内的像素数量，形成直方图

3.  将直方图归一化（如除以总像素数），得到颜色分布描述符

**核心优势**：对尺度和旋转具有不变性（Invariant to changes in scale and rotation）



*   原理：尺度变化（图像块放大 / 缩小）仅改变像素总数，不改变颜色分布比例；旋转变化仅改变像素位置，不改变颜色区间的像素数量，因此直方图保持稳定

**问题**：如何提高对空间布局的敏感性？（How can you be more sensitive to spatial layout?）



*   局限性：颜色直方图仅统计颜色分布，丢失像素的空间位置信息，例如 “左红右蓝” 和 “左蓝右红” 的图像块会有相同的直方图

*   解决思路：采用空间直方图（Spatial Histograms），在划分颜色区间的同时保留局部空间信息

**专业术语解释**：



*   **颜色直方图（Color Histogram）**：描述图像颜色分布的统计图表，反映不同颜色在图像中出现的频率，是一种简单的全局特征描述符

*   **颜色区间（Color Bins）**：将连续的颜色空间离散化为有限个区间，每个区间代表一类相似颜色，用于简化直方图计算

*   **空间布局（Spatial Layout）**：图像中像素的空间位置排列关系，是区分具有相同颜色分布但不同结构的图像块的关键信息



***

### 第 66 页

**空间直方图（Spatial Histograms）**

**方法 4：在空间 “单元”（cells）内计算直方图（Compute histograms over spatial ‘cells’）**

（图中展示将图像块划分为 4×4 的空间单元（cells），每个单元内计算颜色或梯度直方图，最终将所有单元的直方图拼接为总描述符）

**计算方式**：



1.  将图像块划分为多个互不重叠的空间单元（如 4×4 网格，共 16 个单元）

2.  对每个空间单元，单独计算颜色或梯度直方图

3.  按空间顺序（如从左到右、从上到下）将所有单元的直方图拼接，形成空间直方图描述符

**核心优势**：



*   保留粗略的空间布局（Retains rough spatial layout）：通过单元划分，区分不同区域的特征分布，例如 “左上亮、右下暗” 的图像块可与 “左上暗、右下亮” 的图像块区分

*   对小变形具有一定鲁棒性（Some invariance to deformations）：局部单元内的直方图对轻微位置偏移不敏感，可容忍小范围的几何变形

**问题**：如何实现完全的旋转不变性？（How can you be completely invariant to rotation?）

*   局限性：空间直方图的单元按固定方向（如水平 / 垂直）划分，旋转会改变单元内的特征分布，导致描述符变化

*   解决思路：通过方向归一化（如将特征点的主方向作为参考方向，旋转图像块使主方向对齐水平），使空间直方图不受旋转影响

**专业术语解释**：

*   **空间单元（Spatial Cells）**：将图像块均匀划分的小尺寸子区域，每个单元作为独立的统计单位，用于保留局部空间信息

*   **方向归一化（Orientation Normalization）**：以特征点的主方向（如梯度方向的峰值方向）为基准，旋转图像块使主方向与固定方向（如 X 轴）对齐，消除旋转对描述符的影响

*   **主方向（Dominant Orientation）**：特征点邻域内梯度方向出现频率最高的方向，代表特征的主要朝向，是实现旋转不变性的关键参考



***

### 第 67 页

**SIFT（尺度不变特征变换）（SIFT (Scale Invariant Feature Transform)）**

*   中文释义：图像局部特征描述子，具备旋转、尺度、亮度不变性（图像局部特征描述子：旋转､尺度､亮度不变性）

*   核心定位：SIFT 同时包含检测器（detector）和描述符（descriptor），是一套完整的多尺度特征提取与匹配方案

*   参考文献：Distinctive Image Features from Scale-Invariant Keypoints, IJCV, 2004.（经典 SIFT 算法论文）

**1. 多尺度极值检测（Multi-scale extrema detection）**

**目的**：在不同的尺寸下，仍然可以找到这些特征（在不同尺度下稳定检测特征）

*   示例：人眼 / 大脑对物体的识别和分类与物体的尺度（模糊程度）无关（e.g., 人眼 / 脑对物体的识别和分类和其尺度 (模糊程度) 无关）

**高斯金字塔构建（Building the Gaussian Pyramid）**：

1.  尺度空间划分：将尺度空间分为多个组（Octave），每组包含多个尺度层（Scale Level）

2.  高斯平滑：在每个组内，对图像反复应用不同标准差（$\sigma$）的高斯滤波器进行平滑，得到不同模糊程度的图像

3.  下采样：完成一个组的处理后，对该组最后一层图像进行 2 倍下采样（宽高各减半），作为下一组的初始图像

*   底层（第 1 组）为原始图像

*   第 2 组的每个像素是对第 1 组图像应用高斯模板后下采样得到的，以此类推

**不同 sigma 值的多尺度高斯核（Multi-scale Gaussian Kernels with different sigma values）**：

*   $\sigma$（标准差）决定高斯核的尺度：$\sigma$越小，平滑程度越弱，对应小尺度（细节丰富）；$\sigma$越大，平滑程度越强，对应大尺度（全局特征）

**专业术语解释**：

*   **SIFT（Scale Invariant Feature Transform）**：一种经典的图像局部特征提取算法，通过多尺度空间构建、极值检测、方向赋值和描述符生成，实现具有尺度、旋转、亮度不变性的特征描述

*   **高斯金字塔（Gaussian Pyramid）**：构建图像多尺度空间的核心结构，由多个组（Octave）组成，每组内图像尺度逐渐增大（模糊程度增加），组间图像尺寸逐渐减小（下采样）

*   **高斯滤波器（Gaussian Filter）**：基于高斯函数的线性平滑滤波器，用于消除图像噪声、降低分辨率，生成不同尺度的图像，其标准差$\sigma$控制平滑程度和尺度大小

*   **下采样（Subsampling）**：通过间隔选取像素的方式减小图像尺寸的操作，通常为 2 倍下采样（保留偶数行和偶数列像素），用于构建高斯金字塔的不同组



***

### 第 68 页

（无明确标题，为高斯金字塔构建的补充示意图）

（图中展示高斯金字塔的层级结构：从底层原始图像开始，逐层应用高斯平滑，每组包含 4 个尺度层，完成一组后下采样得到下一组的初始图像，标注 “Apply Gaussian filter”（应用高斯滤波器））

**关键细节**：



*   同一组内，相邻尺度层的高斯核标准差按固定比例递增（如$\sigma, k\sigma, k^2\sigma, ...$，$k$为比例因子，通常取$\sqrt{2}$）

*   组间下采样后，图像尺寸变为前一组的 1/4（宽高各减半），但尺度（模糊程度）是前一组的 2 倍，保证尺度空间的连续性

**专业术语解释**：



*   **尺度层（Scale Level）**：高斯金字塔中同一组内，通过不同$\sigma$的高斯平滑得到的图像层，每个尺度层对应一个特定的尺度

*   **比例因子（Scale Factor, k）**：控制同一组内相邻尺度层高斯核标准差的递增比例，确保尺度空间的均匀采样

*   **尺度连续性（Scale Continuity）**：高斯金字塔中，相邻组的尺度范围相互重叠，避免尺度信息丢失，确保多尺度极值检测的完整性



***

### 第 69 页

**1. 多尺度极值检测（Multi-scale extrema detection）**

（图中展示 “高斯金字塔（高斯金字塔）”“高斯差分金字塔（高斯差分金字塔）”“每组极值检测（每组极值检测）” 的流程关系，标注 “降采样（降采样）”“尺度参数（尺度参数）”“log”）

**尺度空间划分规则（Scale space is separated into octaves）**：



*   第 1 组（Octave 1）使用尺度$s$

*   第 2 组（Octave 2）使用尺度$2s$

*   以此类推，第$n$组使用尺度$2^{n-1}s$

**高斯差分金字塔（Difference of Gaussian, DoG）构建**：



1.  对每个组内的高斯尺度层图像，计算相邻两个尺度层的差值（后一层减前一层），得到 DoG 图像

*   例如：组内有 5 个高斯尺度层，可生成 4 个 DoG 尺度层

1.  DoG 图像反映了不同尺度下图像的灰度变化差异，其极值点对应图像中的潜在特征点（角点、边缘交点等）

**组间处理**：每个组处理完成后，将该组的高斯图像下采样 2 倍（尺寸变为 1/4），作为下一组的初始图像，开始下一组的尺度空间构建

**问题**：LOG 与 DOG 的关系？（LOG VS. DOG?）

*   答案：拉普拉斯算子（LOG）是高斯函数的二阶导数，可用于检测多尺度极值，但计算复杂度高；DOG 是高斯函数一阶导数的差分近似，计算效率更高，且能很好地近似 LOG 的极值检测效果，因此 SIFT 使用 DOG 替代 LOG 构建多尺度极值检测空间。

**专业术语解释**：



*   **高斯差分金字塔（Difference of Gaussian, DoG Pyramid）**：由高斯金字塔中相邻尺度层图像的差值构成，用于近似拉普拉斯算子，实现多尺度极值检测，是 SIFT 特征点检测的核心步骤

*   **拉普拉斯算子（Laplacian of Gaussian, LOG）**：高斯平滑后的二阶导数算子，对图像中的局部极值（如角点、边缘）敏感，是多尺度特征检测的经典方法，但计算成本高

*   **潜在特征点（Potential Keypoints）**：在 DoG 图像中检测到的极值点，需进一步筛选（如去除边缘、低对比度点）以得到最终的稳定特征点



***

### 第 70 页

**1. 多尺度极值检测（Multi-scale extrema detection）**



*   **高斯差分（Difference of Gaussian, DoG）**：标注 “Scale of Gaussian variance”（高斯方差尺度）

*   **极值检测规则**：若 DoG 图像中的某一点比其 26 个邻居的数值都大（极大值）或都小（极小值），则将该点选为候选特征点


    *   26 个邻居包括：当前图像中该点的 8 个相邻像素，以及上下相邻尺度层图像中对应位置的 9 个像素（上层 9 个 + 下层 9 个）
    
    *   标注 “Selected if larger than all 26 neighbors”（比所有 26 个邻居都大则选中）

**检测步骤**：



1.  遍历 DoG 金字塔中所有组的所有尺度层（除最顶层和最底层）

2.  对每个像素，与周围 26 个邻居（同层 8 个 + 上下层各 9 个）比较

3.  若该像素是局部极大值或极小值，标记为多尺度候选特征点

**专业术语解释**：



*   **局部极值（Local Extrema）**：在特定局部邻域内数值最大（局部极大值）或最小（局部极小值）的点，在 DoG 图像中，局部极值点对应图像中灰度变化剧烈的位置，是潜在的特征点

*   **候选特征点（Candidate Keypoints）**：通过多尺度极值检测得到的初步特征点集合，需进一步通过阈值筛选、边缘去除等步骤优化，得到稳定的最终特征点

*   **DoG 金字塔遍历（DoG Pyramid Traversal）**：逐组、逐尺度层、逐像素地检查 DoG 图像，寻找局部极值点的过程，是 SIFT 特征检测的核心操作



***

### 第 71 页

**2. 特征点定位（Keypoint localization）**

**方法**：对 DoG 尺度空间进行二阶泰勒级数展开（Second order Taylor series approximation of DoG scale-space），实现亚像素级（sub-pixel）的特征点精确定位

**泰勒展开公式**：

$ 
f(x) = f + \frac{\partial f^T}{\partial x} x + \frac{1}{2} x^T \frac{\partial^2 f}{\partial x^2} x
 $

其中：

*   $x = \{x, y, \sigma\}$：代表特征点的位置（$x,y$）和尺度（$\sigma$）

*   $f$：DoG 图像在当前候选点的函数值

*   $\frac{\partial f}{\partial x}$：一阶偏导数向量（梯度）

*   $\frac{\partial^2 f}{\partial x^2}$：二阶偏导数矩阵（黑塞矩阵，Hessian Matrix）

**极值求解**：对泰勒展开式求导并令导数为零，解得极值点位置：

$ 
x_m = -\left( \frac{\partial^2 f}{\partial x^2} \right)^{-1} \frac{\partial f}{\partial x}
 $

其中$x_m$为精确定位后的亚像素级特征点坐标

**额外筛选**：

*   低对比度去除：将$x_m$代入泰勒展开式，若$f(x_m)$小于阈值（通常取 0.03），则视为低对比度点，予以删除

*   边缘点去除：计算特征点处的黑塞矩阵，若矩阵的两个特征值比值过大（通常取 10），则视为边缘点（仅在一个方向有变化），予以删除

*   目的：保留强对比度、非边缘的稳定特征点

**专业术语解释**：

*   **亚像素定位（Sub-pixel Localization）**：通过数学插值（如泰勒级数展开）将特征点位置从像素级（整数坐）精确到亚像素级（小数坐标），提高特征点定位精度

*   **黑塞矩阵（Hessian Matrix）**：由函数的二阶偏导数构成的方阵，用于描述函数的局部曲率，在特征点定位中用于求解极值点和判断边缘点

*   **低对比度点（Low-Contrast Points）**：DoG 图像中响应值小的候选点，通常由噪声导致，稳定性差，需去除

*   **边缘点（Edge Points）**：在一个方向梯度变化大、另一个方向变化小的候选点，对尺度和旋转变化敏感，匹配稳定性差，需去除



***

### 第 72 页

**3. 方向赋值（Orientation assignment）**

**目的**：为每个特征点分配一个主方向（dominant orientation），使后续生成的描述符具备旋转不变性

**计算步骤**：

1.  确定特征点的尺度对应的高斯图像：对每个特征点，选取 DoG 金字塔中对应尺度的高斯平滑图像$L$（与 DoG 极值点所在尺度层对应的高斯层）

2.  计算特征点邻域内的梯度幅度和方向：

*   梯度幅度（magnitude）：

$ 
     m(x, y) = \sqrt{(L(x+1, y) - L(x-1, y))^2 + (L(x, y+1) - L(x, y-1))^2}
      $



*   梯度方向（orientation）：

$ 
     \theta(x, y) = \tan^{-1}\left( \frac{L(x, y+1) - L(x, y-1)}{L(x+1, y) - L(x-1, y)} \right)
      $

方向范围为 0°\~360°，按 10°\~15° 的间隔划分为 36 个 bin（区间）



1.  构建梯度方向直方图：以特征点为中心，取半径为 3σ（σ 为特征点尺度）的圆形邻域，将邻域内像素的梯度幅度按方向 bin 统计，得到方向直方图

2.  确定主方向：找到方向直方图中的峰值，峰值对应的方向即为特征点的主方向；若存在其他峰值（大于主峰值 80%），则为该特征点分配多个方向（生成多个描述符），提高匹配鲁棒性

**检测结果输出**：检测过程最终返回特征点的四元组信息$\{x, y, \sigma, \theta\}$，分别代表位置（x,y）、尺度（σ）、方向（θ）

**专业术语解释**：



*   **主方向（Dominant Orientation）**：特征点邻域内梯度方向出现频率最高的方向，通过方向直方图峰值确定，是实现描述符旋转不变性的关键

*   **梯度方向直方图（Gradient Orientation Histogram）**：统计特征点邻域内梯度方向分布的直方图，用于确定主方向，直方图的 bin 数量和邻域大小需根据尺度调整

*   **多方向分配（Multi-orientation Assignment）**：对存在多个高概率方向（峰值）的特征点，分配多个主方向并生成对应描述符，可提高特征匹配的召回率（减少漏匹配）



***

### 第 73 页

**特征点定位与选择（Key points localization and selection）**

（图中展示两张原始图像（Original images），标注（a）和（b），（a）图尺寸为 233×189，（b）图未标注尺寸；下方展示特征点筛选过程：



*   初始特征点（initial key points）：832 个

*   梯度阈值筛选后（gradient threshold）：729 个

*   比例阈值筛选后（ratio threshold）：536 个）

**筛选流程说明**：



1.  **初始特征点**：通过多尺度极值检测和亚像素定位得到的特征点集合，包含噪声点、边缘点等不稳定特征

2.  **梯度阈值筛选**：计算特征点邻域内的梯度幅度，去除梯度幅度小于阈值的点（低对比度点），保留 729 个

3.  **比例阈值筛选**：计算特征点黑塞矩阵的特征值比例，去除比例大于阈值的点（边缘点），最终保留 536 个稳定特征点

**核心目的**：通过多轮筛选，去除噪声、边缘等不稳定特征点，保留具有高对比度、非边缘特性的稳定特征点，为后续描述符生成和匹配提供高质量的特征基础

**专业术语解释**：



*   **特征点筛选（Keypoint Selection）**：通过一系列阈值和规则，从初始候选特征点中去除不稳定、低质量的点，保留高质量特征点的过程

*   **梯度阈值（Gradient Threshold）**：用于筛选低对比度点的阈值，梯度幅度低于该阈值的特征点被视为噪声，予以删除

*   **比例阈值（Ratio Threshold）**：用于筛选边缘点的阈值，黑塞矩阵特征值比例高于该阈值的特征点被视为边缘，予以删除

***

### 第 74 页

**4. 特征点描述符（Keypoint Descriptor）**

（图中展示 SIFT 描述符的生成过程：左侧为 “Image Gradients”（图像梯度），标注 “4 x 4 pixel per cell, 4 x 4 cells”（每个单元 4×4 像素，共 4×4 个单元）；中间为 “Gaussian weighting (sigma = half width)”（高斯加权，σ= 窗口宽度的一半）；右侧为 “SIFT descriptor”（SIFT 描述符），标注 “16 cells x 8 directions = 128 dims”（16 个单元 ×8 个方向 = 128 维））

**SIFT 描述符生成步骤**：



1.  **图像块旋转归一化**：以特征点的主方向为基准，将特征点邻域内的图像块旋转，使主方向对齐水平方向，消除旋转对描述符的影响

2.  **划分单元（Cells）**：将旋转后的图像块划分为 4×4 的单元网格（共 16 个单元），每个单元大小为 3σ×3σ（σ 为特征点尺度）

3.  **高斯加权**：对每个单元内的像素梯度幅度进行高斯加权，权重随距离特征点的距离增大而减小，增强中心像素的影响，降低边缘像素的干扰（高斯核 σ= 单元宽度的一半）

4.  **计算单元梯度方向直方图**：对每个单元，将梯度方向划分为 8 个 bin（0°\~360°，每 45° 一个 bin），统计每个 bin 内的梯度加权幅度之和，得到 8 维向量

5.  **拼接描述符**：将 16 个单元的 8 维向量按空间顺序拼接，得到 16×8=128 维的 SIFT 描述符

6.  **归一化**：对 128 维向量进行 L2 归一化，消除光照对比度变化的影响；若归一化后向量元素大于 0.2，则进行阈值截断并再次归一化，增强描述符的鲁棒性

**优缺点分析（Pros and Cons?）**：



*   **优点**：具备尺度、旋转、亮度不变性，描述符区分性强，匹配准确率高，广泛适用于图像拼接、目标识别、立体视觉等场景

*   **缺点**：计算复杂度高（128 维向量），实时性差；对严重透视变形、遮挡的鲁棒性较弱；专利保护限制商业使用

**专业术语解释**：



*   **SIFT 描述符（SIFT Descriptor）**：128 维的特征向量，通过统计特征点邻域内梯度方向和幅度的分布生成，具备多维度不变性，是 SIFT 算法的核心输出

*   **旋转归一化（Rotation Normalization）**：通过旋转图像块使特征点主方向对齐固定方向，消除旋转对描述符的影响，实现旋转不变性

*   **L2 归一化（L2 Normalization）**：将描述符向量除以其 L2 范数（向量元素平方和的平方根），使向量模长为 1，消除光照对比度变化对描述符的影响

*   **阈值截断（Threshold Truncation）**：对归一化后的描述符向量元素进行限制，将大于阈值（通常为 0.2）的元素截断为阈值，减少大梯度异常值对匹配的影响



***

### 第 75 页

**SURF（加速鲁棒特征）（SURF ('Speeded' Up Robust Features)）**



*   核心定位：SURF 是 SIFT 的加速版本，运行速度比 SIFT 快 3 倍（3 times faster）

*   **主要特性**：


    *   擅长处理模糊和旋转图像（Good at handling images with blurring and rotation）
    
    *   不擅长处理视角变化和光照变化（Not good at handling viewpoint change and illumination change）

**关键优化：盒式滤波器近似（Box filter approximation）**



*   **SIFT 与 SURF 的核心差异**：SURF 使用盒式滤波器（Box Filter）近似 SIFT 中的高斯差分（DoG），大幅降低计算复杂度

*   **盒式滤波器优势**：通过积分图像（Integral Image）可快速计算盒式滤波器的卷积结果，且支持不同尺度的并行计算，效率远高于高斯滤波器

（图中展示 “SURF features example”（SURF 特征示例），左侧为原始图像，右侧为检测到的 SURF 特征点（用圆圈标记，圆圈大小代表尺度））

**积分图像简介（A brief intro. of integral images in SURF）**

（图中展示原始图像（original image）和对应的积分图像（integral image）：



*   原始图像像素值：\[\[1,5,2],\[2,4,1],\[2,1,1]]

*   积分图像像素值：\[\[1,6,8],\[3,12,15],\[5,15,19]]）

**专业术语解释**：



*   **SURF（Speeded Up Robust Features）**：一种基于 SIFT 改进的特征提取算法，通过盒式滤波器和积分图像优化计算速度，保持了 SIFT 的部分鲁棒性，适用于实时性要求较高的场景

*   **盒式滤波器（Box Filter）**：一种简单的线性滤波器，滤波器窗口内所有像素权重相同（均为 1），计算卷积时只需求和窗口内像素值，效率高

*   **积分图像（Integral Image）**：一种快速计算图像任意矩形区域像素和的预处理技术，通过一次遍历生成积分图像，后续任意区域求和可在常数时间内完成

*   **并行计算（Parallel Computation）**：多个尺度的盒式滤波器卷积可同时进行，无需等待前一尺度完成，进一步提升 SURF 的计算速度



***

### 第 76 页

**积分图像的计算与应用（Calculation and Application of Integral Image）**



*   **积分图像定义**：设积分图像为$A(x,y)$，原始图像为$I(x,y)$，则：

$ 
  A(x,y) = \sum_{i=1}^x \sum_{j=1}^y I(i,j)
   $

即$A(x,y)$表示原始图像中从左上角（1,1）到（x,y）的矩形区域内所有像素值的和

**任意矩形区域求和公式**：



*   对于原始图像中左上角坐标为$(x_1,y_1)$、右下角坐标为$(x_2,y_2)$的矩形区域，其像素和为：

$ 
  \text{Sum}(x_1,y_1,x_2,y_2) = A(x_2,y_2) - A(x_1-1,y_2) - A(x_2,y_1-1) + A(x_1-1,y_1-1)
   $



*   标注 “Can find the sum of arbitrary block using 3 operations”（使用 3 次运算即可求得任意块的和）—— 实际为 3 次加减运算（减、减、加），总运算次数为常数

**核心优势**：无论矩形区域的大小如何，其像素和的计算都只需 3 次加减运算，时间复杂度为 O (1)，相比传统的逐像素求和（O (w×h)，w、h 为区域宽高），效率极大提升，为盒式滤波器的快速卷积提供基础

**专业术语解释**：



*   **矩形区域求和（Sum of Rectangular Block）**：计算图像中任意矩形子区域内所有像素值的总和，是盒式滤波器卷积、特征提取（如 Haar 特征）的核心操作

*   **常数时间复杂度（Constant Time Complexity, O (1)）**：算法的运行时间不随输入数据大小（如区域尺寸）变化，始终保持固定，是积分图像的核心优势

*   **预处理技术（Preprocessing Technique）**：积分图像是在特征提取前对原始图像进行的预处理，只需生成一次，后续可反复使用，不增加后续操作的时间成本



***

### 第 77 页

**积分图像示例（Integral image example）**

*   **问题**：右下角 2×2 正方形区域的像素和是多少？（What is the sum of the bottom right 2x2 square?）

（图中展示原始图像（image）和积分图像（integral image）：

*   原始图像像素值：\[\[1,5,2],\[2,4,1],\[2,1,1]]（3×3 图像）

*   积分图像像素值：\[\[1,6,8],\[3,12,15],\[5,15,19]]）

**步骤 1：确定目标区域坐标**

*   右下角 2×2 正方形区域的左上角坐标$(x_1,y_1)=(2,2)$，右下角坐标$(x_2,y_2)=(3,3)$（假设坐标从 1 开始）

**步骤 2：应用积分图像求和公式**

$ 
\begin{align*}
\text{Sum}(2,2,3,3) &= A(3,3) - A(1,3) - A(3,1) + A(1,1) \\
&= 19 - 8 - 5 + 1 \\
&= 7
\end{align*}
 $

**验证（传统逐像素求和）**：

*   目标区域像素值为 \[\[4,1],\[1,1]]，求和为 4+1+1+1=7，与积分图像计算结果一致，验证了积分图像的正确性

**专业术语解释**

*   **坐标系统（Coordinate System）**：在积分图像和区域求和中，需明确像素坐标的起始位置（如从 1 开始或从 0 开始），避免坐标偏移导致计算错误

*   **结果验证（Result Verification）**：通过传统方法（如逐像素求和）验证积分图像计算结果的正确性，确保预处理步骤无误差

*   **示例演示（Example Demonstration）**：通过具体的小尺寸图像示例，直观展示积分图像的计算过程和优势，便于理解核心原理

### 第 78 页

**Haar-like 特征（Haar-like Features）**

*   核心定位：首个实时人脸检测器（the first real-time face detector），由 Viola-Jones 提出，是基于机器学习的人脸检测算法的核心特征

**Haar-like 特征定义**：

*   考虑检测窗口（detection window）中特定位置的相邻矩形区域

*   计算每个矩形区域内的像素和，再求这些和的差值

*   公式：$\Delta = \text{dark} - \text{white} = \frac{1}{n} \sum_{\text{dark}}^n I(x) - \frac{1}{n} \sum_{\text{white}}^n I(x)$


    *   其中 “dark” 代表深色区域，“white” 代表浅色区域，$n$为区域内像素数量，$\Delta$为两个区域的平均亮度差值

**Haar-like 特征类型**：

1.  边缘特征（Edge features）：如水平边缘、垂直边缘，对应（a）（b）（c）（d）图

2.  线特征（Line features）：如水平线条、垂直线条，对应（e）（f）图

3.  中心 - 环绕特征（Center-surround features）：中心区域与周围区域的对比，对应（g）（h）图

4.  特殊对角线特征（Special diagonal line feature）：对角线方向的线条特征，对应（a）（b）图

（图中展示不同类型的 Haar-like 特征示意图，标注特征的黑白矩形区域；下方展示人脸 Haar 特征应用示例，标注 “In human faces, the region of eyes is darker than the region of the cheeks.”（在人脸中，眼睛区域比脸颊区域暗））

**人脸检测中的典型 Haar 特征**：一对相邻的矩形，分别位于眼睛和脸颊区域，利用眼睛（深色）和脸颊（浅色）的亮度差异进行人脸识别

**计算速度优势**：由于使用积分图像，任意大小的 Haar-like 特征都可在常数时间内计算（Due to the use of integral images, a Haar-like feature of any size can be calculated in constant time）

**专业术语解释**：

*   **Haar-like 特征（Haar-like Features）**：基于 Haar 小波的矩形特征，通过相邻矩形区域的像素和差值描述图像局部的亮度变化模式，广泛用于目标检测（如人脸检测）

*   **检测窗口（Detection Window）**：在图像中滑动的矩形窗口，用于局部区域的特征提取和目标判断，窗口大小可根据目标尺寸调整

*   **实时检测（Real-time Detection）**：算法的检测速度能满足实时应用需求（如每秒处理 24 帧以上图像），Haar-like 特征的常数时间计算是实现实时性的关键

*   **Viola-Jones 算法（Viola-Jones Algorithm）**：基于 Haar-like 特征和级联分类器的实时人脸检测算法，是人脸检测领域的经典方法

### 第 79 页

**HOG（方向梯度直方图）（HOG (Histogram of Oriented Gradients for Human Detection, CVPR 2005)）**

*   核心定位：用于人体检测的方向梯度直方图，是一种基于局部梯度分布的特征描述符，在行人检测、人体识别中应用广泛

**HOG 特征提取流程**：

1.  输入图像（Input image）

2.  伽马校正与颜色归一化（gamma & colour Normalize）：对图像进行伽马校正（如$I' = I^\gamma$，$\gamma$通常取 0.5），调整图像对比度；将彩色图像转换为灰度图像，消除颜色干扰

3.  计算梯度（Compute gradients）：计算每个像素的 X 和 Y 方向梯度，得到梯度幅度和方向

4.  划分空间单元并统计梯度方向直方图（Compute orientation histograms into spatial cells）：将图像划分为固定大小的空间单元（如 8×8 像素），对每个单元计算梯度方向直方图（如 9 个 bin，0°\~180°，每 20° 一个 bin）

5.  块归一化（Weighted vote over overlapping spatial blocks & Contrast normalize over blocks）：将相邻的多个单元（如 2×2 单元）组成一个块（block），对块内所有单元的直方图进行 L2 归一化，增强对光照和对比度变化的鲁棒性；块之间通常重叠（如重叠 50%），提高特征密度

6.  收集 HOG 特征（Collect HOG's）：将所有块的归一化直方图拼接，形成 HOG 特征向量

7.  线性 SVM 分类（Linear SVM classification）：使用线性支持向量机（SVM）对 HOG 特征进行分类，判断是否为 “人（Person）” 或 “非人（non-person）”

（图中展示 HOG 特征的可视化结果：（a）平均梯度图像；（b）正 SVM 权重最大的块；（c）负 SVM 权重最大的块；（d）测试图像；（e）R-HOG 描述符；（f）（g）带正负 SVM 权重的 R-HOG 描述符）

**核心特性**：HOG 在密集网格上计算局部梯度方向的出现频率，通过块重叠和归一化提高检测精度，对人体的轮廓（尤其是头部、肩部、脚部）敏感，标注 “The most active blocks are centred on the image background just outside the contour.”（最活跃的块集中在轮廓外的图像背景上）

**专业术语解释**：



*   **HOG（Histogram of Oriented Gradients）**：方向梯度直方图，通过统计图像局部区域内梯度方向的分布来描述图像特征，尤其适合描述物体的轮廓结构，在人体检测中性能优异

*   **伽马校正（Gamma Correction）**：调整图像像素值的非线性变换，用于校正显示器的亮度非线性或增强图像的暗部 / 亮部细节，减少光照不均匀的影响

*   **空间单元（Spatial Cell）**：HOG 特征提取中的基本统计单位，通常为 8×8 或 16×16 像素，每个单元内计算梯度方向直方图

*   **块归一化（Block Normalization）**：对相邻单元组成的块进行梯度直方图归一化，消除局部光照和对比度变化的影响，是 HOG 特征鲁棒性的关键步骤

*   **线性 SVM（Linear Support Vector Machine）**：一种线性分类模型，通过寻找最优分类超平面区分正负样本（如人与非人），在 HOG 特征分类中计算效率高、效果好

***

### 第 80 页

**更进一步：机器学习与特征提取的结合（A Step Forward… when machine learning meets feature extraction）**

1.  **卷积神经网络（CNNs）的优势**

*   具备强大的复杂特征提取能力，能更细致地表达图像特征（have a strong ability to extract complex features that express the image in much more detail）

*   优势：可高效学习任务特定的特征（learn the task specific features with high efficiency）—— 传统特征（如 SIFT、HOG）是通用特征，而 CNN 可通过训练学习适配特定任务（如人脸检测、目标识别）的专属特征

1.  **全卷积网络（FCNs）成为主流技术的原因**

*   核心优势 1：可处理任意尺寸的输入图像，无需固定输入尺寸（传统 CNN 的全连接层需固定输入尺寸）

*   核心优势 2：通过上采样（upsampling）和跳跃连接（skip connections），可输出与输入尺寸一致的特征图，适用于图像分割、密集预测等任务

*   核心优势 3：参数共享机制（parameter sharing）减少模型参数数量，降低过拟合风险，提高训练效率和泛化能力

**专业术语解释**：

*   **卷积神经网络（Convolutional Neural Network, CNN）**：一种专门用于处理网格结构数据（如图像）的深度学习模型，通过卷积层、池化层、全连接层等结构提取数据的层次化特征，在计算机视觉领域取得突破性成果

*   **任务特定特征（Task-specific Features）**：针对特定任务（如人脸检测、车辆识别）优化的特征，相比通用特征，能更好地适配任务需求，提升模型性能

*   **全卷积网络（Fully Convolutional Network, FCN）**：去除传统 CNN 中全连接层，仅保留卷积层、池化层和上采样层的网络结构，可接受任意尺寸输入并输出对应尺寸的预测结果，是图像分割、语义理解的核心模型

*   **上采样（Upsampling）**：将低分辨率特征图放大到高分辨率的操作（如双线性插值、转置卷积），用于恢复图像的空间细节，实现密集预测

*   **跳跃连接（Skip Connections）**：将网络浅层的高分辨率特征图与深层的低分辨率特征图连接，融合浅层细节信息和深层语义信息，提升模型对细节的捕捉能力

*   **参数共享（Parameter Sharing）**：CNN 中卷积核的参数在整个图像上共享，无需为每个像素单独学习参数，大幅减少模型参数数量，提高训练效率和泛化能力

***

### 2-1.pdf 第 85 页（文档中未单独标注页码，为第 82 页延续）

*   **优点（Pros）**：继承 CNN 的优势

1.  对尺度、遮挡、变形、旋转等变化的鲁棒性更强（More robust to scale, occlusion, deformation, rotation, etc.）

2.  突破传统计算机视觉技术的局限，实现更复杂的任务（Pushed the limits of what was possible using traditional computer vision techniques），如实时目标检测、高精度图像分割

*   **缺点（Cons）**：继承学习方法的劣势

1.  耗时（Time consuming）：需大规模数据集用于网络训练，数据收集和标注成本高；模型训练过程需大量算力（如 GPU），训练周期长

2.  非通用性（Not universal）：模型拟合特定数据分布，推广到其他场景时可靠性低（fitting to a certain distribution, not reliable when generalizing to other scenario），如在 “自然场景人脸” 数据集上训练的模型，在 “医学影像人脸” 场景下性能大幅下降

**专业术语解释**：

*   **遮挡（Occlusion）**：目标被其他物体部分或完全遮挡的情况，是计算机视觉中的常见挑战，基于 CNN 的特征提取通过学习全局和局部特征，对部分遮挡有更好的鲁棒性

*   **数据分布（Data Distribution）**：数据在特征空间中的分布规律，若测试数据与训练数据的分布差异过大（分布偏移），模型泛化性能会显著下降

*   **泛化能力（Generalization Ability）**：模型对未见过的新数据的适应能力，泛化能力强的模型可在不同场景、不同数据集中保持稳定性能

***

### 2-1.pdf 第 86 页

**特征工程 I 总结（Summary of feature engineering-I）**

*   **特征定义**：特征是图像中物体的部分或模式，有助于识别物体（Features are parts or patterns of an object in an image that help to identify it.）

*   **常见特征类型**：包括角点、边缘、感兴趣区域点、脊线等属性（Features include properties like corners, edges, regions of interest points, ridges, etc.）

**核心特征提取方法**：

- **Harris 角点检测（Harris Corner Detection）**

*   核心：使用高斯窗口函数检测角点（Uses a Gaussian window function to detect corners）

*   优势：旋转不变性，计算简单；劣势：无尺度不变性

1.  **尺度不变特征变换（SIFT, Scale-Invariant Feature Transform）**

*   核心：具备尺度不变性（scale invariant）

*   参考文献：ICCV1999，谷歌学术引用量 23134 次

*   优势：尺度、旋转、亮度不变性，区分性强；劣势：计算复杂，专利保护

1.  **加速鲁棒特征（SURF, Speeded-Up Robust Features）**

*   核心：SIFT 的加速版本（a faster version of SIFT）

*   参考文献：ECCV2006，谷歌学术引用量 16556 次

*   优势：速度比 SIFT 快 3 倍，擅长处理模糊和旋转；劣势：对视角变化鲁棒性弱

1.  **加速分割测试特征（FAST, Features from Accelerated Segment Test）**

*   核心：比 SURF 更快的角点检测方法（a much faster corner detection method compared to SURF）

*   参考文献：ECCV2010，谷歌学术引用量 676 次

*   优势：检测速度极快（基于像素比较的贪心算法）；劣势：描述符简单，区分性弱

1.  **二进制鲁棒独立基本特征（BRIEF, Binary Robust Independent Elementary Features）**

*   核心：可与任意特征检测器配合使用的特征描述符，将浮点描述符转为二进制字符串，减少内存占用（a feature descriptor that can be used with any other feature detector. It reduces the memory usage by converting descriptors in floating point numbers to binary strings）

*   参考文献：ECCV2010，谷歌学术引用量 4688 次

*   优势：内存占用小（二进制字符串），匹配速度快（汉明距离计算）；劣势：对旋转和尺度变化敏感

1.  **定向 FAST 与旋转 BRIEF（ORB, Oriented FAST and Rotated BRIEF）**

*   核心：SIFT 和 SURF 有专利，该算法来自 OpenCV 实验室，是免费替代方案，使用 FAST 关键点检测器和 BRIEF 描述符（SIFT and SURF are patented and this algorithm from OpenCV labs is a free alternative to them, that uses FAST key point detector and BRIEF descriptor.）

*   参考文献：ICCV2011，谷歌学术引用量 10321 次

*   优势：免费开源，速度快，具备旋转不变性（定向 FAST）；劣势：对尺度变化鲁棒性弱于 SIFT

**后续预告**：讲座 3 将讨论用于处理降维数据的机器学习方法（Lecture 3 will discuss machine learning methods to deal with down-sampled data.）

**专业术语解释**：

*   **谷歌学术引用量（Google Scholar Citations）**：衡量学术论文影响力的指标，引用量越高，表明该研究被其他学者认可和应用的程度越高

*   **专利保护（Patented）**：SIFT 和 SURF 算法受专利保护，商业使用需获得授权，限制了其在部分商业产品中的应用

*   **OpenCV（Open Source Computer Vision Library）**：开源计算机视觉库，包含大量经典和前沿的计算机视觉算法（如 ORB、FAST），支持多语言（C++、Python）和多平台，是计算机视觉领域的常用工具

*   **汉明距离（Hamming Distance）**：衡量两个等长二进制字符串差异的指标，即对应位置不同字符的个数，计算速度远快于欧氏距离，适用于 BRIEF、ORB 等二进制描述符的匹配



### 2-2.pdf 第 1 页

媒体与认知（Media and Cognition）

讲座 2：特征工程 I（补充内容）（Lecture 2: Feature Engineering-I (Supplement)）

清华大学电子工程系（Dept. of EE, Tsinghua University）

方璐（Lu FANG）

### 2-2.pdf 第 2 页

**LIFT：学习型不变特征变换（LIFT: Learned Invariant Feature Transform, ECCV’16）**

*   **核心定位**：首个实现完整特征点处理流程（检测、方向估计、特征描述）的架构（The first architecture that implemented the full feature point handling pipeline (detection, orientation estimation, & feature description)）

*   **关键特性**：保持端到端可微性（Preserve the end-to-end differentiability）—— 整个流程的所有模块均可通过反向传播更新参数，支持联合训练，提升整体性能

**LIFT 流程（LIFT pipeline）**：

1.  分数图（SCORE MAP）：生成特征点候选区域的分数图，分数越高代表为特征点的概率越大

2.  检测（DET）：基于分数图，通过 softargmax 操作（将概率分布转为连续坐标）定位特征点位置

3.  裁剪（Crop）：以检测到的特征点为中心，裁剪出固定大小的局部图像块

4.  方向估计（ORI）：计算特征点的主方向

5.  旋转（Rot）：根据主方向旋转裁剪的图像块，实现旋转归一化

6.  描述符生成（DESC）：提取旋转后图像块的特征，生成描述符向量（description vector）

**性能优势**：在多种尺度下，匹配的特征数量比 SIFT 更多（More matched features than SIFT in a variety of scales!）

（图中展示 SIFT 和 LIFT 在不同尺度图像上的特征匹配结果，LIFT 的匹配线数量更多，匹配准确率更高）

**参考文献**：K. M. Yi, E. Trulls, V. Lepetit, and P. Fua. “LIFT: Learned Invariant Feature Transform”, ECCV, 2016.

**专业术语解释**：

*   **端到端可微性（End-to-End Differentiability）**：深度学习模型中，从输入到输出的所有操作均为可微函数，可通过反向传播算法计算每个参数的梯度，实现整个模型的联合训练，避免传统流程中各模块独立优化导致的性能瓶颈

*   **softargmax 操作（Softargmax Operation）**：将离散的概率分布（如分数图的像素分数）转换为连续的坐标值，通过对坐标加权求和（权重为像素分数的 softmax 值）实现，相比传统的.argmax（取最大值坐标），可保留更多位置信息且支持梯度反向传播

*   **ECCV（European Conference on Computer Vision）**：欧洲计算机视觉大会，与 ICCV、CVPR 并称为计算机视觉领域的三大顶会，每两年举办一次，发表该领域的前沿研究成果

### 2-2.pdf 第 3 页

**LIFT：学习型不变特征变换（LIFT: Learned Invariant Feature Transform, ECCV’16）**

*   **核心训练策略：渐进式学习（Progressive learning）**


    *   分阶段训练不同模块，逐步优化模型性能，避免初始训练时多模块耦合导致的优化困难
    
    *   流程示意：

1.  阶段 1（P1）：训练检测模块（DET）得到分数图 s1，通过 softargmax 定位特征点 x2，裁剪图像块 p1；训练方向估计模块（ORI）得到方向 θ1，旋转图像块；训练描述符模块（DESC）得到描述符 d1

2.  阶段 2（P2）：基于阶段 1 的结果，优化检测模块得到 s2，定位 x2，裁剪 p2；优化方向估计模块得到 θ2，旋转；优化描述符模块得到 d2

3.  阶段 3（P3）、阶段 4（P4）：重复上述过程，逐步迭代优化所有模块，直至模型收敛

**训练监督信号：SfM 监督（SfM Supervision）**

*   **SfM（Structure from Motion，运动恢复结构）**：通过多张不同视角的图像，估计相机姿态和场景三维结构的技术，可提供特征点的三维对应关系，作为 LIFT 的监督信号

*   **三大损失函数**：

1.  **描述符损失（Descriptor Loss, **** ****）**：

$ 
     \mathcal{L}_{desc}(p_{\theta}^{k}, p_{\theta}^{l}) = 
     \begin{cases} 
     \left\| h_{\rho}(p_{\theta}^{k}) - h_{\rho}(p_{\theta}^{l}) \right\|_{2} & \text{ï¼æ­£æ ·æ¬å¯¹ï¼åä¸ä¸ç»´ç¹çä¸åè§è§ç¹å¾ç¹ï¼} \\
     \max\left(0, C - \left\| h_{\rho}(p_{\theta}^{k}) - h_{\rho}(p_{\theta}^{l}) \right\|_{2}\right) & \text{ï¼è´æ ·æ¬å¯¹ï¼ä¸åä¸ç»´ç¹çç¹å¾ç¹ï¼}
     \end{cases}
      $

其中$  h_{\rho}(p)  $为特征点 p 的描述符，C 为 margin 参数，正样本对描述符距离越小越好，负样本对描述符距离需大于 C

1.  **方向损失（Orientation Loss, **** ****）**：

$ 
     \mathcal{L}_{orientation}(P^{1}, x^{1}, P^{2}, x^{2}) = \left\| h_{\rho}(G(P^{1}, x^{1})) - h_{\rho}(G(P^{2}, x^{2})) \right\|_{2}
      $

其中$  G(P, x)  $为根据方向旋转图像块的操作，使同一特征点在不同视角下的旋转后图像块描述符差异最小

1.  **检测器损失（Detector Loss, **** ****）**：

$ 
     \mathcal{L}_{detector}(P^{1}, P^{2}, P^{3}, P^{4}) = \gamma \mathcal{L}_{class}(P^{1}, P^{2}, P^{3}, P^{4}) + \mathcal{L}_{pair}(P^{1}, P^{2})
      $

其中$  \mathcal{L}_{class}  $为分类损失（区分特征点和非特征点），$  \mathcal{L}_{pair}  $为配对损失（确保同一三维点的特征点在不同图像中被检测到），$  \gamma  $为权重参数

**参考文献与代码**：K. M. Yi, E. Trulls, V. Lepetit, and P. Fua. “LIFT: Learned Invariant Feature Transform”, ECCV, 2016.（GitHub: [https://github.com/cvlab-epfl/LIFT](https://github.com/cvlab-epfl/LIFT)）

**专业术语解释**：

*   **渐进式学习（Progressive Learning）**：分阶段、逐步优化模型模块的训练策略，先训练基础模块（如检测），再基于基础模块的输出训练后续模块（如方向估计、描述符），降低多模块联合训练的复杂度

*   **运动恢复结构（Structure from Motion, SfM）**：计算机视觉中的经典技术，通过分析图像序列中特征点的对应关系，同时估计相机的内参、外参（姿态）和场景的三维结构，可提供真实的特征点对应关系，用于监督学习型特征提取模型

*   **正样本对 / 负样本对（Positive Pairs/Negative Pairs）**：正样本对指属于同一目标（如同一三维点）的两个样本，负样本对指属于不同目标的两个样本，通过对比损失（Contrastive Loss）优化正、负样本对的特征距离

*   **Margin 参数（Margin Parameter）**：在损失函数中用于控制正负样本对之间的距离边界，确保负样本对的特征距离足够大，正样本对的特征距离足够小，提升特征的区分性

### 2-2.pdf 第 4 页

**SuperPoint：自监督兴趣点检测与描述（SuperPoint: Self-Supervised Interest Point Detection and Description, CVPR 2018）**

*   **核心定位**：一种全卷积网络（FCN），可通过单次前向传播计算类 SIFT 的兴趣点位置和描述符（A FCN that computes SIFT-like interest point locations and descriptors in a single forward pass.）

*   **网络结构**：

1.  **编码器（Encoder）**：采用 VGG 风格的卷积网络（VGG-style encoder）提取图像特征，输出低分辨率的特征图（如输入图像的 1/8 尺寸）

2.  **兴趣点解码器（Interest Point Decoder）**：对编码器输出的特征图进行卷积操作，得到 65 通道的特征图（65 channels，其中 64 通道对应 8×8 的网格位置，1 通道对应置信度），通过 softmax 激活和 reshape 操作，将特征图恢复到输入图像尺寸（H×W），得到每个像素的兴趣点置信度

3.  **描述符解码器（Descriptor Decoder）**：对编码器输出的特征图进行卷积操作，得到 D 通道的描述符特征图（D 为描述符维度），通过双三次插值（Bi-Cubic Interpolate）将特征图恢复到输入图像尺寸，再进行 L2 归一化（L2 Norm），得到每个像素的描述符

（图中展示 SuperPoint 解码器结构：输入图像经过编码器得到 H/8×W/8 的特征图，兴趣点解码器通过卷积和 softmax/reshape 得到 H×W 的置信度图；描述符解码器通过卷积、双三次插值和 L2 归一化得到 H×W×D 的描述符图，标注 “Conv”（卷积）、“Softmax”（softmax 激活）、“Reshape”（重塑）、“Bi-Cubic Interpolate”（双三次插值）、“L2 Norm”（L2 归一化））

**参考文献**：DeTone, D., Malisiewicz, T., & Rabinovich, A. “Superpoint: Self-supervised interest point detection and description”, CVPR workshops, 2018.

**专业术语解释**：

*   **自监督学习（Self-Supervised Learning）**：一种无需人工标注数据的学习方式，通过从数据本身挖掘监督信号（如图像的几何变换、上下文关系）训练模型，SuperPoint 通过图像的单应性变换（Homography）生成伪标签，实现自监督训练

*   **VGG 风格编码器（VGG-style Encoder）**：采用类似 VGG 网络的卷积结构（小卷积核、多卷积层堆叠），逐步提取图像的浅层细节特征和深层语义特征，输出低分辨率、高语义的特征图

*   **双三次插值（Bi-Cubic Interpolation）**：一种图像缩放插值算法，通过周围 16 个像素的加权平均计算目标像素值，相比双线性插值，能更好地保留图像细节，用于将低分辨率特征图恢复到原始图像尺寸

*   **L2 归一化（L2 Normalization）**：对描述符向量进行归一化处理，使向量的 L2 范数为 1，消除特征向量的尺度差异，便于后续的相似度计算（如余弦相似度）

*   **CVPR（Computer Vision and Pattern Recognition）**：计算机视觉与模式识别会议，与 ICCV、ECCV 并称为计算机视觉领域的三大顶会，每年举办一次，发表该领域的前沿研究成果

### 2-2.pdf 第 5 页

**SuperPoint：自监督兴趣点检测与描述（SuperPoint: Self-Supervised Interest Point Detection and Description, CVPR 2018）**

*   **核心训练策略：半监督学习（Semi-supervised learning）**


    *   结合少量标注数据的预训练和大量无标注数据的自标注，降低对人工标注数据的依赖，提升模型泛化能力
    
    *   **三阶段训练流程**：

1.  **阶段 1：兴趣点预训练（Interest Point Pre-Training）**

*   输入：带标注兴趣点的图像（Labeled Interest Point Images）

*   过程：训练基础检测器（Base Detector），学习兴趣点的基本特征（如角点、边缘交点）

*   输出：初步训练好的基础检测器

1.  **阶段 2：兴趣点自标注（Interest Point Self-Labeling）**

*   输入：无标注图像（Unlabeled Image）、基础检测器

*   关键技术：单应性适配（Homographic Adaptation）—— 对无标注图像随机采样多个单应性变换（Homography），生成变换后的图像；将基础检测器应用于变换后的图像，得到兴趣点响应图；将响应图逆变换回原始图像坐标系，聚合所有变换的响应结果，得到兴趣点的伪真值（Pseudo-Ground Truth Interest Points）

*   输出：带有伪真值兴趣点的无标注图像

1.  **阶段 3：联合训练（Joint Training）**

*   输入：带伪真值的图像

*   过程：同时优化兴趣点损失（Interest Point Loss）和描述符损失（Descriptor Loss），更新 SuperPoint 的编码器和解码器

*   输出：最终训练好的 SuperPoint 模型

**单应性适配的变换类型（Homographic Adaptation Transformations）**：

*   对称随机裁剪（Symmetric Random Root Center Crop）

*   平移（Translation）

*   尺度变化（Scale）

*   平面内旋转（In-plane Rotation）

*   透视变形（Perspective Distort）

*   单应性裁剪（Homographic Crop）

（图中展示单应性适配流程：无标注图像→采样随机单应性变换→生成变换图像→应用基础检测器得到响应图→逆变换响应图→聚合得到兴趣点超集，标注 “Sample Random Homography”（采样随机单应性）、“Warp”（变换）、“Apply Base Detector”（应用基础检测器）、“Get Point Response”（得到点响应）、“Unwarp Heatmaps”（逆变换热力图）、“Aggregate Interest Point Superset”（聚合兴趣点超集））

**专业术语解释**：



*   **半监督学习（Semi-supervised Learning）**：结合少量标注数据（Labeled Data）和大量无标注数据（Unlabeled Data）进行模型训练的学习方式，既利用标注数据提供准确监督信号，又利用无标注数据扩大训练数据量，提升模型泛化能力

*   **单应性变换（Homography）**：描述两个平面之间投影关系的变换，可表示平移、旋转、缩放、透视等几何变换，数学上用 3×3 的单应性矩阵表示，SuperPoint 通过单应性变换生成图像的不同视角版本，用于挖掘自监督信号

*   **伪真值（Pseudo-Ground Truth）**：通过模型预测或数据变换生成的、替代人工标注的标签，虽然准确性可能低于人工标注，但数量多、获取成本低，可用于自监督或半监督训练

*   **单应性适配（Homographic Adaptation）**：通过对图像施加多个单应性变换，利用基础模型在变换图像上的预测结果，聚合得到原始图像的高质量伪标签的技术，是 SuperPoint 自标注阶段的核心

### 2-2.pdf 第 6 页

**LF-Net：从图像中学习局部特征（LF-Net: Learning Local Features from Images, NeurIPS 2018）**

*   **核心定位**：一种多尺度全卷积网络（multi-scale FCN），用于同时预测关键点的位置（locations）、尺度（scales）和方向（orientations）

*   **网络结构（LF-Net architecture）**：

1.  **检测器网络（Detector Network）**：

*   输出：尺度空间分数图（scale-space score map）和密集方向估计图（dense orientation estimates）

*   功能：通过分数图筛选关键点，通过方向估计图得到每个关键点的方向

1.  **空间变换网络（STN, Spatial Transformer Network）**：

*   功能：可微分采样器（differentiable sampler），根据检测器输出的关键点位置、尺度和方向，从原始图像中裁剪并归一化（旋转、缩放）局部图像块（patches）

1.  **描述符网络（Descriptor Network）**：

*   输入：STN 输出的归一化图像块

*   输出：每个关键点的描述符向量（descriptor $  D_i  $）

（图中展示 LF-Net 架构：输入图像→检测器网络生成分数图和方向图→筛选关键点→STN 裁剪归一化图像块→描述符网络生成描述符，标注 “Detector”（检测器）、“STN”（空间变换网络）、“Descriptor”（描述符）、“\[x\_i,y\_i,S\_i,θ\_i] patches”（带位置、尺度、方向的图像块））

**尺度不变关键点检测（Scale-invariant key-point detection）**：

*   **核心公式**：

$ 
  S = \sum_{n} \overline{h}^{n} \odot softmax_{n}\left( \overline{h}^{n} \right)
   $

其中$  \overline{h}^{n}  $为不同尺度下的特征图，$  \odot  $为逐元素乘法，$  softmax_n  $为对不同尺度的特征图进行 softmax 操作

*   **检测步骤**：

1.  将特征图按均匀间隔缩放 N 次（如从 1/2 到 2 倍尺度），得到$  h_1, h_2, ..., h_N  $（$  1 â¤ n < N  $）

2.  对每个缩放后的特征图，应用可学习滤波器（learnable filter）进行卷积

3.  对卷积结果进行加权平均（权重为 softmax 后的尺度概率），得到尺度空间分数图 S

4.  选取分数图 S 中前 K 个像素（Top-K pixels）作为检测到的关键点

**参考文献**：Ono, Y., Trulls, E., Fua, P., & Yi, K. M., “LF-Net: Learning local features from images”, NeurIPS 2018.

**专业术语解释**：

*   **多尺度全卷积网络（Multi-scale FCN）**：在网络中处理多个尺度的特征图，通过融合不同尺度的信息，实现对不同尺寸关键点的检测，LF-Net 通过多尺度特征图生成尺度空间分数图，确保尺度不变性

*   **空间变换网络（Spatial Transformer Network, STN）**：一种可微分的空间变换模块，能根据输入特征自适应地对图像进行裁剪、旋转、缩放等变换，输出归一化的特征，LF-Net 使用 STN 实现关键点局部图像块的几何归一化

*   **可微分采样器（Differentiable Sampler）**：STN 中的核心组件，通过双线性插值等可微操作，根据目标坐标从原始图像中采样像素值，确保变换过程可通过反向传播更新参数

*   **NeurIPS（Neural Information Processing Systems）**：神经信息处理系统大会，是机器学习和人工智能领域的顶会，每年举办一次，发表该领域的前沿研究成果

*   **Top-K 检测（Top-K Detection）**：在分数图中选取分数最高的前 K 个像素作为关键点，K 为预设参数，可根据任务需求调整，平衡关键点数量和检测精度

### 2-2.pdf 第 7 页

**LF-Net：从图像中学习局部特征（LF-Net: Learning Local Features from Images, NeurIPS 2018）**

*   **核心训练监督信号：利用稀疏深度和相对相机姿态线索（Exploit sparse depth and relative camera pose cues (from SfM) for image-level supervision）**


    *   通过运动恢复结构（SfM）获取图像对的稀疏深度信息（sparse depth）和相对相机姿态（relative camera pose），作为监督信号，无需人工标注关键点对应关系

*   **三大损失函数**：

1.  **图像级损失（Image-level Loss, **** ****）**：

$ 
     \mathcal{L}_{im}(S_i, S_j) = \left| S_i - g(w(S_j)) \right|^2
      $

其中$  S_i  $和$  S_j  $为图像 i 和图像 j 的分数图，$  w(\cdot)  $为根据 SfM 结果将图像 j 的关键点投影到图像 i 的 warp 操作，$  g(\cdot)  $为插值函数，确保投影后的分数图与图像 i 的分数图差异最小

1.  **配对损失（Pair Loss, **** ****）**：

$ 
     \mathcal{L}_{pair}(D_i^k, \hat{D}_j^k) = \sum_{k} \left| D_i^k - \hat{D}_j^k \right|^2
      $

其中$  D_i^k  $为图像 i 中第 k 个关键点的描述符，$  \hat{D}_j^k  $为图像 j 中与第 k 个关键点对应的投影关键点的描述符，使对应关键点的描述符差异最小

1.  **三角化损失（Triangulation Loss, **** ****）**：

$ 
     \mathcal{L}_{tri}(D_i^k, \hat{D}_j^k, \hat{D}_j^{k'}) = \sum_{k} \max\left(0, \left| D_i^k - \hat{D}_j^k \right|^2 - \left| D_i^k - \hat{D}_j^{k'} \right|^2 + C\right)
      $

其中$  \hat{D}_j^{k'}  $为图像 j 中与第 k 个关键点不对应的其他关键点的描述符，C 为 margin 参数，确保对应关键点的描述符距离小于非对应关键点的距离

1.  **几何损失（Geometric Loss, **** ****）**：

$ 
     \mathcal{X}_{geom}(s_i^k, \theta_i^k, \hat{s}_j^k, \hat{\theta}_j^k) = \lambda_{ori} \sum_{k} \left| \theta_i^k - \hat{\theta}_j^k \right|^2 + \lambda_{scale} \sum_{k} \left| s_i^k - \hat{s}_j^k \right|^2
      $

其中$  s_i^k  $和$  \theta_i^k  $为图像 i 中第 k 个关键点的尺度和方向，$  \hat{s}_j^k  $和$  \hat{\theta}_j^k  $为图像 j 中对应关键点的尺度和方向，$  \lambda_{ori}  $和$  \lambda_{scale}  $为权重参数，使对应关键点的尺度和方向差异最小

（图中展示损失函数计算流程：检测器生成分数图→分数图清洁（Det. cleaning）→warp 操作（根据 SfM 结果将图像 1 的关键点投影到图像 2）→描述符网络生成描述符→计算图像级损失$  \mathcal{L}_{im}  $、配对损失$  \mathcal{L}_{pair}  $、三角化损失$  \mathcal{L}_{tri}  $和几何损失$  \mathcal{X}_{geom}  $，标注 “Gradient Flow”（梯度流）、“O.G”（原始图像）、“warp”（投影））

**“warp” 操作说明**：根据 SfM 结果（三维位置和相对相机姿态），将图像 1 中的关键点投影到图像 2 中（project the key-point on image 1 to image 2 based on SfM results (3D position and relative camera pose)）

**专业术语解释**：

*   **稀疏深度信息（Sparse Depth）**：通过 SfM 得到的场景中少量关键点的三维深度信息（与相机的距离），不同于稠密深度（如 RGB-D 相机获取的每个像素的深度），稀疏深度仅包含关键位置的深度

*   **相对相机姿态（Relative Camera Pose）**：两个相机之间的位置和姿态关系，通常用旋转矩阵（Rotation Matrix）和平移向量（Translation Vector）表示，用于将一个相机坐标系下的点投影到另一个相机坐标系

*   **三角化损失（Triangulation Loss）**：基于三角化原理（通过两个相机的投影关系恢复三维点）设计的损失函数，确保对应关键点的描述符距离小于非对应关键点，提升描述符的区分性

*   **几何损失（Geometric Loss）**：用于优化关键点的几何属性（尺度、方向）的损失函数，使同一三维点在不同图像中的关键点几何属性一致，提升关键点的几何一致性

### 2-2.pdf 第 8 页

**LoFTR：基于 Transformer 的无检测器局部特征匹配（LoFTR: Detector-Free Local Feature Matching with Transformers, CVPR’21）**

*   **传统流程的问题（Classical workflow: feature detection, feature description, and feature matching）**：


    *   传统特征匹配分为 “特征检测→特征描述→特征匹配” 三个独立步骤
    
    *   核心问题：每个步骤都对低纹理、重复图案、视角变化、光照变化、运动模糊等情况敏感（Every process is sensitive to poor texture, repetitive patterns, viewpoint change, illumination variation, motion blur, etc.），一步出错会导致整体匹配失败

*   **核心创新：统一流水线（Unified Pipeline）**：


    *   问题：是否可能设计一个利用全局上下文（global context）的统一流水线？（Is it possible for a unified pipeline that makes use of global context?）
    
    *   答案：LoFTR 去除独立的特征检测步骤，直接在全局上下文下实现端到端的特征匹配，提升对复杂场景的鲁棒性

**参考文献**：（标注 “SuperClue ar1197”“err R: 9.8°, err t:31.5”，为性能对比标注）

**专业术语解释**：

*   **无检测器（Detector-Free）**：不同于传统方法先检测关键点再匹配，LoFTR 直接在图像的密集像素上进行匹配，无需单独的关键点检测步骤，避免了检测步骤对低纹理等场景的敏感性

*   **全局上下文（Global Context）**：图像的整体信息（如全局纹理、结构、语义），传统方法的局部特征提取忽略全局上下文，LoFTR 通过 Transformer 的注意力机制捕捉全局上下文，提升匹配的准确性

*   **运动模糊（Motion Blur）**：由于相机或物体运动导致的图像模糊，会破坏局部特征的完整性，传统特征检测方法对运动模糊敏感，LoFTR 的全局匹配方式可部分缓解该问题

*   **低纹理区域（Poor Texture Region）**：图像中灰度或纹理变化平缓的区域（如墙面、天空），传统方法难以在该区域检测到稳定的关键点，LoFTR 通过密集匹配可在低纹理区域寻找潜在对应关系

### 2-2.pdf 第 9 页

**LoFTR：基于 Transformer 的无检测器局部特征匹配（LoFTR: Detector-Free Local Feature Matching with Transformers, CVPR’21）**

*   **核心流程**：

1.  **先在粗粒度级别建立像素级密集匹配，再在细粒度级别优化得到高质量稀疏匹配（First establish pixel-wise dense matches at a coarse level and later refine good sparse matches at a fine level）**

2.  **通过交错的多个自注意力层和交叉注意力层，学习密集排列的全局一致匹配先验（Learn the densely arranged globally-consented matching priors by interleaving multiple self and cross attention layers）**

**四模块网络结构**：

1.  **局部特征 CNN（Local Feature CNN）**：

*   输入：一对图像 A 和 B

*   输出：1/8 分辨率的特征图$  F_A  $和$  F_B  $（通道数为$  N_f  $），提取图像的局部特征

1.  **粗粒度局部特征 Transformer 模块（Coarse-Level Local Feature Transform LoFTR Module）**：

*   输入：$  F_A  $和$  F_B  $

*   操作：


    *   自注意力层（Self-Attention Layer）：捕捉单幅图像内的全局上下文关系
    
    *   交叉注意力层（Cross-Attention Layer）：捕捉两幅图像间的特征对应关系
    
    *   交错堆叠多个自注意力和交叉注意力层，学习匹配先验

*   输出：经过注意力增强的粗粒度特征图

1.  **匹配模块（Matching Module）**：

*   输入：增强后的粗粒度特征图

*   操作：


    *   计算特征相关性矩阵（correlation matrix）
    
    *   对相关性矩阵进行 softmax 操作，得到匹配置信度矩阵$  P_{c}  $
    
    *   根据置信度矩阵选取粗粒度匹配对$  M_c = \{(i,j)\}  $（i 为图像 A 的像素索引，j 为图像 B 的像素索引）

*   输出：粗粒度匹配对和对应的置信度

1.  **粗到细模块（Coarse-to-Fine Module）**：

*   输入：粗粒度匹配对、原始图像特征

*   操作：


    *   以粗粒度匹配对为中心，在 1/2 分辨率的细粒度特征图上裁剪局部区域（cropping on F）
    
    *   对局部区域进行精细匹配，优化匹配位置

*   输出：最终的高质量稀疏匹配对

（图中展示网络结构细节：输入图像→局部特征 CNN 得到 1/8 分辨率特征图→LoFTR 模块（自注意力 + 交叉注意力）→匹配模块计算置信度矩阵→粗到细模块裁剪局部区域并精细匹配，标注 “positional encoding”（位置编码）、“FA, B”（图像 A 和 B 的特征图）、“confidence matrix Pc”（置信度矩阵）、“My={(i,j)}”（匹配对）、“cropping on F”（特征图裁剪））

**专业术语解释**：

*   **密集匹配（Dense Matching）**：在图像的每个像素或密集网格点上寻找对应关系，不同于传统的稀疏匹配（仅在关键点上匹配），可提供更完整的对应关系，适用于图像拼接、立体匹配等任务

*   **稀疏匹配（Sparse Matching）**：仅在图像的少量关键点上寻找对应关系，计算量小，适用于实时性要求高的任务（如目标跟踪），LoFTR 先密集匹配再筛选得到稀疏匹配，平衡完整性和效率

*   **自注意力层（Self-Attention Layer）**：Transformer 中的核心模块，通过计算特征图内每个位置与其他位置的注意力权重，捕捉单幅图像的全局上下文信息（如远处像素的纹理关联）

*   **交叉注意力层（Cross-Attention Layer）**：Transformer 中的模块，计算两幅图像特征图之间的注意力权重，捕捉图像间的特征对应关系，是 LoFTR 学习匹配先验的核心

*   **相关性矩阵（Correlation Matrix）**：衡量图像 A 和图像 B 特征图中每个像素对相似性的矩阵，元素值越大，代表两个像素的特征越相似，越可能是对应点

*   **位置编码（Positional Encoding）**：Transformer 不包含位置信息，通过位置编码将像素的空间位置信息注入特征向量，确保模型能区分不同位置的像素

### 2-2.pdf 第 10 页

**LoFTR：基于 Transformer 的无检测器局部特征匹配（LoFTR: Detector-Free Local Feature Matching with Transformers, CVPR’21）**

*   **核心流程重复强调**：

1.  先在粗粒度级别建立像素级密集匹配，再在细粒度级别优化得到高质量稀疏匹配（First establish pixel-wise dense matches at a coarse level and later refine good sparse matches at a fine level）

2.  通过交错的多个自注意力层和交叉注意力层，学习密集排列的全局一致匹配先验（Learn the densely arranged globally-consented matching priors by interleaving multiple self and cross attention layers）

（图中展示 LoFTR 的匹配效果示意图：左侧为输入的一对图像，右侧为匹配结果，用彩色线条连接对应点，标注 “105”“SSOI A P 9ral” 等细节，直观展示匹配的准确性）

**专业术语解释**：

*   **全局一致匹配先验（Globally-Consented Matching Priors）**：模型学习到的、符合图像全局几何和纹理一致性的匹配规则，确保匹配对不会出现全局几何矛盾（如明显的错位），提升匹配的整体一致性

*   **粗粒度 / 细粒度（Coarse-Grained/Fine-Grained）**：粗粒度指低分辨率、大范围的匹配，计算量小，用于快速筛选潜在对应关系；细粒度指高分辨率、局部范围的匹配，计算量较大，用于优化匹配位置的精度，两者结合平衡速度和精度

### 2-2.pdf 第 11 页

**LoFTR：基于 Transformer 的无检测器局部特征匹配（LoFTR: Detector-Free Local Feature Matching with Transformers, CVPR’21）**

*   **性能优势**：匹配成功的数量超过之前的最先进方法（More successful matches than prior state-of-the-arts!）

*   **对比方法**：

1.  SuperPoint + SuperGlue：基于 CNN 的特征检测 + Transformer 匹配

2.  DRC-Net：传统特征匹配方法的改进版本

3.  LoFTR：无检测器的 Transformer 匹配方法

（图中展示三种方法在不同场景（如低纹理、重复图案、视角变化）下的匹配结果对比，LoFTR 的匹配线条数量更多、错误匹配更少，标注 “1oopino”“1oobnl” 等场景标识）

**关键性能提升原因**：

1.  无检测器设计：避免了低纹理区域关键点检测失败的问题，可在全图像像素上寻找匹配

2.  全局上下文捕捉：Transformer 的注意力机制能利用全局纹理和结构信息，减少重复图案导致的错误匹配

3.  粗到细优化：先快速筛选再精细优化，兼顾匹配数量和精度

**专业术语解释**：

*   最先进方法（State-of-the-Art, SOTA）：当前在某一任务上性能最好的方法，LoFTR 在特征匹配任务上超越了之前的 SOTA 方法

*   SuperGlue：一种基于 Transformer 的特征匹配模型，需与 SuperPoint 等检测器配合使用，LoFTR 将检测和匹配整合为端到端流程，性能更优

*   DRC-Net（Dense Reconstruction and Correspondence Network）：一种传统的密集匹配网络，依赖手工设计的特征或早期 CNN 特征，性能弱于基于 Transformer 的方法

### 2-2.pdf 第 12 页

**高级特征：从结构到语义（High-level Features: From Structure to Semantic）**

*   **特征层级提升**：从传统的结构特征（如角点、边缘、纹理），逐步发展到语义特征（如物体类别、部件属性）

*   **示例任务**：

1.  **目标检测（Detection）**：识别图像中的目标并框出位置（如 “Building”（建筑））

2.  **图像分割（Segmentation）**：将图像中的像素分类到对应的语义类别（如 “Building”“Road”“Tree”）

3.  **视觉问答（Visual Question Answering, VQA）**：根据图像内容回答自然语言问题

**GPT-4 视觉输入示例（Example of GPT-4 visual input）**：

*   **用户问题（User）**：这张图有趣的地方在哪里？逐面板描述。（What is funny about this image? Describe it panel by panel.）

*   **GPT-4 回答（GPT-4）**：

1.  图像展示了一个 “闪电数据线”（Lightning Cable）适配器的包装，包含三个面板

2.  面板 1（Panel 1）：一部智能手机的充电口插着一个 VGA 接口（一种用于电脑显示器的大型蓝色 15 针接口）

3.  面板 2（Panel 2）：“闪电数据线” 适配器的包装，上面印有 VGA 接口的图片

4.  面板 3（Panel 3）：VGA 接口的特写，末端有一个小型闪电接口（用于 iPhone 和其他苹果设备充电）

5.  笑点（Humor）：将大型、过时的 VGA 接口插入小型、现代的智能手机充电口，这种荒谬感产生了幽默效果

**来源（Source）**：[http://www.reddit.com/r/hmmm/comments/ubab5v/hmm/](http://www.reddit.com/r/hmmm/comments/ubab5v/hmm/)

**专业术语解释**：

*   **结构特征（Structural Features）**：描述图像几何结构和局部形态的特征，如角点、边缘、纹理，不包含语义信息

*   **语义特征（Semantic Features）**：包含图像内容语义信息的特征，如 “这是建筑”“这是猫的眼睛”，能直接反映图像的内容含义

*   **视觉问答（Visual Question Answering, VQA）**：结合计算机视觉和自然语言处理的任务，模型需理解图像内容和自然语言问题，生成准确的回答，是图像高级语义理解的重要应用

*   **GPT-4（Generative Pre-trained Transformer 4）**：OpenAI 开发的大型语言模型，具备视觉输入能力，可处理图像和文本的多模态任务，如视觉问答、图像描述

### 2-2.pdf 第 13 页

**自监督视觉 Transformer 的新兴特性（Emerging Properties in Self-Supervised Vision Transformers, CVPR’21）**

*   **DINO：无标签蒸馏（DINO: Distillation with no labels）**


    *   **核心思想**：通过 “学生 - 教师” 网络的非对称学习（Asymmetric learning of ‘student’ and ‘teacher’ network），让学生网络匹配教师网络的输出分布
    
    *   **流程**：

1.  教师网络（Teacher Network）：参数通过学生网络参数的指数移动平均（Exponential Moving Average, EMA）更新，保持稳定

2.  学生网络（Student Network）：参数通过反向传播更新，学习匹配教师网络的输出

3.  输入：对同一图像生成两个不同的增强视图（如裁剪、旋转、颜色抖动），分别输入学生和教师网络

4.  损失：最小化学生网络和教师网络输出分布的 KL 散度（KL Divergence），实现无监督特征学习

*   **自监督预训练的优势**：在视觉 Transformer（ViT）特征上进行自监督预训练，可显著提升语义理解能力（Self-supervised pretraining on ViT features boosts semantics）

**DINO 网络结构**：

1.  输入：图像补丁（Patch）+ 位置编码（Position Embedding）

2.  线性投影（Linear Projection of Flattened Patches）：将图像补丁展平并投影到高维特征空间

3.  Transformer 编码器（Transformer Encoder）：提取图像的全局特征

4.  MLP 头（MLP Head）：将 Transformer 输出的特征映射到最终的输出分布

（图中展示 DINO 结构：输入图像→补丁划分和位置编码→线性投影→Transformer 编码器→MLP 头→输出分布，标注 “Patch + Position Embedding”“Linear Projection of Flattened Patches”“Transformer Encoder”“MLP Head”）

**专业术语解释**：

*   **视觉 Transformer（Vision Transformer, ViT）**：将 Transformer 架构应用于计算机视觉任务的模型，通过将图像划分为补丁（Patch），视为序列数据输入 Transformer，能有效捕捉图像的全局特征

*   **知识蒸馏（Knowledge Distillation）**：将复杂模型（教师模型）的知识迁移到简单模型（学生模型）的技术，DINO 是无监督知识蒸馏，无需人工标注数据

*   **非对称学习（Asymmetric Learning）**：学生和教师网络的结构或更新方式不同，DINO 中教师网络参数通过 EMA 更新，学生网络通过反向传播更新，确保教师网络的稳定性

*   **指数移动平均（Exponential Moving Average, EMA）**：一种参数更新策略，教师网络的参数 = 动量系数 × 教师网络参数 + (1 - 动量系数)× 学生网络参数，使教师网络参数更新平滑、稳定

*   **KL 散度（KL Divergence）**：衡量两个概率分布差异的指标，DINO 中用于衡量学生和教师网络输出分布的差异，KL 散度越小，分布越相似

### 2-2.pdf 第 14 页

**自监督视觉 Transformer 的新兴特性（Emerging Properties in Self-Supervised Vision Transformers, CVPR’21）**

*   **核心特性**：

1.  **语义表达能力**：自监督 ViT 特征能明确传递图像的布局和语义信息（Self-supervised ViT features explicitly convey layouts and semantics）

*   示例：通过可视化 ViT 的注意力图，可观察到模型自动关注图像中的语义关键区域（如物体的主要部件）

1.  **高分类准确率**：在 ImageNet 数据集上，仅通过 k 近邻（k-NN）分类（无需任何微调），即可达到 78.3% 的 top-1 准确率（78.3% top-1 accuracy on ImageNet with k-NN, without any finetuning）

*   传统自监督方法（如 MoCo、SimCLR）的 k-NN 准确率通常低于 75%，DINO 的 ViT 特征大幅提升了自监督特征的分类性能

（图中展示 ViT 特征的可视化结果：左侧为原始图像，右侧为注意力图，注意力图中亮度高的区域对应图像的语义关键部分（如物体的轮廓、部件），直观展示 ViT 特征的语义捕捉能力）

**专业术语解释**：

*   **注意力图（Attention Map）**：可视化 Transformer 注意力权重的图表，亮度高的区域表示模型在该位置分配了更高的注意力权重，反映模型对图像不同区域的关注程度，可用于分析模型的语义理解能力

*   **k 近邻分类（k-Nearest Neighbors, k-NN）**：一种简单的分类方法，将测试样本的特征与训练样本的特征进行相似度计算，选取最相似的 k 个训练样本，以多数样本的类别作为测试样本的类别，无需训练分类器

*   **ImageNet 数据集**：大型图像分类数据集，包含 1000 个类别、120 万张训练图像和 5 万张验证图像，是评估图像分类算法性能的常用基准

*   **top-1 准确率（Top-1 Accuracy）**：分类任务的评估指标，预测结果中概率最高的类别与真实类别一致的样本比例，是衡量分类性能的核心指标

*   **微调（Finetuning）**：在预训练模型的基础上，使用目标任务的数据集更新部分或全部参数，使模型适配目标任务，DINO 的 ViT 特征无需微调即可通过 k-NN 达到高准确率，表明其特征的通用性和语义表达能力强