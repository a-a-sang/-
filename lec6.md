# 卷积神经网络（CNN）发展历程及基本结构

### 第 1 页：封面

**媒体与认知（Media and Cognition）**

**第 6 讲：卷积神经网络（一）（Lecture 6: Convolutional Neural Networks - I）**

清华大学电子工程系（Dept. of EE, Tsinghua University）

方璐（Lu FANG）

### 第 2 页：视觉数据处理

视觉是人类获取信息的主要途径，约 70% 的信息来自视觉。视觉数据处理的主要任务包括：



*   **分类（Classification）**：判断图像属于哪一类，比如区分图像中的动物是猫还是狗。

*   **回归（Regression）**：预测图像中目标的位置、姿态等连续值，像预测车辆在图像中的具体坐标位置。

*   **识别（Recognition）**：识别图像中的对象，例如人脸识别技术识别出照片中的人物身份。

*   **语义分割（Semantic Segmentation）**：对图像中每个像素进行分类，将图像中的道路、建筑、天空等不同类别区域在像素级别区分开来。

*   **目标检测（Object Detection）**：找出图像中所有目标并分类，在一幅街景图像中检测出汽车、行人、交通信号灯等目标。

*   **实例分割（Instance Segmentation）**：对每个实例进行像素级分割，不仅区分出不同类别的目标，还能将同一类别的不同实例（如多个人）在像素层面分开。

*   **图像生成（Image Generation）**：从无到有生成图像，比如根据文本描述生成相应的图像场景。

*   **风格迁移（Style Transfer）**：将一幅图像的内容与另一幅的风格结合，把梵高画作的风格迁移到普通风景照片上。

*   **超分辨率（Super - resolution）**：提高图像分辨率，将低分辨率图像转换为高分辨率图像。

**核心思想**：充分利用图像中的空间结构信息！

### 第 3 页：多层感知机与卷积的对比



*   **回顾：多层感知机（Multilayer Perceptron，MLP）**


    *   由全连接层（Fully Connected Layer）组成，每层神经元与上一层所有神经元连接。
    
    *   输入图像需拉伸为一维向量，**丢失空间信息**。例如，将 32×32×3 的 RGB 图像拉伸为 3072 维向量后，图像原本的空间结构被破坏，像素之间的相邻关系等空间信息无法体现。之后，这个向量会经过权重矩阵进行计算，最终得到输出。

*   **卷积（Convolution）**


    *   保留并利用空间结构和层级模式，通过滑动窗口计算局部区域的点积。
    
    *   权重共享，减少参数量。比如，在提取图像边缘特征时，同一个卷积核在图像不同位置滑动应用，不需要为每个位置单独学习一套权重。
    
    *   能够捕捉局部特征并组合成更复杂的全局特征，先识别出图像中的边缘、角点等局部特征，再逐步组合这些特征来识别更复杂的物体形状。

### 第 4 页：卷积运算示例

卷积通过滑动核（filter，也叫滤波器）与图像局部区域进行计算：



1.  将核覆盖在图像的一个区域上。

2.  计算对应元素的乘积之和。例如，对于一个 3×3 的核和图像上对应的 3×3 区域，将核中每个元素与图像区域中对应位置的元素相乘，然后把这些乘积相加。

3.  将结果作为输出图像对应位置的像素值。

4.  按步长（stride）移动核，重复上述过程。步长决定了核每次移动的距离，如果步长为 1，核每次移动一个像素位置；如果步长为 2，核每次移动两个像素位置。

这种操作能够提取图像的局部特征，如边缘、纹理等。比如，使用特定的核可以检测出图像中的水平边缘、垂直边缘或者特定方向的纹理。

### 第 5 页：CNN 基本结构

卷积神经网络（Convolutional Neural Networks，CNN）通常由以下层交替堆叠而成：



*   **卷积层（Conv Layer，即 Convolution Layer 的缩写）**：提取特征，通过卷积核对输入数据进行卷积操作，获取图像中的各种特征，像边缘、颜色、纹理等。

*   **池化层（Pool Layer，即 Pooling Layer 的缩写）**：降采样，增强不变性。它可以减少数据的维度，同时使特征在一定程度上具有平移、旋转等不变性。

*   **全连接层（FC Layer，即 Fully Connected Layer 的缩写）**：分类决策，将前面提取的特征进行综合，最终做出分类或回归的决策。

典型结构：Conv → Pool → Conv → Pool → Conv → Conv → Conv → Pool → FC → FC → FC

**发展趋势**：



*   使用更小的卷积核（如 3×3），这样可以在保持感受野大小的同时，减少参数数量，提高计算效率，并且通过多层小卷积核的堆叠可以增加网络的非线性表达能力。

*   网络深度不断增加，随着网络深度的加深，能够学习到更复杂、更抽象的特征，从而提升模型的性能。

*   逐渐减少或去除池化层和全连接层，走向全卷积网络（Fully - Convolutional Networks）。全卷积网络可以直接对输入图像进行卷积操作，避免了池化层和全连接层带来的信息丢失，同时可以输出与输入图像尺寸相关的结果，更适用于一些需要精确空间信息的任务，如语义分割。

**经典模型**：LeNet、AlexNet、VGG 等。

### 第 6 页：CNN 发展时间线



*   **认知神经科学基础**


    *   **1959 年**：Hubel 和 Wiesel 发现视觉皮层中的简单细胞（Simple Cells）和复杂细胞（Complex Cells）。简单细胞对特定方向和位置的边缘有强烈响应，复杂细胞则对特定方向的边缘响应，但对位置的要求相对宽松，具有一定的位置不变性。
    
    *   **1981 年**：因发现视觉系统的信息处理机制获诺贝尔生理学或医学奖，他们的研究揭示了视觉信息在大脑中的处理方式，为后续的计算机视觉和神经网络研究提供了重要的生物学基础。
    
    *   **1982 年**：David Marr 发表 “Vision”，奠定计算机视觉基础。他在书中提出了一种计算理论框架，试图解释人类视觉系统如何从图像中提取信息，对计算机视觉领域的发展产生了深远影响。

*   **早期神经网络模型**


    *   **1980 年**：Fukushima 提出 Neocognitron（新认知机），CNN 的雏形。它模拟了视觉系统的层级结构，是第一个受生物视觉系统启发的卷积神经网络模型。
    
    *   **1989 年**：LeCun 首次用反向传播（Backpropagation）训练 CNN 识别手写数字。反向传播算法是一种用于训练神经网络的高效算法，它通过计算误差的梯度并反向传播来更新网络的权重，使得神经网络能够不断学习和优化。

*   **深度学习黄金时代**


    *   **2007 年**：Poggio 提出 HMAX 认知计算模型，该模型模拟了灵长类动物视觉系统的处理过程，为理解和构建智能视觉系统提供了新的思路。
    
    *   **2012 年**：AlexNet 在 ImageNet 上取得突破，CNN 开始普及。AlexNet 在 ImageNet 大规模视觉识别挑战赛中取得了优异成绩，大幅降低了错误率，引起了学术界和工业界对深度学习和 CNN 的广泛关注。
    
    *   **2015 年**：ResNet 通过残差连接（Residual Connection）实现超深网络训练，解决了深层网络训练中的梯度消失（Vanishing Gradient）和梯度爆炸（Exploding Gradient）问题，使得训练非常深的神经网络成为可能。

*   **代表人物**


    *   **Kunihiko Fukushima**：Neocognitron 提出者，为 CNN 的发展奠定了早期基础。
    
    *   **Yann LeCun**：CNN 与反向传播结合的重要推动者，他的工作使得 CNN 能够有效训练，在手写数字识别等任务中取得成功。
    
    *   **Alex Krizhevsky**：AlexNet 主要作者，其工作推动了深度学习在计算机视觉领域的快速发展。
    
    *   **Kaiming He**：ResNet、Faster R - CNN 等重要贡献者，他提出的残差连接等技术对深度学习的发展产生了深远影响。

### 第 7 页：视觉皮层的感受野

\*\* 感受野（Receptive Field）\*\* 是指视觉空间中能够影响单个神经元放电的区域。



*   **视觉皮层的层级结构**


    *   **简单细胞（Simple Cells）**：对特定方向和位置的边缘响应，它们只对输入图像中特定方向（如水平、垂直或倾斜）且位于其感受野内的边缘有强烈的神经冲动反应。
    
    *   **复杂细胞（Complex Cells）**：对特定方向的边缘响应，但具有位置不变性。与简单细胞相比，复杂细胞对边缘的位置要求不那么严格，即使边缘在一定范围内移动，复杂细胞仍能产生响应。
    
    *   **超复杂细胞（Hypercomplex Cells）**：对更复杂的形状特征响应，能够识别更复杂的图形模式，比如对特定的角、曲线组合等有反应。

*   **大脑视觉处理特点**


    *   视网膜→LGN（外侧膝状体，Lateral Geniculate Nucleus）→V1（初级视觉皮层，Primary Visual Cortex）→V2→V4→IT（下颞叶，Inferior Temporal Gyrus）等层级结构，视觉信息在这些层级中逐步处理和分析。
    
    *   邻近皮层细胞代表视野中邻近区域（拓扑映射，Topographic Mapping），这种映射关系使得视觉信息在大脑中的处理具有空间上的一致性。
    
    *   随着层级提高，感受野增大，特征抽象程度提高。在低级皮层（如 V1），神经元感受野较小，只能检测到简单的边缘等低级特征；而在高级皮层（如 IT），神经元感受野较大，可以对复杂的物体形状、类别等高级特征进行识别。

这一神经科学发现直接启发了 CNN 的层级结构设计。

### 第 8 页：Neocognitron - CNN 的起源

\*\*Neocognitron（新认知机）\*\* 由 Fukushima 于 1980 年提出，是第一个受生物视觉系统启发的卷积神经网络模型。



*   **核心特点**


    *   模拟视觉系统的层级结构，模仿大脑视觉皮层从低级到高级逐步处理视觉信息的过程。
    
    *   交替使用 “简单细胞层”（S 层）和 “复杂细胞层”（C 层）。
    
    *   S 层：执行卷积操作，提取局部特征，通过卷积核对输入数据进行卷积，获取图像中的局部信息，如边缘、纹理等。
    
    *   C 层：执行池化操作，提供位置不变性，通过池化操作（如最大池化、平均池化）减少数据维度，同时使特征在一定程度上对位置变化不敏感。

*   **结构**：输入层 → S1 层 → C1 层 → S2 层 → C2 层 → ... → 输出层

*   **局限性**


    *   缺乏有效的训练算法，当时没有像现在这样成熟的反向传播等训练算法，使得模型的训练效率和效果受到限制。
    
    *   主要依赖手工设计的特征提取器，特征提取器的设计需要人工经验，缺乏自适应性。

尽管如此，Neocognitron 奠定了 CNN 的基本框架：卷积 + 池化的交替结构。

### 第 9 页：用梯度下降训练 CNN

**LeNet - 5**（LeCun, 1989）是第一个成功应用反向传播训练的 CNN，用于手写数字识别。



*   **数据集**


    *   MNIST 手写数字数据集，是一个广泛用于训练和测试机器学习算法的手写数字图像数据集。
    
    *   60,000 张训练图像，10,000 张测试图像。
    
    *   每张图像为 32×32 像素的灰度图，灰度图只有一个通道，像素值表示灰度强度，范围通常是 0 - 255。

*   **网络结构**


    *   7 层结构：2 个卷积层、2 个池化层、3 个全连接层。
    
    *   卷积核大小：5×5，步长 1。卷积核大小决定了每次卷积操作所覆盖的图像区域大小，步长决定了卷积核在图像上滑动的距离。
    
    *   池化核大小：2×2，步长 2。池化核大小和步长影响池化操作对特征图的降采样程度。

*   **应用**


    *   被银行用于识别支票上手写的数字，通过对大量手写数字图像的学习，LeNet - 5 能够准确识别出支票上的数字，提高了银行处理业务的效率和准确性。
    
    *   受当时计算资源限制，网络规模较小。在当时的计算条件下，大规模的神经网络训练成本高昂，所以 LeNet - 5 的网络规模相对较小。

*   **贡献**


    *   证明了反向传播可以有效训练深度卷积网络，为后续深度学习的发展提供了重要的训练方法。
    
    *   确立了 CNN 的基本架构模式：Conv→Pool→Conv→Pool→FC→FC→FC。这种架构模式成为后来许多 CNN 模型的基础。

### 第 10 页：CNN 与深度学习的结合

2012 年以来，随着计算能力的提升和大数据的出现，CNN 与深度学习结合取得了突破性进展。



*   **代表性模型**


    *   **AlexNet**（2012）：ImageNet 竞赛冠军，深度 CNN 的里程碑。在 ImageNet 大规模视觉识别挑战赛中，AlexNet 以显著优势战胜其他方法，展示了深度卷积神经网络在图像识别任务中的强大能力。
    
    *   **VGGNet**（2014）：使用更小的卷积核（3×3）和更深的网络。通过堆叠多个 3×3 的卷积层来替代大尺寸卷积核，在减少参数数量的同时增加了网络的深度和非线性表达能力。
    
    *   **GoogLeNet**（2014）：引入 Inception 模块，提高计算效率。Inception 模块通过并行使用不同大小的卷积核和池化操作，能够同时提取不同尺度的特征，有效提高了模型的计算效率和性能。
    
    *   **ResNet**（2015）：引入残差连接，解决深层网络训练问题。残差连接使得梯度能够更顺畅地在网络中传播，避免了深层网络训练中的梯度消失问题，使得训练超深网络成为可能。
    
    *   **DenseNet**（2016）：密集连接，增强特征复用。DenseNet 中每一层都与前面所有层直接相连，这种密集连接方式促进了特征的复用，减少了参数数量，同时增强了特征传播的效率。
    
    *   **SENet**（2017）：引入通道注意力机制（Channel Attention Mechanism），通过学习不同通道特征的重要性，对通道进行加权，从而更有效地利用特征信息，提升模型性能。

这些模型在 ImageNet 等大型视觉识别任务上不断刷新性能记录，推动了计算机视觉的快速发展。

### 第 11 - 14 页：CNN 基本构建模块 - 卷积层



*   **基本概念**：卷积层是 CNN 的核心组件，通过滑动卷积核对输入图像进行特征提取。卷积层中的卷积核在输入图像上滑动，与图像的局部区域进行卷积操作，从而提取出各种特征。

*   **操作过程**：对于一张 3×32×32 的 RGB 图像（3 通道，32×32 像素）：


    *   使用一个 3×5×5 的卷积核（3 通道，5×5 大小），卷积核的通道数需要与输入图像的通道数相同，这样才能对每个通道进行卷积操作。
    
    *   将卷积核与图像的 5×5×3 局部区域计算点积。具体来说，就是将卷积核中每个元素与图像局部区域中对应位置的元素相乘，然后把这些乘积相加。
    
    *   加上偏置项（Bias），偏置项是一个额外的参数，为卷积操作增加了灵活性，得到输出特征图的一个像素值。
    
    *   按步长滑动卷积核，重复上述过程。步长决定了卷积核每次移动的距离，如果步长为 1，卷积核每次移动一个像素位置；如果步长为 2，卷积核每次移动两个像素位置。

*   **计算公式**


    *   输入大小：C\_in × H × W，其中 C\_in 表示输入通道数，H 表示输入图像的高度，W 表示输入图像的宽度。
    
    *   卷积核大小：C\_out × C\_in × K\_h × K\_w，C\_out 表示输出通道数，K\_h 表示卷积核的高度，K\_w 表示卷积核的宽度。
    
    *   输出大小：C\_out × H' × W'


        *   H' = floor((H - K\_h + 2P) / S + 1)
    
        *   W' = floor((W - K\_w + 2P) / S + 1)
    
        *   P 为填充（Padding），Padding 是在图像边缘填充的像素数量，用于控制输出特征图的大小和保留边缘信息；S 为步长（Stride）。

*   **多卷积核与多通道**


    *   使用多个卷积核可以提取不同特征，比如一个卷积核可能对水平边缘敏感，另一个卷积核对垂直边缘敏感。
    
    *   每个卷积核都作用于所有输入通道，确保能够从各个通道中提取特征。
    
    *   输出特征图的通道数等于卷积核数量，每个卷积核生成一个对应的通道。

*   **批量处理**：卷积操作可以同时处理一个批次的图像，提高计算效率。在实际训练中，通常会将多个图像组成一个批次（Batch）输入到卷积层进行计算，这样可以充分利用 GPU 的并行计算能力，加快训练速度。

### 第 15 页：卷积层的批量计算与参数数量分析

#### 1. 批量计算的维度扩展

当处理批量（Batch）图像时，卷积层的输入维度会从 **C\_in × H × W**（单张图像：输入通道数 × 高度 × 宽度）扩展为 **N × C\_in × H × W**，其中 **N** 表示批量大小（即一次处理的图像数量）。

对应的输出维度则从 **C\_out × H' × W'** 扩展为 **N × C\_out × H' × W'**，计算过程为：对批量中的每张图像分别执行卷积操作，再将结果按批量维度拼接。

**示例**：若输入为 8（批量大小）× 3（RGB 通道）× 32×32（图像尺寸），使用 16 个 3×5×5（输入通道 × 核高 × 核宽）的卷积核，步长 S=1、无填充（P=0），则输出维度为 8×16×28×28（计算过程：H'=(32-5+0)/1 +1=28，W' 同理）。

#### 2. 卷积层的参数数量计算

卷积层的参数仅包含 **卷积核权重** 和 **偏置项**，与输入图像尺寸、批量大小无关，核心公式如下：



*   单个卷积核的权重参数数量：**C\_in × K\_h × K\_w**（输入通道数 × 核高度 × 核宽度）

*   所有卷积核的权重参数总量：**C\_out × C\_in × K\_h × K\_w**（输出通道数 × 单个核权重数）

*   偏置项数量：**C\_out**（每个输出通道对应 1 个偏置）

*   总参数数量：**C\_out × (C\_in × K\_h × K\_w + 1)**

**示例**：若 C\_in=3、K\_h=K\_w=5、C\_out=16，则总参数数 = 16×(3×5×5 +1)=16×76=1216，远少于全连接层（如 32×32×3 图像拉伸为 3072 维向量，若全连接层输出 16 个神经元，参数数 = 3072×16 +16=49168），体现了卷积层 “参数共享” 的优势。

### 第 16 页：卷积操作的数学本质与特征提取逻辑

#### 1. 卷积的数学定义（离散形式）

对于输入特征图 **X**（维度：C\_in × H × W）和卷积核 **K**（维度：C\_out × C\_in × K\_h × K\_w），输出特征图 **Y**（维度：C\_out × H' × W'）中每个元素 **Y(c\_out, h', w')** 的计算式为：

$Y(c_{out}, h', w') = \sum_{c_{in}=1}^{C_{in}} \sum_{i=1}^{K_h} \sum_{j=1}^{K_w} K(c_{out}, c_{in}, i, j) \times X(c_{in}, h' \times S + i - P, w' \times S + j - P) + b(c_{out})$



*   符号说明：$c_{out}$ 为输出通道索引，$c_{in}$ 为输入通道索引，$h'、w'$ 为输出特征图的空间坐标，$i、j$ 为卷积核的空间坐标，$S$ 为步长，$P$ 为填充，$b(c_{out})$ 为第 $c_{out}$ 个输出通道的偏置。

*   核心逻辑：对每个输出位置，通过三重求和实现 “输入局部区域与卷积核的点积 + 偏置”，本质是**线性变换**，需配合激活函数引入非线性。

#### 2. 特征提取的层级逻辑



*   浅层卷积核：通常提取**低级特征**（如边缘、纹理、颜色块）。例如，3×3 卷积核可检测水平边缘（核值为 \[\[-1,-1,-1],\[2,2,2],\[-1,-1,-1]]）、垂直边缘（核值为 \[\[-1,2,-1],\[-1,2,-1],\[-1,2,-1]]）。

*   深层卷积核：通过组合浅层特征，提取**高级特征**（如角点、纹理组合、物体部件，如车轮、眼睛）。

*   多通道作用：不同输入通道对应图像的不同信息（如 RGB 的红、绿、蓝通道），卷积核通过跨通道求和，融合多通道信息生成更全面的特征。

### 第 17 页：卷积层的填充（Padding）类型与作用

#### 1. 填充的核心目的



*   避免边缘信息丢失：卷积操作中，图像边缘像素仅被少数卷积核覆盖（如 3×3 核无填充时，边缘像素仅参与 1 次计算，中心像素参与 9 次），填充可增加边缘像素的参与次数。

*   控制输出尺寸：通过填充使输出特征图尺寸与输入一致（“Same Padding”），或按需求调整尺寸（“Valid Padding” 无填充）。

#### 2. 常见填充类型



| 填充类型      | 定义                                   | 适用场景                       | 计算示例（输入 H=32，K\_h=5）    |
| ------------- | -------------------------------------- | ------------------------------ | -------------------------------- |
| Valid Padding | 无填充（P=0）                          | 需快速缩小特征图尺寸时         | H'=(32-5)/1 +1=28                |
| Same Padding  | 填充后输出尺寸 = 输入尺寸（P=(K-1)/2） | 需保留输入空间尺寸，如语义分割 | P=(5-1)/2=2，H'=(32-5+4)/1 +1=32 |
| 自定义填充    | 按特定需求设置 P（如 P=1、P=3）        | 特殊网络结构设计               | P=1 时，H'=(32-5+2)/1 +1=30      |

#### 3. 填充的实现方式



*   零填充（Zero Padding）：最常用方式，在图像边缘填充 0 值，不引入额外信息，计算简单。

*   反射填充（Reflection Padding）：以图像边缘为对称轴，反射填充边缘外的像素（如输入边缘像素为 \[1,2,3]，填充后为 \[2,1,2,3,2]），适用于需保留边缘纹理的场景（如风格迁移）。

*   复制填充（Replication Padding）：将边缘像素复制到填充区域（如输入边缘像素为 \[1,2,3]，填充后为 \[1,1,2,3,3]），适用于医学影像等对边缘连续性要求高的任务。

### 第 18 页：激活函数在卷积层后的作用与选择

#### 1. 激活函数的核心必要性

卷积层的输出是**线性变换**（点积 + 偏置），若仅堆叠线性层，整个网络仍等价于一个线性模型，无法拟合复杂的非线性数据（如图像中的物体形状、纹理关联）。激活函数通过**引入非线性变换**，使网络能够学习复杂特征映射，是 CNN 表达能力的关键。

#### 2. 常用激活函数及特性



| 激活函数                                    | 数学表达式                                  | 优点                                                       | 缺点                                                    | 适用场景                             |
| ------------------------------------------- | ------------------------------------------- | ---------------------------------------------------------- | ------------------------------------------------------- | ------------------------------------ |
| ReLU（Rectified Linear Unit，修正线性单元） | $f(x) = \max(0, x)$                         | 计算快（仅比较和取最大值）、缓解梯度消失（x>0 时梯度 = 1） | 存在 “死亡 ReLU” 问题（x≤0 时梯度 = 0，神经元永久失活） | 卷积层、全连接层（目前最常用）       |
| Leaky ReLU                                  | $f(x) = \max(\alpha x, x)$（α 通常 = 0.01） | 解决死亡 ReLU 问题（x≤0 时梯度 =α≠0）                      | 额外引入 α 参数，需调优                                 | 替代 ReLU，适用于深层网络            |
| Sigmoid                                     | $f(x) = \frac{1}{1 + e^{-x}}$               | 输出在 \[0,1]，可表示概率                                  | 梯度消失严重（x>3 或 x<-3 时梯度≈0）、计算慢            | 早期网络，现仅用于输出层（如二分类） |
| Tanh（双曲正切）                            | $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$  | 输出在 \[-1,1]，中心对称                                   | 仍存在梯度消失问题                                      | 早期网络，现较少使用                 |

#### 3. 激活函数的位置

在 CNN 中，激活函数通常**紧跟在卷积层之后**（即 “Conv → ReLU” 顺序），部分网络也会在全连接层后使用，但池化层后一般不使用（池化层本身为非线性操作，且激活函数会放大池化后的噪声）。

### 第 19 页：卷积层与激活函数的组合示例

#### 1. 单通道输入的组合计算

**输入**：1（灰度通道）× 5×5（图像尺寸），像素值如下：

$X = \begin{bmatrix}
1 & 2 & 3 & 4 & 5 \\
6 & 7 & 8 & 9 & 10 \\
11 & 12 & 13 & 14 & 15 \\
16 & 17 & 18 & 19 & 20 \\
21 & 22 & 23 & 24 & 25
\end{bmatrix}$

**卷积核**：1（输出通道）× 1×3×3（输入通道 × 核高 × 核宽），核值如下：

$K = \begin{bmatrix}
-1 & 0 & 1 \\
-2 & 0 & 2 \\
-1 & 0 & 1
\end{bmatrix}$

**参数**：步长 S=1，无填充 P=0，偏置 b=0，激活函数 ReLU。

#### 2. 计算过程



1.  卷积操作：输出特征图尺寸 H'=W'=(5-3)/1 +1=3，每个元素计算如下：

*   Y(1,1,1) = (1×(-1)+2×0+3×1) + (6×(-2)+7×0+8×2) + (11×(-1)+12×0+13×1) + 0 = (-1+0+3)+(-12+0+16)+(-11+0+13) = 2+4+2=8

*   Y(1,1,2) = (2×(-1)+3×0+4×1) + (7×(-2)+8×0+9×2) + (12×(-1)+13×0+14×1) = (-2+0+4)+(-14+0+18)+(-12+0+14)=2+4+2=8

*   同理计算剩余元素，得到卷积输出：

$Y_{conv} = \begin{bmatrix}
8 & 8 & 8 \\
24 & 24 & 24 \\
40 & 40 & 40
\end{bmatrix}$



1.  激活函数（ReLU）：因所有输出均为正，激活后结果与 $Y_{conv}$ 一致。

#### 3. 多通道输入的组合逻辑

若输入为 3（RGB 通道）×5×5，卷积核为 2（输出通道）×3×3×3，则：



*   每个输出通道的卷积结果 = 该通道对应的 3 个单通道卷积结果之和 + 偏置

*   再对每个输出通道分别应用 ReLU 激活，最终得到 2×3×3 的激活后特征图。

### 第 20 页：空间维度计算的特殊情况与处理

#### 1. 步长与核大小不匹配导致的非整数输出

当按公式 $H' = \text{floor}((H - K_h + 2P)/S + 1)$ 计算时，若 $(H - K_h + 2P)$ 不能被 S 整除，会出现非整数结果，此时需通过 **向下取整（floor）** 确保输出尺寸为整数。

**示例**：输入 H=7，K\_h=5，S=2，P=0，则 $(7-5+0)/2=1$，H'=1+1=2（整数，正常）；若 H=8，K\_h=5，S=2，P=0，则 $(8-5+0)/2=1.5$，向下取整后 H'=1+1=2（输出高度为 2）。

#### 2. 不对称填充（Asymmetric Padding）

当 K 为偶数时，$(K-1)/2$ 为非整数（如 K=4 时，(4-1)/2=1.5），此时无法通过对称填充实现 “Same Padding”，需使用不对称填充（如左侧填充 1 像素，右侧填充 2 像素）。

**示例**：输入 H=5，K\_h=4，需输出 H'=5（Same Padding），则 $(5 - 4 + 2P)/1 +1=5$ → 2P=3 → 左侧 P1=1，右侧 P2=2（总填充 3 像素），确保输入扩展为 5+3=8，卷积后 H'=(8-4)/1 +1=5。

#### 3. 膨胀卷积（Dilated Convolution）的维度计算

膨胀卷积（也叫空洞卷积，Dilated Convolution）通过在卷积核元素间插入 “空洞”（零值），扩大感受野而不增加参数。其等效核大小为 $K_{dilated} = K + (K-1)(r-1)$，其中 $r$ 为膨胀率（Dilation Rate，r=1 时为普通卷积）。

维度计算公式调整为：$H' = \text{floor}((H - K_{dilated} + 2P)/S + 1)$

**示例**：K=3，r=2，等效核大小 = 3+(3-1)(2-1)=5；输入 H=7，S=1，P=0，则 H'=(7-5)/1 +1=3（感受野从 3 扩大到 5，参数仍为 3×3=9）。

### 第 21 页：池化层的基本原理与类型

#### 1. 池化层的核心作用



*   **降采样（Downsampling）**：减少特征图的空间维度（高度和宽度），降低后续层的计算量和参数量（如 2×2 池化可使特征图尺寸减半，计算量减少 75%）。

*   **增强空间不变性**：使特征对输入图像的微小平移、缩放不敏感（如池化后，物体位置偏移 1-2 像素，特征仍保持一致）。

*   **防止过拟合**：通过减少特征数量，降低模型对局部噪声的依赖，提高泛化能力。

#### 2. 常见池化类型及操作



| 池化类型                       | 操作逻辑                                     | 优点                                           | 缺点                           | 适用场景                            |
| ------------------------------ | -------------------------------------------- | ---------------------------------------------- | ------------------------------ | ----------------------------------- |
| 最大池化（Max Pooling）        | 取池化窗口内的最大值作为输出                 | 保留局部区域的最强特征（如边缘、角点），计算快 | 丢失窗口内其他像素的信息       | 卷积层后（最常用，如 AlexNet、VGG） |
| 平均池化（Mean Pooling）       | 取池化窗口内的平均值作为输出                 | 保留局部区域的整体信息，平滑噪声               | 削弱强特征，可能导致特征模糊   | 全连接层前（如 GoogLeNet 的输出层） |
| 随机池化（Stochastic Pooling） | 按窗口内像素值的概率分布随机选择输出         | 结合 Max 和 Mean 的优势，增强泛化能力          | 计算复杂，存在随机性           | 需避免过拟合的深层网络              |
| L2 池化（L2 Pooling）          | 取池化窗口内像素值的 L2 范数（平方和开根号） | 突出局部区域的整体强度，鲁棒性强               | 计算量大于 Max 和 Mean Pooling | 早期网络（如 SIFT 特征提取）        |

#### 3. 池化层的参数

池化层无**可学习参数**，仅需设置两个超参数：



*   池化窗口大小（Pool Size，如 2×2、3×3）：常用 2×2，平衡降采样率和信息保留。

*   步长（Stride）：通常与窗口大小相同（如 2×2 窗口，步长 = 2），避免窗口重叠导致的冗余计算；若步长 < 窗口大小（如 2×2 窗口，步长 = 1），则会产生重叠池化（Overlapping Pooling），可增强特征鲁棒性（如 AlexNet 使用 3×3 窗口，步长 = 2）。

### 第 22 页：池化层的计算示例与特殊操作

#### 1. 最大池化与平均池化对比计算

以 **2×2 池化窗口、步长 S=2、无填充** 为例，输入特征图（激活后）如下：

$X = \begin{bmatrix}
7 & 8 & 4 & 4 \\
3 & 6 & 1 & 4 \\
4 & 7 & 7 & 8 \\
2 & 2 & 1 & 2
\end{bmatrix}$



* **最大池化计算**：取每个 2×2 窗口的最大值

  窗口 1（左上角）：max (7,8,3,6)=8；窗口 2（右上角）：max (4,4,1,4)=4

  窗口 3（左下角）：max (4,7,2,2)=7；窗口 4（右下角）：max (7,8,1,2)=8

  输出：$Y_{max} = \begin{bmatrix} 8 & 4 \\ 7 & 8 \end{bmatrix}$

* **平均池化计算**：取每个 2×2 窗口的平均值（保留整数）

  窗口 1：(7+8+3+6)/4=24/4=6；窗口 2：(4+4+1+4)/4=13/4≈3

  窗口 3：(4+7+2+2)/4=15/4≈4；窗口 4：(7+8+1+2)/4=18/4≈4

  输出：$Y_{mean} = \begin{bmatrix} 6 & 3 \\ 4 & 4 \end{bmatrix}$

#### 2. 重叠池化（Overlapping Pooling）示例

当 **池化窗口 = 3×3、步长 S=2** 时，窗口间会产生重叠（重叠区域为 1 像素）。

输入 H=7，输出 H'=(7-3)/2 +1=3（计算过程：(7-3)=4，4/2=2，2+1=3），每个窗口重叠 1 行 / 列，可增强特征的鲁棒性（如 AlexNet 的池化层设计）。

#### 3. 全局池化（Global Pooling）



* **操作逻辑**：将整个特征图作为一个池化窗口，输出单个值（全局最大值或全局平均值）。

  若输入为 C\_out×H×W，全局池化后输出为 C\_out×1×1（每个通道对应 1 个值）。

* **优势**：替代全连接层的部分功能，减少参数量（无权重参数），避免过拟合，且输出维度与输入空间尺寸无关（便于处理不同尺寸图像）。

* **应用**：GoogLeNet 的输出层使用全局平均池化，将 1024×7×7 的特征图转换为 1024×1×1，再连接 1000 类全连接层。

### 第 23 页：感受野（Receptive Field）的定义与基础计算

#### 1. 感受野的核心概念

**感受野**是指输出特征图上的一个像素，对应输入图像（或前一层特征图）的区域大小，单位为 “像素”。它决定了该像素能 “看到” 的输入信息范围，是理解 CNN 如何捕捉不同尺度特征的关键。

#### 2. 单层卷积层的感受野计算

对于**单卷积层**（无池化），感受野大小等于卷积核大小（K）。



*   示例：输入图像为 32×32，使用 5×5 卷积核（S=1，P=0），输出特征图上每个像素的感受野为 5×5（即该像素由输入的 5×5 区域计算得到）。

#### 3. 多层堆叠的感受野计算（基础公式）

当网络由 “卷积层 + 池化层” 交替堆叠时，感受野需从**输出层反向推导**，核心公式如下（假设各层步长均为 S，无膨胀）：

$RF_{l} = RF_{l+1} \times S_{l} + (K_{l} - S_{l})$



*   符号说明：$RF_{l}$ 为第 l 层的感受野，$RF_{l+1}$ 为第 l+1 层的感受野（初始值：输出层 RF=1），$S_{l}$ 为第 l 层的步长，$K_{l}$ 为第 l 层的卷积核 / 池化核大小。

#### 4. 两层网络的感受野示例

网络结构：输入层 → Conv1（K1=3，S1=1） → Conv2（K2=3，S2=1）



*   反向推导：

1. 输出层（Conv2 输出）RF2=1

2. Conv1 的 RF1 = RF2×S1 + (K1 - S1) = 1×1 + (3-1)=3

3. 输入层的感受野（即 Conv2 输出像素对应输入的区域）= RF1=3？

   （*注：实际两层 3×3 卷积的感受野为 5×5，上述基础公式需修正，完整推导见第 24 页*）

### 第 24 页：感受野的完整推导与多层网络示例

#### 1. 完整推导公式（考虑所有层的累积效应）

对于 L 层网络（从输入层到输出层），第 1 层（输入层）的感受野（即输出层像素对应输入的区域）公式为：

$RF = 1 + \sum_{i=1}^{L} (K_{i} - 1) \times \prod_{j=1}^{i-1} S_{j}}$



*   符号说明：$K_{i}$ 为第 i 层的核大小（卷积核 / 池化核），$S_{j}$ 为第 j 层的步长，$\prod_{j=1}^{0} S_{j}=1$（空乘积）。

*   核心逻辑：每一层的感受野扩展量为 “(核大小 - 1)× 前序所有层的步长乘积”，累积后加 1（初始输出层 RF=1）。

#### 2. 两层 3×3 卷积的感受野修正计算

网络结构：输入层 → Conv1（K1=3，S1=1） → Conv2（K2=3，S2=1）



* 代入公式：

  $RF = 1 + [(3-1)×1] + [(3-1)×1×1] = 1 + 2 + 2 = 5$

* 结论：两层 3×3 卷积（S=1）的感受野为 5×5，等效于一层 5×5 卷积，但多了一次非线性激活（ReLU），表达能力更强。

#### 3. 含池化层的多层网络示例

网络结构：输入层 → Conv1（K1=3，S1=1） → Pool1（Kp=2，Sp=2） → Conv2（K2=3，S2=1）



* 反向推导（按完整公式）：

  $RF = 1 + [(3-1)×1] + [(2-1)×1×1] + [(3-1)×1×1×2]$

  $= 1 + 2 + 1 + 4 = 8$

* 验证：

1. Conv2 输出 RF=1，对应 Pool1 输出的 3×3 区域（因 Conv2 K=3，S=1）

2. Pool1 输出的 3×3 区域对应 Conv1 输出的 6×6 区域（因 Pool1 K=2，S=2，3×2=6）

3. Conv1 输出的 6×6 区域对应输入的 8×8 区域（因 Conv1 K=3，S=1，6+3-1=8）

   最终输入感受野为 8×8，与公式计算一致。

### 第 25 页：感受野的关键影响因素与设计原则

#### 1. 影响感受野的核心因素



*   **核大小（K）**：K 越大，单一层的感受野扩展量越大（(K-1) 项），但参数量也会增加（如 5×5 核参数是 3×3 核的 2.7 倍）。

*   **步长（S）**：S 越大，后续层的感受野扩展速度越快（前序步长乘积项），但会导致特征图尺寸快速缩小，可能丢失细节。

*   **网络深度（L）**：深度越深，感受野累积越大（多层扩展量叠加），但需避免梯度消失（如 ResNet 通过残差连接解决）。

*   **膨胀率（r）**：膨胀卷积（Dilated Conv）通过增大 r，可在不增加参数的情况下扩大感受野（等效 K=K+(K-1)(r-1)）。

#### 2. 感受野设计原则



*   **小核多堆叠优于大核**：如 3 个 3×3 卷积（感受野 7×7）比 1 个 7×7 卷积，参数量更少（3×(3²C²)=27C² vs 7²C²=49C²），且多两次 ReLU 激活，非线性表达更强。

*   **步长控制在 1-2**：步长 = 1 时，感受野扩展平稳，特征图尺寸缩小慢（需配合 Pooling 降采样）；步长 = 2 时，感受野扩展快，但需避免连续使用（防止细节丢失）。

*   **目标尺度匹配感受野**：检测小目标（如 10×10 像素）时，感受野需设计为 10-20×10-20；检测大目标（如 100×100 像素）时，感受野需≥100×100（如 ResNet-50 的最后卷积层感受野约 224×224，匹配 ImageNet 输入尺寸）。

### 第 26 页：CNN 的经典架构模式（基础版）

#### 1. 核心架构范式

CNN 的经典架构遵循 “**特征提取→分类决策**” 的两步逻辑，基础模式为：

$[(Conv → ReLU) × N → Pool] × M → (FC → ReLU) × K → Softmax$



*   符号说明：N 为每组卷积层的数量（通常 1-3），M 为 “卷积组 + 池化” 的堆叠次数（通常 3-5），K 为全连接层的数量（通常 1-3）。

#### 2. 各模块的功能分工



| 模块        | 功能                           | 设计要点                                      |
| ----------- | ------------------------------ | --------------------------------------------- |
| Conv → ReLU | 提取局部特征，引入非线性       | 卷积核 K=3/5，S=1，Same Padding（保留尺寸）   |
| Pool        | 降采样，扩大感受野，增强不变性 | 池化核 K=2，S=2（尺寸减半），Max Pooling 为主 |
| FC → ReLU   | 融合全局特征，映射到类别空间   | 神经元数量从大到小（如 4096→1024→1000）       |
| Softmax     | 输出类别概率分布（多分类任务） | 输出维度 = 类别数，损失函数用交叉熵           |

#### 3. 基础架构示例（LeNet-5 简化版）



* 结构：Input (32×32×1) → (Conv1 (5×5, 6)→ReLU) → Pool1 (2×2, S=2) → (Conv2 (5×5, 16)→ReLU) → Pool2 (2×2, S=2) → FC1 (120)→ReLU → FC2 (84)→ReLU → FC3 (10)→Softmax

* 维度变化：

  32×32×1 → 28×28×6（Conv1，K=5，S=1，P=0）→ 14×14×6（Pool1）→ 10×10×16（Conv2）→ 5×5×16（Pool2）→ 120（FC1，5×5×16=400→120）→ 84（FC2）→10（FC3）

### 第 27 页：CNN 架构的发展趋势（2012-2016）

#### 1. 核大小：从大核到小核堆叠



*   早期（2012，AlexNet）：使用 11×11（Conv1）、5×5（Conv2）大核，目的是快速缩小尺寸（11×11 核 S=4，32×32→10×10），但参数量大。

*   中期（2014，VGG）：全用 3×3 小核堆叠（如 VGG-16 有 13 个 3×3 卷积层），通过多层堆叠扩大感受野（如 3 个 3×3→感受野 7×7），参数量更少，表达能力更强。

*   创新（2014，GoogLeNet）：引入 1×1 卷积核，作用是 “降维”（如输入 256 通道→1×1 卷积→64 通道），减少后续 3×3/5×5 卷积的参数量（如 64×3×3 vs 256×3×3，参数量减少 75%）。

#### 2. 网络深度：从浅层到超深



*   2012（AlexNet）：8 层权重层（5 Conv + 3 FC）

*   2014（VGG）：16/19 层权重层（13/16 Conv + 3 FC）

*   2015（ResNet）：50/101/152 层权重层，通过残差连接（Residual Connection）解决深层网络的梯度消失问题，首次实现超深网络的有效训练。

#### 3. 全连接层：从多到少，逐步替代



*   早期（AlexNet/VGG）：3 个全连接层（如 AlexNet 的 FC6=4096，FC7=4096，FC8=1000），参数量占比大（VGG-16 全连接层参数量占总参数的 75%）。

*   中期（GoogLeNet）：用 “全局平均池化 + 1 个 FC” 替代多 FC 层，参数量从千万级降至百万级（GoogLeNet 仅 500 万参数，远少于 AlexNet 的 6000 万）。

*   后期（ResNet-50）：仅保留 1 个全连接层（1000 类），前序用卷积层替代 FC 的特征融合功能，进一步减少参数。

### 第 28 页：全连接层（Fully Connected Layer）的作用与局限

#### 1. 全连接层的核心功能



*   **全局特征融合**：卷积层和池化层提取的是局部 / 区域特征（如边缘、部件），全连接层通过 “每个神经元与前一层所有神经元连接”，将分散的局部特征融合为全局特征（如 “车轮 + 车窗 + 车身”→“汽车”）。

*   **类别映射**：将高维特征（如 ResNet-50 的 2048 维）映射到低维类别空间（如 1000 维），输出每个类别的分数（Logits），再通过 Softmax 转换为概率。

#### 2. 全连接层的参数计算

若前一层特征图维度为 **C×H×W**（展平后为 D=C×H×W 维向量），全连接层有 **N** 个神经元，则：



*   权重参数数量：D×N

*   偏置参数数量：N

*   总参数数量：D×N + N = N×(D+1)

示例：VGG-16 的 FC6 层，前一层为 5×5×512=12800 维，N=4096，则参数数 = 4096×(12800+1)≈5243 万（占 VGG-16 总参数的 40%）。

#### 3. 全连接层的局限



*   **参数量大**：易导致过拟合（需用 Dropout 正则化），且占用更多内存（如 VGG-16 的全连接层参数需约 200MB 存储）。

*   **空间信息丢失**：将 2D 特征图展平为 1D 向量，破坏了像素间的空间关联（如 “猫的眼睛在耳朵下方” 的空间关系被淡化），不适用于语义分割、目标检测等需空间信息的任务。

*   **输入尺寸固定**：全连接层的输入维度（D=C×H×W）固定，无法处理不同尺寸的输入图像（如 VGG-16 仅支持 224×224 输入，若输入 256×256，展平后维度变化，全连接层无法匹配）。

### 第 29 页：CNN 的训练流程与反向传播核心（续）

#### 1. 正向传播（Forward Propagation）完整流程

以 “Conv→ReLU→Pool→FC→Softmax” 为例，正向传播（FP）具体步骤（输入为 N×C\_in×H×W 的批量图像）：



1.  **卷积层（Conv）**：通过公式 $Y_{conv} = X * K + b$ 计算（\* 表示卷积操作），输出 N×C\_out×H'×W' 的特征图。

2.  **ReLU 激活**：对 $Y_{conv}$ 逐元素应用 $f(x)=\max(0,x)$，输出激活后特征图 $Y_{relu}$（维度不变）。

3.  **池化层（Pool）**：对 $Y_{relu}$ 执行 Max/Mean Pooling，输出 N×C\_out×H''×W'' 的降采样特征图 $Y_{pool}$（H''=H'/S\_pool，W''=W'/S\_pool）。

4.  **全连接层（FC）**：将 $Y_{pool}$ 展平为 N×D（D=C\_out×H''×W''）的向量，通过 $Y_{fc}=W_{fc} \cdot X_{flat} + b_{fc}$ 计算，输出 N×N\_class 的类别分数（Logits）。

5.  **Softmax 层**：对 $Y_{fc}$ 应用 $Softmax(x_i)=\frac{e^{x_i}}{\sum_{j=1}^{N_class} e^{x_j}}$，输出 N×N\_class 的类别概率分布 $Y_{prob}$。

#### 2. 损失函数计算（以多分类为例）

使用**交叉熵损失（Cross-Entropy Loss）**，衡量预测概率与真实标签（One-Hot 编码）的差异，公式为：

$L = -\frac{1}{N} \sum_{n=1}^{N} \sum_{c=1}^{N_class} y_{n,c} \log(Y_{prob,n,c})$



*   符号说明：$y_{n,c}$ 为第 n 个样本的真实标签（c 类为 1，其余为 0），$Y_{prob,n,c}$ 为第 n 个样本预测为 c 类的概率，N 为批量大小。

#### 3. 反向传播（Backward Propagation，BP）核心逻辑

反向传播的目标是计算损失 L 对所有可学习参数（卷积核 K、偏置 b、全连接权重 W\_fc）的梯度，核心遵循**链式法则**：

$\frac{\partial L}{\partial \theta} = \frac{\partial L}{\partial Y_{last}} \cdot \frac{\partial Y_{last}}{\partial Y_{last-1}} \cdot \dots \cdot \frac{\partial Y_1}{\partial \theta}$



*   关键层的梯度计算（以 Max Pooling 和 Conv 层为例）：

1.  **Softmax→FC 层**：梯度 $\frac{\partial L}{\partial W_{fc}} = \frac{1}{N} X_{flat}^T \cdot (Y_{prob} - Y_{true})$（$Y_{true}$ 为 One-Hot 标签），$\frac{\partial L}{\partial b_{fc}} = \frac{1}{N} \sum_{n=1}^{N} (Y_{prob,n} - Y_{true,n})$。

2.  **FC→Pool 层**：将 FC 层的梯度 $\frac{\partial L}{\partial X_{flat}}$ 重塑为 N×C\_out×H''×W''，得到 Pool 层输出的梯度 $\frac{\partial L}{\partial Y_{pool}}$。

3.  **Pool→ReLU 层（Max Pooling）**：Max Pooling 的梯度具有 “稀疏性”—— 仅在正向传播时取最大值的位置传递梯度（其余位置梯度为 0），即 $\frac{\partial L}{\partial Y_{relu}} = \frac{\partial L}{\partial Y_{pool}} \odot M$（$\odot$ 为逐元素乘，M 为掩码矩阵，最大值位置为 1，其余为 0）。

4.  **ReLU→Conv 层**：ReLU 的梯度为 $\frac{\partial L}{\partial Y_{conv}} = \frac{\partial L}{\partial Y_{relu}} \odot I(Y_{conv}>0)$（I 为指示函数，$Y_{conv}>0$ 时为 1，否则为 0）。

5.  **Conv 层参数梯度**：卷积核梯度 $\frac{\partial L}{\partial K} = \frac{1}{N} X_{pad}^T * \frac{\partial L}{\partial Y_{conv}}$（$X_{pad}$ 为输入填充后的数据），偏置梯度 $\frac{\partial L}{\partial b} = \frac{1}{N} \sum_{n=1}^{N} \sum_{h',w'} \frac{\partial L}{\partial Y_{conv,n,:,h',w'}}$（对空间维度求和）。

### 第 30 页：CNN 训练的正则化技术（防止过拟合）

#### 1. 过拟合的表现与原因



*   **表现**：训练集损失持续下降、准确率持续上升，但测试集损失先降后升、准确率先升后降（模型在训练集上 “memorize” 噪声，泛化能力差）。

*   **原因**：CNN 参数量大（如 VGG-16 约 1.38 亿参数）、训练数据量不足或数据多样性不够。

#### 2. 常用正则化技术



| 正则化技术                            | 核心原理                                   | 实现方式                                                     | 适用层                           |
| ------------------------------------- | ------------------------------------------ | ------------------------------------------------------------ | -------------------------------- |
| Dropout                               | 随机 “关闭” 部分神经元，避免依赖特定神经元 | 训练时：以概率 p（通常 0.5）随机置零神经元输出；测试时：不置零，输出乘以 (1-p) 或直接使用 | 全连接层（为主）、卷积层（较少） |
| L2 正则化（权重衰减）                 | 对权重参数添加 L2 惩罚项，限制权重过大     | 损失函数添加 $\lambda \sum_{\theta} \|\theta\|_2^2$（$\lambda$ 为正则化强度，通常 1e-4\~1e-2） | 卷积层、全连接层的权重参数       |
| 数据增强（Data Augmentation）         | 扩大训练集规模，增加数据多样性             | 几何变换（随机翻转、旋转、裁剪、缩放）、颜色抖动（随机调整亮度、对比度、饱和度）、噪声添加 | 输入层（训练时实时增强）         |
| 批量归一化（Batch Normalization，BN） | 标准化层输入，加速训练并增强泛化能力       | 对每个批次的特征图，按通道计算均值和方差，进行标准化：$y = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$（$\gamma、\beta$ 为可学习参数） | 卷积层、全连接层后（ReLU 前）    |

#### 3. 示例：Dropout 在全连接层的应用



*   训练时：FC 层输出 $Y_{fc}$ 经过 Dropout 后为 $Y_{dropout} = Y_{fc} \odot M$（M 为随机掩码，元素为 1 的概率为 0.5），再输入下一层。

*   测试时：FC 层输出直接为 $Y_{fc}$（不乘掩码），或乘以 0.5（保持输出期望一致），避免测试时输出波动。

### 第 31 页：数据增强的具体方法与实践原则

#### 1. 几何变换类增强（保留语义信息）



*   **随机水平翻转（Random Horizontal Flip）**：以 50% 概率水平翻转图像（如猫的图像翻转后仍为猫），适用于无左右语义差异的场景（如 ImageNet 分类），不适用于有方向要求的场景（如手写数字 “6” 和 “9”）。

*   **随机裁剪（Random Crop）**：从原始图像中随机裁剪固定尺寸的区域（如从 256×256 图像中裁剪 224×224），训练时增强模型对图像局部区域的鲁棒性，测试时通常使用中心裁剪。

*   **随机旋转（Random Rotation）**：在一定角度范围内（如 ±15°）随机旋转图像，避免模型对物体姿态过于敏感（需注意旋转后图像边缘的填充，通常用黑色或反射填充）。

*   **随机缩放（Random Resize）**：将图像随机缩放到不同尺寸（如 0.8\~1.2 倍），再裁剪到固定尺寸，增强模型对物体尺度变化的适应能力。

#### 2. 颜色与像素类增强（改变外观，保留结构）



*   **颜色抖动（Color Jitter）**：随机调整图像的亮度（Brightness，如 0.8\~1.2 倍）、对比度（Contrast，如 0.8\~1.2 倍）、饱和度（Saturation，如 0.8\~1.2 倍）、色调（Hue，如 ±0.1），适用于 RGB 图像，增强模型对光照和颜色变化的鲁棒性。

*   **灰度化（Grayscale）**：以一定概率（如 10%）将 RGB 图像转换为灰度图，避免模型过度依赖颜色信息（如交通标志识别中，颜色可能受天气影响）。

*   **高斯噪声（Gaussian Noise）**：向图像像素添加少量高斯噪声（方差通常 0.01\~0.05），增强模型对噪声的抵抗能力（需控制噪声强度，避免掩盖有效特征）。

#### 3. 实践原则



*   **不改变语义**：增强后的图像需保持原始类别标签（如不能将 “狗” 的图像翻转成 “猫”）。

*   **与任务匹配**：目标检测任务中，增强需同步调整边界框坐标（如裁剪图像时，需对应裁剪边界框）；语义分割任务中，需同步增强像素标签。

*   **适度增强**：过度增强（如旋转角度过大、噪声过强）会破坏有效特征，导致训练数据失真。

### 第 32 页：ImageNet 数据集与 ILSVRC 挑战赛

#### 1. ImageNet 数据集基本信息



*   **规模**：包含超过 1400 万张标注图像，涵盖 1000 个类别（如 “猫”“狗”“汽车”“飞机” 等），每个类别约有 1000\~5000 张图像。

*   **图像分辨率**：图像分辨率不固定（从几百到几千像素不等），训练时需统一调整为固定尺寸（如 224×224、299×299）。

*   **标注质量**：由人工标注类别标签，部分图像包含目标边界框标注（用于目标检测任务），是计算机视觉领域最权威的大规模数据集之一。

#### 2. ILSVRC 挑战赛（ImageNet Large Scale Visual Recognition Challenge）



*   **举办时间**：2010\~2017 年（后停止举办，转为学术基准），每年吸引全球顶尖科研机构和企业参赛。

*   **核心任务**：

1.  **图像分类（Classification）**：判断图像属于 1000 个类别中的哪一个，评估指标为 top-1 准确率（预测概率最高的类别正确）和 top-5 准确率（预测概率前 5 的类别中包含正确类别）。

2.  **目标检测（Object Detection）**：找出图像中所有目标并标注类别和边界框，评估指标为 mAP（mean Average Precision）。

3.  **图像分割（Segmentation）**：对图像中每个像素进行类别标注，评估指标为 mIoU（mean Intersection over Union）。

#### 3. ILSVRC 的里程碑意义



*   **2012 年（AlexNet）**：首次使用深度 CNN 参赛，top-5 错误率从传统方法的 26% 降至 15.4%，标志着深度学习在计算机视觉领域的崛起。

*   **2015 年（ResNet）**：top-5 错误率降至 3.6%，首次超过人类水平（人类在该任务上的错误率约 5%），证明了超深网络的潜力。

*   **技术推动**：ILSVRC 催生了 AlexNet、VGG、GoogLeNet、ResNet 等经典 CNN 模型，奠定了现代计算机视觉的技术基础。

### 第 33 页：AlexNet 详解（一）—— 架构与创新点

#### 1. AlexNet 的背景与数据集



*   **发表时间**：2012 年（Alex Krizhevsky 等，多伦多大学），发表于 NeurIPS 会议。

*   **数据集**：ILSVRC 2012，训练集 120 万张图像，验证集 5 万张，测试集 10 万张（1000 类）。

*   **硬件环境**：使用两块 NVIDIA GTX 580 GPU 并行训练（当时 GPU 显存有限，需拆分网络到两块 GPU）。

#### 2. 完整架构（输入 227×227×3，注意：论文中写 224×224，实际计算为 227×227）



| 层序号 | 层类型   | 核大小 / 参数  | 步长（S） | 填充（P） | 输出维度  | 其他操作                                      |
| ------ | -------- | -------------- | --------- | --------- | --------- | --------------------------------------------- |
| 1      | Conv1    | 96 个 11×11×3  | 4         | 0         | 55×55×96  | ReLU、BN（无，当时未提出）、GPU 拆分（48+48） |
| 2      | MaxPool1 | 3×3            | 2         | 0         | 27×27×96  | 重叠池化（窗口 3×3，步长 2）                  |
| 3      | LRN1     | 局部响应归一化 | -         | -         | 27×27×96  | 增强泛化能力                                  |
| 4      | Conv2    | 256 个 5×5×48  | 1         | 2         | 27×27×256 | ReLU、GPU 拆分（128+128）                     |
| 5      | MaxPool2 | 3×3            | 2         | 0         | 13×13×256 | 重叠池化                                      |
| 6      | LRN2     | 局部响应归一化 | -         | -         | 13×13×256 | -                                             |
| 7      | Conv3    | 384 个 3×3×256 | 1         | 1         | 13×13×384 | ReLU、GPU 合并（无拆分）                      |
| 8      | Conv4    | 384 个 3×3×192 | 1         | 1         | 13×13×384 | ReLU、GPU 拆分（192+192）                     |
| 9      | Conv5    | 256 个 3×3×192 | 1         | 1         | 13×13×256 | ReLU、GPU 拆分（128+128）                     |
| 10     | MaxPool3 | 3×3            | 2         | 0         | 6×6×256   | 重叠池化                                      |
| 11     | FC6      | 4096 个神经元  | -         | -         | 4096      | ReLU、Dropout（p=0.5）                        |
| 12     | FC7      | 4096 个神经元  | -         | -         | 4096      | ReLU、Dropout（p=0.5）                        |
| 13     | FC8      | 1000 个神经元  | -         | -         | 1000      | Softmax（输出类别概率）                       |

#### 3. 核心创新点（1）



*   **ReLU 激活函数**：首次在深度 CNN 中大规模使用 ReLU 替代传统 Sigmoid/Tanh，解决了梯度消失问题（ReLU 在 x>0 时梯度 = 1，避免梯度指数级衰减），训练速度比 Sigmoid 快数倍。

*   **GPU 并行训练**：将网络拆分为两部分，分别在两块 GPU 上训练（如 Conv1 的 96 个核拆分为 48+48），突破当时 GPU 显存限制（GTX 580 仅 3GB 显存），加速训练。

#### 1. 核心创新点（2）

*   **重叠池化（Overlapping Pooling）**：池化窗口 3×3，步长 2（非重叠池化通常步长 = 窗口大小），重叠区域为 1 像素。相比非重叠池化，重叠池化减少了过拟合（论文中实验证明，错误率降低约 0.4%），增强了特征的鲁棒性。


* **局部响应归一化（Local Response Normalization，LRN）**：在 Conv1 和 Conv2 后应用，模拟生物视觉系统的 “侧抑制” 机制 —— 增强响应大的神经元，抑制邻近神经元，公式如下：

  $b_{x,y,i} = a_{x,y,i} / \left( k + \alpha \sum_{j=\max(0,i-n/2)}^{\min(N-1,i+n/2)} a_{x,y,j}^2 \right)^\beta$


    *   符号说明：$a_{x,y,i}$ 为归一化前的特征值（x,y 为空间坐标，i 为通道索引），$b_{x,y,i}$ 为归一化后的值；k=2、n=5、α=1e-4、β=0.75 为论文中设置的超参数；N 为当前层的通道数。
    
    *   作用：减少过拟合，使模型对局部特征的响应更稳定（后续 VGGNet 证明 LRN 作用有限，逐渐被 Batch Normalization 替代）。

*   **数据增强技术**：针对 ILSVRC 数据集不足的问题，AlexNet 采用两种关键增强方式：

1.  **随机裁剪与水平翻转**：将 256×256 图像随机裁剪为 227×227（训练时），测试时取 5 个角落 + 中心裁剪 + 水平翻转，共 10 个样本，取预测平均值（提升泛化能力）。

2.  **颜色抖动**：在 RGB 颜色空间中，通过主成分分析（PCA）调整像素值，公式为 $I_{aug} = I + \epsilon_1 \lambda_1 u_1 + \epsilon_2 \lambda_2 u_2 + \epsilon_3 \lambda_3 u_3$（$\epsilon_i$ 为服从 N (0,0.1) 的随机值，$\lambda_i$ 为协方差矩阵特征值，$u_i$ 为特征向量），模拟光照和颜色变化。

#### 2. 性能表现



*   **ILSVRC 2012 结果**：top-1 错误率 37.5%，top-5 错误率 15.4%，远超第二名（传统方法，top-5 错误率 26.2%），差距显著。

*   **参数量与计算量**：约 6000 万可学习参数，单次前向传播需约 1.5×10^8 次浮点运算（FLOPs），在当时 GPU 环境下训练需 5-6 天。

#### 3. 局限性



*   未使用批量归一化（BN），训练时需精细调整学习率（初始 0.01，后期逐步衰减）。

*   全连接层参数量过大（FC6+FC7+FC8 约 4000 万参数，占总参数的 67%），导致模型内存占用高。

*   仅支持固定尺寸输入（227×227），灵活性差。

### 第 35 页：VGGNet 详解（一）—— 背景与核心设计理念

#### 1. VGGNet 的背景



*   **发表时间**：2014 年（Karen Simonyan 等，牛津大学视觉几何组 VGG），发表于 ICLR 会议。

*   **数据集**：ILSVRC 2014，与 AlexNet 相同（1000 类）。

*   **竞赛结果**：在分类任务中 top-5 错误率 7.3%（AlexNet 为 15.4%），检测任务中获得第二名（仅次于 GoogLeNet），以其简洁、规整的架构成为后续迁移学习的常用模型。

#### 2. 核心设计理念：小核堆叠替代大核

VGGNet 的核心创新是**用多个 3×3 小卷积核堆叠，替代 AlexNet 中的 11×11、5×5 大核**，主要优势如下：



*   **感受野等效**：n 个 3×3 卷积核堆叠的感受野 = (2n+1)×(2n+1) 大核的感受野（如 2 个 3×3→7×7，3 个 3×3→9×9）。


    *   示例：3 个 3×3 卷积（步长 1，Same Padding）的感受野计算：
    
        第 1 层 RF=3，第 2 层 RF=3 + (3-1)×1=5，第 3 层 RF=5 + (3-1)×1=7，等效于 1 个 7×7 卷积的感受野（RF=7）。

*   **参数更少**：n 个 3×3 卷积的参数量 = n×(3×3×C²)，1 个 (2n+1)×(2n+1) 卷积的参数量 = (2n+1)²×C²（C 为输入 / 输出通道数）。


    *   示例：C=256，3 个 3×3 卷积参数量 = 3×9×256²=1,769,472；1 个 7×7 卷积参数量 = 49×256²=3,211,264，参数减少 45%。

*   **非线性更强**：每个 3×3 卷积后均接 ReLU 激活，n 个卷积核对应 n 次非线性变换，而大核仅 1 次，使模型能学习更复杂的特征映射。

#### 3. 网络配置：VGG-16 与 VGG-19

VGGNet 有 6 种配置（A、A-LRN、B、C、D、E），其中最常用的是**VGG-16（配置 D）** 和**VGG-19（配置 E）**，命名中的 “16”“19” 指 “权重层数量”（卷积层 + 全连接层）。

两种配置的核心差异是卷积层数量：



*   VGG-16：13 个卷积层（分为 5 组，每组卷积核数量分别为 64、128、256、256、512、512、512、512、512、512、512、512、512）+ 3 个全连接层 = 16 层。

*   VGG-19：16 个卷积层（在 VGG-16 基础上，每组多 1 个 3×3 卷积）+ 3 个全连接层 = 19 层。

两种配置的其他设计完全一致：



*   池化层：5 个 Max Pooling 层（均为 2×2 窗口，步长 2，Same Padding），分别位于每组卷积层之后，用于降采样（每次池化后特征图尺寸减半）。

*   全连接层：3 个 FC 层，神经元数量分别为 4096、4096、1000（与 AlexNet 相同）。

*   激活函数：所有卷积层、全连接层后均接 ReLU（无 LRN，证明 LRN 对 VGGNet 无明显提升）。

### 第 36 页：VGGNet 详解（二）—— 完整架构与性能

#### 1. VGG-16 的完整架构（输入 224×224×3）



| 层组     | 层类型   | 核大小 / 参数                       | 步长（S） | 填充（P） | 输出维度    | 权重层计数 |
| -------- | -------- | ----------------------------------- | --------- | --------- | ----------- | ---------- |
| 第 1 组  | Conv1-1  | 64 个 3×3×3                         | 1         | 1         | 224×224×64  | 1          |
|          | ReLU     | -                                   | -         | -         | 224×224×64  | -          |
|          | Conv1-2  | 64 个 3×3×64                        | 1         | 1         | 224×224×64  | 2          |
|          | ReLU     | -                                   | -         | -         | 224×224×64  | -          |
|          | MaxPool1 | 2×2                                 | 2         | 1         | 112×112×64  | -          |
| 第 2 组  | Conv2-1  | 128 个 3×3×64                       | 1         | 1         | 112×112×128 | 3          |
|          | ReLU     | -                                   | -         | -         | 112×112×128 | -          |
|          | Conv2-2  | 128 个 3×3×128                      | 1         | 1         | 112×112×128 | 4          |
|          | ReLU     | -                                   | -         | -         | 112×112×128 | -          |
|          | MaxPool2 | 2×2                                 | 2         | 1         | 56×56×128   | -          |
| 第 3 组  | Conv3-1  | 256 个 3×3×128                      | 1         | 1         | 56×56×256   | 5          |
|          | ReLU     | -                                   | -         | -         | 56×56×256   | -          |
|          | Conv3-2  | 256 个 3×3×256                      | 1         | 1         | 56×56×256   | 6          |
|          | ReLU     | -                                   | -         | -         | 56×56×256   | -          |
|          | Conv3-3  | 256 个 3×3×256                      | 1         | 1         | 56×56×256   | 7          |
|          | ReLU     | -                                   | -         | -         | 56×56×256   | -          |
|          | MaxPool3 | 2×2                                 | 2         | 1         | 28×28×256   | -          |
| 第 4 组  | Conv4-1  | 512 个 3×3×256                      | 1         | 1         | 28×28×512   | 8          |
|          | ReLU     | -                                   | -         | -         | 28×28×512   | -          |
|          | Conv4-2  | 512 个 3×3×512                      | 1         | 1         | 28×28×512   | 9          |
|          | ReLU     | -                                   | -         | -         | 28×28×512   | -          |
|          | Conv4-3  | 512 个 3×3×512                      | 1         | 1         | 28×28×512   | 10         |
|          | ReLU     | -                                   | -         | -         | 28×28×512   | -          |
|          | MaxPool4 | 2×2                                 | 2         | 1         | 14×14×512   | -          |
| 第 5 组  | Conv5-1  | 512 个 3×3×512                      | 1         | 1         | 14×14×512   | 11         |
|          | ReLU     | -                                   | -         | -         | 14×14×512   | -          |
|          | Conv5-2  | 512 个 3×3×512                      | 1         | 1         | 14×14×512   | 12         |
|          | ReLU     | -                                   | -         | -         | 14×14×512   | -          |
|          | Conv5-3  | 512 个 3×3×512                      | 1         | 1         | 14×14×512   | 13         |
|          | ReLU     | -                                   | -         | -         | 14×14×512   | -          |
|          | MaxPool5 | 2×2                                 | 2         | 1         | 7×7×512     | -          |
| 全连接层 | FC6      | 4096 个神经元（输入 7×7×512=25088） | -         | -         | 4096        | 14         |
|          | ReLU     | -                                   | -         | -         | 4096        | -          |
|          | Dropout  | p=0.5                               | -         | -         | 4096        | -          |
|          | FC7      | 4096 个神经元                       | -         | -         | 4096        | 15         |
|          | ReLU     | -                                   | -         | -         | 4096        | -          |
|          | Dropout  | p=0.5                               | -         | -         | 4096        | -          |
|          | FC8      | 1000 个神经元                       | -         | -         | 1000        | 16         |
|          | Softmax  | -                                   | -         | -         | 1000        | -          |

#### 2. 性能与参数量



*   **ILSVRC 2014 结果**：VGG-16 top-5 错误率 7.3%，VGG-19 top-5 错误率 7.1%，均显著优于 AlexNet（15.4%）。

*   **参数量**：VGG-16 约 1.38 亿参数，VGG-19 约 1.44 亿参数，其中全连接层占比 75%（FC6+FC7+FC8 约 1.02 亿参数），导致模型内存占用高（约 500MB）。

*   **计算量**：VGG-16 单次前向传播约 15×10^9 FLOPs（是 AlexNet 的 10 倍），训练需更强的 GPU 算力（如使用 4 块 GPU 训练 2-3 周）。

#### 3. VGGNet 的优势与局限性



*   **优势**：

1.  架构规整，仅使用 3×3 卷积和 2×2 池化，易于实现和修改。

2.  特征提取能力强，浅层提取边缘、纹理，深层提取物体部件（如车轮、眼睛），适合迁移学习（如用于目标检测、语义分割的 backbone）。

*   **局限性**：

1.  参数量和计算量过大，部署到移动端或嵌入式设备困难。

2.  全连接层灵活性差，需固定输入尺寸（224×224），且易过拟合（需依赖 Dropout）。

### 第 36 页：VGGNet 详解（二）—— 完整架构与性能（补充说明）

#### 3. VGGNet 的迁移学习应用（补充）

VGGNet 因其特征提取能力强、架构规整的特点，成为**迁移学习（Transfer Learning）** 的经典 backbone：



*   **场景 1：小数据集分类**：将 VGG-16 的 FC8 层替换为新的全连接层（输出类别数 = 目标任务类别数），冻结前 13 层卷积权重，仅训练新 FC 层，可快速收敛并避免过拟合。

*   **场景 2：目标检测 / 语义分割**：将 VGG-16 的 MaxPool5 后的 FC6、FC7 层替换为卷积层（称为 “全卷积化”），利用其深层高维特征进行目标定位或像素分类（如 Faster R-CNN、FCN 均以 VGG 为基础）。

### 第 37 页：GoogLeNet 详解（一）—— 背景与 Inception 模块（续）

#### 3. 基础 Inception 模块结构（v1 版本）（续）



1.  **1×1+3×3 卷积分支**：先 1×1 卷积（降维至 C2' 通道），再接 3×3 卷积（步长 1，Same Padding），输出 C2 通道（C2=C2'）。

*   作用：通过 1×1 降维减少 3×3 卷积的计算量。例如，输入 256 通道，若直接用 3×3 卷积输出 64 通道，计算量 = 256×3×3×H×W×64；若先 1×1 降维至 64 通道，再 3×3 卷积输出 64 通道，计算量 = 256×1×1×H×W×64 + 64×3×3×H×W×64，减少约 70% 计算量。

1.  **1×1+5×5 卷积分支**：先 1×1 卷积（降维至 C3' 通道），再接 5×5 卷积（步长 1，Same Padding），输出 C3 通道（C3=C3'）。

*   作用：捕捉大尺度特征（如物体整体轮廓），同时通过 1×1 降维解决 5×5 卷积参数量过大的问题。

1.  **MaxPool+1×1 卷积分支**：先 3×3 MaxPool（步长 1，Same Padding，保持尺寸不变），再接 1×1 卷积（降维至 C4 通道）。

*   作用：保留局部最大特征，同时通过 1×1 卷积调整通道数，与其他分支对齐。

#### 4. Inception 模块的通道配置（v1 示例）

论文中 Inception 模块的典型通道配置（输入 256 通道）：



*   C1=64（1×1 分支），C2'=64→C2=64（1×1+3×3 分支），C3'=48→C3=64（1×1+5×5 分支），C4=64（MaxPool+1×1 分支），总输出通道 = 64+64+64+64=256（与输入通道一致，便于后续堆叠）。

### 第 38 页：GoogLeNet 详解（二）—— 完整架构与性能

#### 1. 完整架构（输入 224×224×3，共 22 层权重层）

GoogLeNet 通过堆叠 Inception 模块构建，为避免梯度消失，引入**辅助分类器（Auxiliary Classifier）**，架构分为 5 个 “模块组”：



| 模块组   | 层类型      | 核大小 / 参数                | 步长（S） | 填充（P） | 输出维度   | 权重层计数     |
| -------- | ----------- | ---------------------------- | --------- | --------- | ---------- | -------------- |
| 初始层   | Conv1       | 64 个 7×7×3                  | 2         | 3         | 112×112×64 | 1              |
|          | ReLU        | -                            | -         | -         | 112×112×64 | -              |
|          | MaxPool1    | 3×3                          | 2         | 1         | 56×56×64   | -              |
| 模块组 1 | Conv2-1     | 64 个 1×1×64                 | 1         | 0         | 56×56×64   | 2              |
|          | ReLU        | -                            | -         | -         | 56×56×64   | -              |
|          | Conv2-2     | 192 个 3×3×64                | 1         | 1         | 56×56×192  | 3              |
|          | ReLU        | -                            | -         | -         | 56×56×192  | -              |
|          | MaxPool2    | 3×3                          | 2         | 1         | 28×28×192  | -              |
| 模块组 2 | Inception3a | 见通道配置（输出 256 通道）  | 1         | -         | 28×28×256  | 4-7（4 层）    |
|          | Inception3b | 见通道配置（输出 480 通道）  | 1         | -         | 28×28×480  | 8-11（4 层）   |
|          | MaxPool3    | 3×3                          | 2         | 1         | 14×14×480  | -              |
| 模块组 3 | Inception4a | 见通道配置（输出 512 通道）  | 1         | -         | 14×14×512  | 12-15（4 层）  |
|          | Inception4b | 见通道配置（输出 512 通道）  | 1         | -         | 14×14×512  | 16-19（4 层）  |
|          | Inception4c | 见通道配置（输出 512 通道）  | 1         | -         | 14×14×512  | 20-23（4 层）  |
|          | Inception4d | 见通道配置（输出 528 通道）  | 1         | -         | 14×14×528  | 24-27（4 层）  |
|          | Inception4e | 见通道配置（输出 832 通道）  | 1         | -         | 14×14×832  | 28-31（4 层）  |
|          | MaxPool4    | 3×3                          | 2         | 1         | 7×7×832    | -              |
| 模块组 4 | Inception5a | 见通道配置（输出 832 通道）  | 1         | -         | 7×7×832    | 32-35（4 层）  |
|          | Inception5b | 见通道配置（输出 1024 通道） | 1         | -         | 7×7×1024   | 36-39（4 层）  |
|          | AvgPool     | 7×7（全局平均池化）          | 1         | -         | 1×1×1024   | -              |
|          | Dropout     | p=0.4                        | -         | -         | 1×1×1024   | -              |
| 输出层   | FC          | 1000 个神经元                | -         | -         | 1000       | 40（第 40 层） |
|          | Softmax     | -                            | -         | -         | 1000       | -              |



*   **辅助分类器**：在 Inception4a 和 Inception4d 后各添加 1 个辅助分类器（结构：AvgPool→Conv→FC→Softmax），训练时将辅助分类器损失（乘以 0.3）与主分类器损失相加，缓解深层网络梯度消失（测试时仅用主分类器）。

#### 2. 性能与参数量



*   **ILSVRC 2014 结果**：top-5 错误率 6.7%（优于 VGG-16 的 7.3%），检测任务 mAP（mean Average Precision）显著优于 VGGNet。

*   **参数量**：仅约 500 万参数（是 AlexNet 的 1/12，VGG-16 的 1/27），计算量约 1.5×10^9 FLOPs（与 AlexNet 相当，远低于 VGG-16 的 15×10^9 FLOPs），实现 “高效高精度”。

#### 3. 优势与局限性



*   **优势**：

1.  Inception 模块多尺度特征融合，提升特征表达能力。

2.  1×1 卷积降维 + 全局平均池化，大幅减少参数量和计算量，便于部署。

*   **局限性**：

1.  模块结构复杂，通道配置需手工设计，灵活性差。

2.  辅助分类器增加训练复杂度，且对性能提升有限（后续版本移除）。

### 第 39 页：网络深度的挑战 —— 梯度消失与退化问题

#### 1. 深度增加的直观预期

理论上，更深的网络应具有更强的表达能力：浅层网络可学习的函数，深层网络通过 “恒等映射”（即深层网络的部分层不改变输入）也能学习；且深层网络可学习更复杂的函数（如多尺度特征组合）。

#### 2. 实际挑战：梯度消失（Vanishing Gradient）



*   **现象**：在反向传播时，梯度从输出层向输入层传递，经过多层非线性激活后，梯度值呈指数级衰减，导致浅层网络的权重几乎无法更新（梯度≈0）。

*   **数学本质**：假设网络每层的梯度传递系数为 α（α<1，如 ReLU 在 x>0 时梯度 = 1，但卷积层权重初始化通常较小，α≈0.9），则经过 L 层后，浅层梯度 =α^L × 输出层梯度。当 L=100 时，α^L≈0.9^100≈2.65×10^-5，梯度几乎消失。

*   **传统解决方案**：

1.  权重初始化（如 Xavier 初始化、He 初始化）：使每层输入输出的方差一致，减缓梯度衰减。

2.  批量归一化（BN）：标准化层输入，使梯度在合理范围内传递。

3.  激活函数优化（如 ReLU 替代 Sigmoid）：减少梯度衰减速率。

#### 3. 新挑战：网络退化（Degradation）



*   **现象**：当网络深度超过一定阈值（如 20 层）后，即使使用 BN 和 He 初始化，**训练误差和测试误差反而上升**（与 “深度增加→性能提升” 的预期相反），称为 “网络退化”。

*   **原因**：深层网络难以学习 “恒等映射”—— 当浅层网络已达到最优性能时，深层网络需学习 “输入 = 输出” 的恒等变换，但传统网络（Plain Network）的线性变换 + 非线性激活难以拟合这种简单映射，导致性能下降。

*   **实验验证**（ResNet 论文）：在 CIFAR-10 数据集上，20 层 Plain Network 测试误差为 8.75%，56 层 Plain Network 测试误差升至 14.88%，证明退化问题真实存在。

### 第 40 页：ResNet 详解（一）—— 核心思想与残差连接

#### 1. ResNet 的背景



*   **发表时间**：2015 年（Kaiming He 等，Microsoft Research），发表于 CVPR 会议。

*   **数据集**：ILSVRC 2015（1000 类）、CIFAR-10/100（小图像数据集）。

*   **竞赛结果**：ILSVRC 2015 分类任务 top-5 错误率 3.57%（首次超过人类水平，人类错误率约 5%），检测、分割任务均获第一名，彻底解决网络退化问题。

#### 2. 核心思想：残差学习（Residual Learning）

传统 Plain Network 学习 “完整映射” H (x) = F (x)（x 为输入，F (x) 为网络拟合的函数）；ResNet 通过**残差连接（Residual Connection）**，学习 “残差映射” F (x) = H (x) - x，即 H (x) = F (x) + x（H (x) 为期望的完整映射，x 为输入的恒等映射）。



*   **优势**：

1.  若 H (x)≈x（恒等映射），则 F (x)≈0，网络仅需学习 “接近零的残差”，比学习完整映射更简单。

2.  梯度可通过残差连接直接传递：$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial H(x)} \cdot \frac{\partial H(x)}{\partial x} = \frac{\partial L}{\partial H(x)} \cdot \left( \frac{\partial F(x)}{\partial x} + 1 \right)$，避免梯度消失（即使$\frac{\partial F(x)}{\partial x}$趋近于 0，$\frac{\partial L}{\partial x}$仍≈$\frac{\partial L}{\partial H(x)}$，梯度可有效传递）。

#### 3. 残差块（Residual Block）结构

ResNet 的基本单元是 “残差块”，分为两种类型：



| 残差块类型                 | 适用场景                            | 结构（输入 x→输出 H (x)）                                    | 关键设计                                                     |
| -------------------------- | ----------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 基本块（Basic Block）      | 浅层 ResNet（如 ResNet-18/34）      | x → Conv1（3×3，C\_out）→ BN → ReLU → Conv2（3×3，C\_out）→ BN → + x（恒等连接）→ ReLU | 输入输出通道数相同，步长 = 1 时，直接恒等连接；步长 = 2 时，需用 1×1 卷积调整 x 的维度 |
| 瓶颈块（Bottleneck Block） | 深层 ResNet（如 ResNet-50/101/152） | x → Conv1（1×1，C\_mid）→ BN → ReLU → Conv2（3×3，C\_mid）→ BN → ReLU → Conv3（1×1，C\_out）→ BN → + x（恒等连接 / 1×1 卷积连接）→ ReLU | 用 1×1 卷积 “降维→卷积→升维”，减少参数量（如 C\_out=256，C\_mid=64，参数量减少约 75%） |



*   **维度匹配**：当残差块输入输出通道数不同或步长≠1 时（如降采样），需用 1×1 卷积调整恒等连接的维度：x → Conv（1×1，C\_out，S=2）→ BN → + F (x)，确保加法操作维度一致。

### 第 41 页：ResNet 详解（二）—— 完整架构与性能

#### 1. 典型 ResNet 配置（输入 224×224×3）

ResNet 有多种配置，核心差异是残差块数量和类型，常用的有 ResNet-18（18 层权重层）、ResNet-34、ResNet-50、ResNet-101、ResNet-152：



| 网络型号   | 残差块类型 | 残差块数量（各模块组） | 总权重层数量 | 参数量（百万） | 输出维度变化（示例 ResNet-50）                               |
| ---------- | ---------- | ---------------------- | ------------ | -------------- | ------------------------------------------------------------ |
| ResNet-18  | 基本块     | \[2,2,2,2]             | 18           | 11.7           | 224×224×3 → 112×112×64 → 56×56×64 → 28×28×128 → 14×14×256 → 7×7×512 → 1×1×512 → 1000 |
| ResNet-34  | 基本块     | \[3,4,6,3]             | 34           | 21.8           | -                                                            |
| ResNet-50  | 瓶颈块     | \[3,4,6,3]             | 50           | 25.6           | -                                                            |
| ResNet-101 | 瓶颈块     | \[3,4,23,3]            | 101          |                |                                                              |



| 网络型号   | 残差块类型 | 残差块数量（各模块组） | 总权重层数量 | 参数量（百万） | 输出维度变化（示例 ResNet-50）                               |
| ---------- | ---------- | ---------------------- | ------------ | -------------- | ------------------------------------------------------------ |
| ResNet-101 | 瓶颈块     | \[3,4,23,3]            | 101          | 44.5           | 224×224×3 → Conv1（7×7,64,S=2）→112×112×64 → MaxPool（3×3,S=2）→56×56×64 → 模块组 1（3 个瓶颈块，64→64）→56×56×64 → 模块组 2（4 个瓶颈块，64→128,S=2）→28×28×128 → 模块组 3（23 个瓶颈块，128→256,S=2）→14×14×256 → 模块组 4（3 个瓶颈块，256→512,S=2）→7×7×512 → AvgPool（7×7）→1×1×512 → FC→1000 |
| ResNet-152 | 瓶颈块     | \[3,8,36,3]            | 152          | 60.4           | -                                                            |

#### 2. ResNet-50 的关键层设计（补充）



*   **初始层**：Conv1（7×7 卷积，64 个核，步长 2，填充 3）→ BN → ReLU → MaxPool（3×3，步长 2，填充 1），将 224×224×3 输入转为 56×56×64，快速缩小尺寸并提取低级特征。

*   **模块组 1（conv2\_x）**：3 个瓶颈块，输入输出通道 64，步长 1（无降采样），保持 56×56 尺寸，通过 1×1→3×3→1×1 卷积堆叠提取中级特征。

*   **模块组 2-4（conv3\_x\~conv5\_x）**：每个模块组第一个瓶颈块步长 = 2（降采样），通道数翻倍（64→128→256→512），尺寸减半（56→28→14→7），逐步扩大感受野并提升特征抽象度。

*   **输出层**：全局平均池化（7×7→1×1）→ 全连接层（512→1000）→ Softmax，避免全连接层参数量过大（仅 512×1000=51.2 万参数）。

#### 3. 性能表现



*   **ILSVRC 2015 结果**：ResNet-50 top-5 错误率 3.57%，ResNet-152 top-5 错误率 3.31%，首次超越人类在该任务上的错误率（约 5%）。

*   **CIFAR-10 结果**：ResNet-110 测试错误率 6.43%，远低于 20 层 Plain Network 的 8.75%，证明残差连接有效解决退化问题。

*   **迁移学习能力**：ResNet 的深层特征具有强泛化性，成为目标检测（如 Faster R-CNN、YOLO）、语义分割（如 FCN、Mask R-CNN）的主流 backbone，性能提升显著。

### 第 42 页：CNN 为何优于 MLP（多层感知机）—— 核心特性对比

#### 1. 空间信息利用：CNN 保留结构，MLP 丢失结构



*   **MLP 的局限**：输入图像需拉伸为一维向量（如 32×32×3 图像→3072 维向量），破坏像素间的空间关联（如 “猫的眼睛在耳朵下方” 的位置关系被转化为线性序列），无法捕捉局部特征（如边缘、纹理）的空间依赖。

*   **CNN 的优势**：通过局部连接（卷积核仅覆盖局部区域）和空间滑动，保留图像的二维结构，直接学习像素间的空间关系（如水平边缘由相邻像素的灰度差异构成），更符合视觉数据的本质特性。

#### 2. 参数效率：CNN 权重共享，MLP 参数爆炸



*   **MLP 的参数计算**：若输入为 3072 维，隐藏层为 1000 个神经元，则全连接层参数 = 3072×1000 + 1000=3.073×10^6；若叠加多层，参数呈指数级增长（如 3 层 MLP 参数可能超 1 亿），易过拟合且训练困难。

*   **CNN 的参数计算**：以 32×32×3 输入、6 个 5×5 卷积核为例，卷积层参数 = 6×(3×5×5 + 1)=6×76=456，仅为 MLP 的 1/6700；即使堆叠 10 层卷积，总参数仍远低于同等深度的 MLP，实现 “高效特征学习”。

#### 3. 特征层级提取：CNN 逐步抽象，MLP 单一映射



*   **MLP 的特征学习**：通过全连接层直接将原始像素映射到类别，缺乏中间特征的层级抽象，难以学习复杂的视觉模式（如 “车轮→车身→汽车” 的特征组合）。

*   **CNN 的特征学习**：浅层卷积层提取低级特征（边缘、颜色、纹理），中层组合为中级特征（角点、部件、纹理组合），深层抽象为高级特征（物体整体形状、类别特征），符合人类视觉系统的 “层级加工” 机制，能高效学习复杂视觉概念。

#### 4. 平移不变性：CNN 天然具备，MLP 需额外学习



*   **MLP 的局限**：若图像中的物体平移（如猫从左移到右），拉伸后的向量中像素位置完全改变，MLP 需重新学习该平移后的特征，泛化能力差。

*   **CNN 的优势**：卷积核在图像上滑动时权重共享，同一特征（如边缘）在不同位置被同一卷积核检测，配合池化层的降采样，使 CNN 对物体的微小平移、缩放具有天然的鲁棒性，无需额外学习平移变换。

### 第 43 页：生成模型简介 —— 从判别到生成的视觉任务扩展

#### 1. 视觉任务的两大分支：判别任务与生成任务



*   **判别任务（Discriminative Task）**：学习从输入数据（图像）到标签（类别、边界框）的映射，核心是 “区分不同类别或预测属性”，如分类（Classification）、目标检测（Object Detection）、语义分割（Semantic Segmentation），CNN 是判别任务的主流模型。

*   **生成任务（Generative Task）**：学习输入数据（图像）的概率分布，核心是 “从分布中采样生成新数据”，目标是生成与真实数据相似的、全新的样本，如图像生成（Image Generation）、风格迁移（Style Transfer）、图像修复（Image Inpainting）。

#### 2. 生成模型的核心目标

生成模型的数学目标是**拟合真实数据分布 p\_data (x)**，即通过模型参数 θ 学习一个近似分布 p\_θ(x)，使得 p\_θ(x) 与 p\_data (x) 的差异最小化（常用 KL 散度、JS 散度衡量）。



*   **关键能力**：

1.  采样（Sampling）：从 p\_θ(x) 中随机采样，生成全新的、符合真实数据特征的样本（如生成不存在的人脸、风景）。

2.  重构（Reconstruction）：将输入数据 x 通过编码器映射到潜在空间（Latent Space），再通过解码器重构回 x'，使 x' 与 x 尽可能相似（如 VAE 的重构损失）。

#### 3. 生成模型的应用场景



*   **艺术创作**：文本到图像生成（如 DALL・E、MidJourney）、风格迁移（将照片转为梵高画风）、AI 绘画。

*   **数据增强**：生成标注困难的稀缺数据（如医学影像中的罕见病例图像），扩大训练集，提升判别模型泛化能力。

*   **图像修复与编辑**：去除图像中的水印、修复老照片破损区域、根据文本指令编辑图像内容（如 “给猫添加帽子”）。

*   **3D 生成**：从 2D 图像生成 3D 模型（如 EG3D 生成高质量 3D 人脸）、文本到 3D 资产生成（如用于游戏、影视建模）。

#### 4. 典型生成模型家族



*   **生成对抗网络（GAN，Generative Adversarial Networks）**：通过生成器与判别器的博弈训练，生成高真实度样本，2014 年由 Ian Goodfellow 提出。

*   **变分自编码器（VAE，Variational Autoencoders）**：基于概率图模型，通过变分推断学习潜在空间，生成样本具有较好的多样性。

*   **扩散模型（Diffusion Models）**：通过逐步添加噪声再逆转过程学习数据分布，2020 年后成为主流，生成质量超越 GAN（如 DALL・E 2、Stable Diffusion）。

### 第 44 页：生成对抗网络（GAN）的核心原理 ——Minimax 博弈

#### 1. GAN 的基本框架

GAN 由两个相互对抗的神经网络组成，通过 “零和博弈” 实现共同优化：



*   **生成器（Generator，G）**：输入随机噪声 z（通常服从正态分布 N (0,1)），输出伪造数据 G (z)（如伪造的图像），目标是让 G (z) 尽可能接近真实数据 x，欺骗判别器。

*   **判别器（Discriminator，D）**：输入数据（真实数据 x 或伪造数据 G (z)），输出该数据为真实数据的概率 D (x)（或 D (G (z))），目标是准确区分真实数据和伪造数据，不被生成器欺骗。

#### 2. 博弈过程的数学描述

GAN 的训练目标是求解**Minimax 优化问题**，目标函数如下：

$\min_{G} \max_{D} V(D, G) = \mathbb{E}_{x \sim p_{data}(x)} [\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(z)))]$



*   符号说明：$\mathbb{E}$ 表示期望，$x \sim p_{data}(x)$ 表示从真实数据分布中采样，$z \sim p_{z}(z)$ 表示从噪声分布中采样，$V(D,G)$ 为判别器与生成器的价值函数。

#### 3. 判别器的优化目标（Maximize V (D,G)）

判别器的目标是最大化价值函数，即：



*   对真实数据 x，最大化$\log D(x)$ → 使 D (x) 尽可能接近 1（判断真实数据为 “真”）。

*   对伪造数据 G (z)，最大化$\log (1 - D(G(z)))$ → 使 D (G (z)) 尽可能接近 0（判断伪造数据为 “假”）。

*   最优判别器$D^*$：当生成器 G 固定时，最优判别器的输出为$D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_{g}(x)}$，其中$p_g(x)$为生成器生成的数据分布（证明见第 45 页）。

#### 4. 生成器的优化目标（Minimize V (D,G)）

生成器的目标是最小化价值函数，即：



*   最小化$\mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(z)))]$ → 使 D (G (z)) 尽可能接近 1（让判别器将伪造数据误判为 “真”）。

*   训练初期的改进：当 G 生成的样本质量较差时，D (G (z))≈0，$\log (1 - D(G(z)))≈0$，梯度接近 0，训练缓慢。因此，实际中常用**替换目标函数**：$\max_{G} \mathbb{E}_{z \sim p_{z}(z)} [\log D(G(z))]$，通过最大化判别器对伪造数据的 “真实概率”，提供更强的梯度信号，加速训练。

### 第 45 页：GAN 的最优判别器与训练步骤

#### 1. 最优判别器$D^*$的推导

假设生成器 G 固定，需找到使 V (D,G) 最大的判别器 D。将价值函数改写为积分形式（连续数据分布）：

$V(D,G) = \int_{x} p_{data}(x) \log D(x) dx + \int_{z} p_{z}(z) \log (1 - D(G(z))) dz$

通过变量替换（令$x = G(z)$，则$dz = \frac{dx}{|det \nabla G(z)|}$），第二项可改写为$\int_{x} p_{g}(x) \log (1 - D(x)) dx$，其中$p_g(x) = p_{z}(G^{-1}(x)) \cdot |det \nabla G(G^{-1}(x))|^{-1}$为生成数据的分布。

此时价值函数变为：

$V(D,G) = \int_{x} [p_{data}(x) \log D(x) + p_{g}(x) \log (1 - D(x))] dx$

对 D (x) 求导并令导数为 0，解得最优判别器：

$D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_{g}(x)}$



*   结论：当$p_g(x) = p_{data}(x)$（生成分布与真实分布一致）时，$D^*(x) = 0.5$，判别器无法区分真实与伪造数据，GAN 达到纳什均衡（Nash Equilibrium）。

#### 2. GAN 的训练步骤（迭代优化）

GAN 采用 “交替训练” 策略，即固定一个网络，更新另一个网络的参数，重复迭代直至收敛，具体步骤如下：

**步骤 1：初始化参数**



*   初始化生成器 G 的参数 θ\_G（如卷积核权重、偏置）和判别器 D 的参数 θ\_D。

*   设置超参数：批量大小 N、学习率 η、迭代次数 T、噪声维度 z\_dim。

**步骤 2：迭代训练（共 T 轮）**

For t = 1 to T:

**子步骤 2.1：训练判别器（更新 θ\_D，k 轮）**



1. 从真实数据分布$p_{data}(x)$中采样 N 个样本：$x^{(1)}, x^{(2)}, ..., x^{(N)}$。

2. 从噪声分布$p_{z}(z)$中采样 N 个噪声向量：$z^{(1)}, z^{(2)}, ..., z^{(N)}$，生成伪造样本：$g^{(i)} = G(z^{(i)})$（i=1..N）。

3. 计算判别器的损失函数（基于二元交叉熵）：

   $L_D = -\frac{1}{N} \sum_{i=1}^{N} [\log D(x^{(i)}) + \log (1 - D(g^{(i)}))]$

4. 计算$L_D$对 θ\_D 的梯度，使用梯度上升（或 Adam 优化器）更新 θ\_D（目标是最大化 V (D,G)，即最小化 - L\_D）。

5. 重复 k 次（通常 k=1，即每轮迭代训练 1 次判别器）。

**子步骤 2.2：训练生成器（更新 θ\_G，1 轮）**



1. 从噪声分布$p_{z}(z)$中采样 N 个噪声向量：$z^{(1)}, ..., z^{(N)}$，生成伪造样本：$g^{(i)} = G(z^{(i)})$。

2. 计算生成器的损失函数（替换目标）：

   $L_G = -\frac{1}{N} \sum_{i=1}^{N} \log D(g^{(i)})$

3. 计算$L_G$对 θ\_G 的梯度，使用梯度下降（或 Adam 优化器）更新 θ\_G（目标是最小化 L\_G，即最大化$\log D(g^{(i)})$）。

**步骤 3：生成样本**

训练结束后，从$p_{z}(z)$中采样噪声 z，输入生成器 G，得到最终的生成样本 G (z)。

#### 3. 训练稳定性问题（初始 GAN 的挑战）



*   **模式崩溃（Mode Collapse）**：生成器仅生成少数几种类型的样本（如仅生成笑脸人脸），无法覆盖真实数据的所有模式，原因是判别器对部分样本过度惩罚，导致生成器 “规避风险”。

*   **梯度消失**：训练后期，若判别器过强（D (G (z))≈0），生成器损失$L_G = -\log D(G(z))$的梯度接近 0，无法更新参数。

*   **训练不稳定**：Minimax 博弈的动态平衡难以维持，参数更新易导致一方过强，另一方无法学习，需精细调整学习率、批量大小等超参数。





*   **解决方案初探**：

1.  **调整网络结构**：使用卷积层替代全连接层（如 DCGAN，Deep Convolutional GAN），利用权重共享和局部连接提升特征提取能力，减少模式崩溃。

2.  **优化器改进**：使用 Adam 优化器（学习率通常设为 0.0002）替代 SGD，通过动量和自适应学习率缓解梯度波动，提升训练稳定性。

3.  **数据预处理**：对输入数据进行标准化（如将像素值归一化到 \[-1,1]），使判别器和生成器的输入分布更稳定，避免梯度异常。

### 第 46 页：GAN 的经典变体 ——DCGAN 与 CGAN

#### 1. DCGAN（Deep Convolutional GAN，深度卷积 GAN）



*   **发表时间**：2015 年（Alec Radford 等），首次将卷积神经网络与 GAN 结合，解决了传统 GAN 生成图像模糊、训练不稳定的问题。

*   **核心结构设计**：



| 网络部分 | 关键设计                                                     | 示例（生成器，输入 100 维噪声）                              |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 生成器 G | 1. 无全连接层，全用转置卷积（Transposed Convolution）升维；2. 转置卷积后接 BN（批量归一化），除输出层外；3. 激活函数用 ReLU（除输出层用 Tanh，将像素值映射到 \[-1,1]） | 100×1 → 转置 Conv（1024×4×4）→ BN → ReLU → 转置 Conv（512×8×8）→ BN → ReLU → ... → 转置 Conv（3×64×64）→ Tanh → 输出 64×64×3 图像 |
| 判别器 D | 1. 无全连接层，全用普通卷积降维；2. 卷积后接 BN（除输入层外）；3. 激活函数用 Leaky ReLU（避免死亡 ReLU） | 64×64×3 → Conv（64×32×32）→ BN → Leaky ReLU → Conv（128×16×16）→ BN → Leaky ReLU → ... → Conv（1×1×1）→ Sigmoid → 输出真实概率 |



*   **创新点与效果**：

1.  转置卷积实现 “从噪声到图像” 的端到端生成，生成 64×64 分辨率的清晰图像（如人脸、风景）。

2.  BN 的引入使梯度传递更稳定，大幅减少模式崩溃，训练收敛速度提升 30% 以上。

3.  生成器的潜在空间具有 “可解释性”：通过插值噪声向量，可生成平滑过渡的图像（如从 “微笑人脸” 过渡到 “皱眉人脸”）。

#### 2. CGAN（Conditional GAN，条件 GAN）



*   **核心思想**：在 GAN 的基础上引入 “条件信息”（如类别标签、文本描述、边缘图像），使生成器能定向生成特定类型的样本，解决传统 GAN “生成内容不可控” 的问题。

*   **结构改进**：

1.  生成器 G：输入为 “噪声 z + 条件信息 y”（如将 10 维类别标签 one-hot 编码后与 100 维噪声拼接为 110 维向量），输出条件化伪造样本 G (z|y)。

2.  判别器 D：输入为 “数据 x + 条件信息 y”，输出该数据在条件 y 下为真实数据的概率 D (x|y)。

* **目标函数调整**：

  $\min_{G} \max_{D} V(D, G) = \mathbb{E}_{x \sim p_{data}(x)} [\log D(x|y)] + \mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(z|y)|y))]$

* **应用场景**：

1.  条件图像生成：输入 “猫” 的标签，生成猫的图像；输入 “红色汽车” 的文本，生成红色汽车图像。

2.  图像修复：输入 “破损图像 + 完整区域边缘”，生成修复后的完整图像。

3.  风格迁移：输入 “内容图像 + 风格标签”，生成指定风格的迁移图像。

### 第 47 页：GAN 的进阶变体 ——CycleGAN 与 Pix2Pix

#### 1. Pix2Pix（Pixel-to-Pixel GAN）



*   **任务定位**：解决 “有配对数据的图像到图像转换” 问题（如 “边缘图→照片”“灰度图→彩色图”“卫星图→地图”），核心是 “监督式图像转换”。

*   **核心结构：U-Net 生成器 +  PatchGAN 判别器**：

1.  **U-Net 生成器**：采用编码器 - 解码器结构，编码器通过卷积降维提取特征，解码器通过转置卷积升维恢复尺寸，同时引入 “跳跃连接”（将编码器的浅层特征直接传递到解码器），保留图像细节（如边缘、纹理）。

2.  **PatchGAN 判别器**：不判断整个图像的真实性，而是将图像分割为多个 “patch”（如 70×70 像素块），判断每个 patch 的真实性，再取平均值作为最终输出。这种设计使判别器更关注局部细节，生成图像的纹理更真实。

*   **损失函数**：

1.  **对抗损失（Adversarial Loss）**：与传统 GAN 一致，确保生成图像被判别器误判为真实。

2.  **L1 重构损失（L1 Reconstruction Loss）**：$L_{L1} = \mathbb{E}_{(x,y) \sim p_{data}} [\|G(x) - y\|_1]$，确保生成图像 G (x) 与真实目标 y 在像素层面尽可能相似（避免生成图像与目标无关）。

*   **效果示例**：


    *   输入手绘的鞋子边缘图，生成照片级真实的鞋子图像；输入建筑线框图，生成 3D 渲染风格的建筑图。

#### 2. CycleGAN（Cycle-Consistent GAN）



* **任务定位**：解决 “无配对数据的图像到图像转换” 问题（如 “马→斑马”“夏天风景→冬天风景”“照片→梵高画风”），核心是 “无监督式跨域转换”，无需人工标注配对样本。

* **核心创新：循环一致性损失（Cycle Consistency Loss）**：

  假设存在两个域 X（如马的图像）和 Y（如斑马的图像），CycleGAN 包含两个生成器（G: X→Y，将马转为斑马；F: Y→X，将斑马转为马）和两个判别器（D\_Y: 判断图像是否为真实 Y 域图像；D\_X: 判断图像是否为真实 X 域图像）。

  为确保转换的 “可逆性”（马→斑马→马应与原马一致），引入循环一致性损失：

  $L_{cycle} = \mathbb{E}_{x \sim p_{data}(x)} [\|F(G(x)) - x\|_1] + \mathbb{E}_{y \sim p_{data}(y)} [\|G(F(y)) - y\|_1]$

* **总损失函数**：

  $L_{total} = L_{adv}(G, D_Y) + L_{adv}(F, D_X) + \lambda L_{cycle}$

  （λ 为超参数，通常设为 10，平衡对抗损失和循环损失）

* **效果与局限**：


    *   成功实现无配对跨域转换，如将普通照片转为莫奈、梵高风格的画作，将马的图像转为斑马图像（保留姿态，替换纹理）。
    
    *   局限：对结构差异大的域（如 “猫→狗”）转换效果较差，易出现 “结构扭曲”（如猫的耳朵被转为狗耳朵时形状异常）。

### 第 48 页：扩散模型（Diffusion Models）—— 超越 GAN 的生成模型

#### 1. 扩散模型的核心思想

扩散模型受 “热力学扩散过程” 启发，通过**正向扩散（逐步添加噪声）** 和**反向扩散（逐步去除噪声）** 学习数据分布：



* **正向扩散（Forward Diffusion）**：从真实数据 x\_0 出发，在 T 步内逐步向数据中添加高斯噪声，最终得到纯噪声 x\_T（服从 N (0,I)），每一步噪声添加满足：

  $x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_t$

  其中$\epsilon_t \sim N(0,I)$为第 t 步的噪声，$\alpha_t \in (0,1)$为噪声系数（通常从 0.999 递减到 0.99），确保逐步添加噪声且不破坏数据结构。

* **反向扩散（Reverse Diffusion）**：从纯噪声 x\_T 出发，学习一个神经网络$\epsilon_\theta(x_t, t)$（称为 “噪声预测器”），预测第 t 步数据中的噪声$\epsilon_t$，并通过以下公式反向恢复真实数据 x\_0：

  $x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sigma_t z$

  其中$\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$为累积噪声系数，$\sigma_t$为方差，$z \sim N(0,I)$为随机噪声（确保生成样本的多样性）。

#### 2. 扩散模型与 GAN 的核心差异



| 对比维度   | 扩散模型（Diffusion Models）                      | 生成对抗网络（GAN）                        |
| ---------- | ------------------------------------------------- | ------------------------------------------ |
| 训练方式   | 基于 MSE 损失的监督式学习（预测噪声）             | 基于 Minimax 博弈的对抗式学习              |
| 生成质量   | 高分辨率（可达 1024×1024），细节丰富，无模式崩溃  | 中低分辨率（通常≤256×256），易出现模式崩溃 |
| 训练稳定性 | 稳定，无对抗博弈的动态平衡问题                    | 不稳定，需精细调整超参数                   |
| 计算效率   | 训练慢（需 T=1000 步扩散），推理慢（需 T 步反向） | 训练快，推理快（1 步生成）                 |
| 潜在空间   | 连续可解释，插值效果好                            | 不连续，插值易出现异常样本                 |

#### 3. 扩散模型的经典应用



*   **DALL·E 2**：OpenAI 提出，结合文本编码器（CLIP）和扩散模型，实现 “文本到高分辨率图像” 生成（如输入 “一只穿着西装的企鹅在太空站喝咖啡”，生成对应图像）。

*   **Stable Diffusion**：开源扩散模型，通过 “latent diffusion”（在潜在空间而非像素空间扩散），将推理速度提升 10 倍，可在消费级 GPU 上实时生成 512×512 图像。

*   **图像修复与超分辨率**：利用反向扩散过程，对破损图像或低分辨率图像逐步去噪，生成完整高分辨率图像（修复效果优于 GAN，如去除老照片的划痕并提升分辨率）。

### 第 49 页：变分自编码器（VAE）—— 基于概率的生成模型

#### 1. VAE 的基本框架

VAE 是结合 “自编码器（Autoencoder）” 和 “变分推断（Variational Inference）” 的生成模型，核心是学习数据的概率分布并生成新样本，同时具备 “重构” 和 “采样” 能力。



*   **结构组成**：

1.  **编码器（Encoder）**：输入真实数据 x，输出潜在变量 z 的概率分布参数（均值 μ(x) 和方差 σ²(x)），即 z \~ q\_θ(z|x)（通常假设为正态分布 N (μ(x), σ²(x) I)）。

2.  **解码器（Decoder）**：输入潜在变量 z，输出数据 x 的概率分布参数（如对图像数据，输出每个像素的均值，假设 x \~ p\_φ(x|z) 为伯努利分布或正态分布）。

* **核心目标：最大化证据下界（ELBO，Evidence Lower Bound）**：

  由于直接最大化$\log p_φ(x)$（数据 x 的边缘概率）难以计算，VAE 通过最大化 ELBO 间接优化：

  $\text{ELBO}(\theta, \phi; x) = \mathbb{E}_{q_θ(z|x)} [\log p_φ(x|z)] - D_{KL}(q_θ(z|x) \| p(z))$


    *   第一项$\mathbb{E}[\log p_φ(x|z)]$：重构损失（Reconstruction Loss），确保解码器能从 z 重构回 x（与自编码器类似）。
    
    *   第二项$-D_{KL}(q\|p)$：KL 散度损失（KL Divergence Loss），约束编码器输出的分布 q\_θ(z|x) 接近先验分布 p (z)（通常设为 N (0,I)），确保潜在空间的平滑性和可采样性。

#### 2. 重参数化技巧（Reparameterization Trick）

为解决 “潜在变量 z 是随机变量，无法直接求导” 的问题，VAE 引入重参数化：



* 将 z 表示为 “确定性函数 + 随机噪声”：$z = \mu(x) + \sigma(x) \cdot \epsilon$，其中$\epsilon \sim N(0,I)$为独立于模型参数的噪声。

* 此时，ELBO 对参数 θ 和 φ 的梯度可通过$\epsilon$传递，即：

  $\nabla_{\theta,\phi} \text{ELBO} = \nabla_{\theta,\phi} \mathbb{E}_\epsilon [\log p_φ(x|\mu(x) + \sigma(x)\epsilon) - D_{KL}(q_θ\|p)]$

  通过采样多个$\epsilon$并计算梯度均值，实现参数更新。

#### 3. VAE 与 GAN、扩散模型的对比



| 模型类型 | 核心优势                                                     | 核心劣势                                                     | 适用场景                                                     |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| VAE      | 1. 训练稳定，无对抗博弈问题；2. 潜在空间连续可解释，支持插值；3. 可量化生成样本的概率 | 1. 生成样本质量较低（模糊，细节少）；2. 重构损失导致生成样本偏向 “平均化”（如生成的人脸缺乏个性） | 1. 数据压缩与重构；2. 潜在空间分析；3. 小数据集的生成任务    |
| GAN      | 1. 生成样本质量高（纹理清晰）；2. 推理速度快（1 步生成）     | 1. 训练不稳定，易模式崩溃；2. 潜在空间不连续，插值效果差     | 1. 高真实度图像生成；2. 图像风格迁移；3. 无配对图像转换      |
| 扩散模型 | 1. 生成质量最高（高分辨率，细节丰富）；2. 训练稳定，无模式崩溃；3. 支持文本等条件生成 | 1. 训练和推理速度慢（需千步扩散）；2. 计算资源消耗大（需大显存 GPU） | 1. 文本到高分辨率图像生成；2. 图像修复与超分辨率；3. 专业艺术创作 |

### 第 50 页：生成模型的评估指标

#### 1. 主观评估指标



*   **人工评分（Human Evaluation）**：邀请人类评委对生成样本的 “真实度”“多样性”“与条件的匹配度” 打分（如 1-5 分制），是最直接的评估方式，但成本高、主观性强。

*   **用户研究（User Study）**：设计对比实验，让用户在 “真实样本” 和 “生成样本” 中选择 “更真实的样本”，计算生成样本被误判为真实的比例（称为 “欺骗率”，Fooling Rate），欺骗率越高，生成质量越好。

#### 2. 客观评估指标



*   **Inception Score（IS， inception 分数）**：

1.  原理：利用预训练的 Inception-V3 分类模型，计算生成样本的两个指标：

*   类别分布的边际熵 H (p (y))：衡量生成样本的多样性（熵越大，多样性越高）。

*   条件熵 H (p (y|x))：衡量生成样本的类别清晰度（熵越小，类别越清晰）。

1.  计算方式：$IS = \exp(\mathbb{E}_x [H(p(y|x)) - H(p(y))])$，IS 值越高，生成样本的 “多样性” 和 “清晰度” 平衡越好

*   3\. 局限性：


    *   依赖 Inception-V3 的分类能力，对非自然图像（如艺术画、抽象图）评估不准确（分类模型难以给此类图像分配清晰类别）。
    
    *   仅关注 “类别多样性” 和 “清晰度”，无法评估生成样本的 “结构合理性”（如生成的 “人脸” 眼睛位置异常，IS 值仍可能较高）。
    
    *   对样本数量敏感，需至少 5000 个生成样本才能保证评估稳定性。

*   **Fréchet Inception Distance（FID，弗雷歇 inception 距离）**：

1. 原理：基于 “生成样本分布” 与 “真实样本分布” 在 Inception-V3 特征空间中的 “Fréchet 距离”（衡量两个高斯分布的相似性），公式如下：

   $FID = \| \mu_r - \mu_g \|_2^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2\sqrt{\Sigma_r \Sigma_g})$

   其中$\mu_r、\Sigma_r$为真实样本在 Inception-V3 特征层的均值和协方差矩阵，$\mu_g、\Sigma_g$为生成样本的均值和协方差矩阵；$\text{Tr}$为矩阵的迹（对角线元素之和）。

2. 解读：FID 值越小，说明生成样本分布与真实样本分布越接近，生成质量越好（通常自然图像生成任务中，FID<100 为良好，FID<50 为优秀）。

3. 优势：

*   不依赖分类标签，对非自然图像评估更准确。

*   能捕捉样本的 “结构特征”（如人脸的器官位置、物体的形状比例），评估更全面。

1.  局限性：计算复杂度高（需计算高维协方差矩阵），且对特征层选择敏感（通常选择 Inception-V3 的倒数第二层特征）。

*   **Precision and Recall（精确率与召回率）**：

1.  原理：将生成样本和真实样本映射到特征空间后，定义：

*   **精确率（Precision）**：生成样本中 “落在真实样本分布内” 的比例（衡量生成样本的 “真实性”，比例越高，生成样本越接近真实）。

*   **召回率（Recall）**：真实样本分布中 “被生成样本覆盖” 的比例（衡量生成样本的 “多样性”，比例越高，生成样本覆盖的真实模式越全面）。

1.  应用：通过 Precision-Recall 曲线综合评估生成模型的 “真实性” 和 “多样性”，避免单一指标的片面性（如 GAN 易出现高 Precision、低 Recall，即真实但多样性差）。

### 第 51 页：GAN 的最新变体（2018-2023）

#### 1. StyleGAN（Style Generative Adversarial Network）



*   **发表时间**：2018 年（NVIDIA），2020 年推出 StyleGAN2，2022 年推出 StyleGAN3，是目前生成高质量人脸、人体图像的主流模型。

*   **核心创新：风格控制与渐进式训练**：

1.  **风格注入（Style Injection）**：生成器引入 “风格向量”（Style Vector），通过 AdaIN（Adaptive Instance Normalization）将风格向量注入到生成器的不同层级：

* 浅层注入：控制图像的 “全局风格”（如亮度、色彩基调）。

* 深层注入：控制图像的 “局部细节”（如人脸的发型、眼睛形状）。

  实现 “精细化风格控制”，如生成 “金发、微笑、戴眼镜” 的特定人脸。

1.  **渐进式训练（Progressive Growing）**：从低分辨率（4×4）开始训练，逐步增加生成图像的分辨率（8×8→16×16→...→1024×1024），每阶段仅训练新增的高分辨率层，确保低分辨率结构稳定后再优化细节，生成 1024×1024 超高清图像。

*   **效果**：生成的人脸图像在 FID 指标上达到 1.8（当时最优），可通过调整风格向量实现 “表情编辑”“姿态调整”“属性修改”（如将人脸从 “无胡子” 改为 “有胡子”）。

#### 2. StyleGAN3（2022）：解决姿态扭曲问题



*   **核心改进**：引入 “equivariant normalization”（等变归一化），使生成器对图像的 “旋转、缩放、平移” 具有等变性，解决 StyleGAN2 中生成图像 “姿态扭曲” 的问题（如生成的人体手臂比例异常、人脸倾斜）。

*   **应用**：生成高质量的人体、动物图像，支持 “姿态控制”（如生成 “站立”“坐姿” 的人体，姿态自然无扭曲）。

#### 3. GigaGAN（2023，NVIDIA）：超大规模 GAN



*   **核心突破**：通过 “混合专家模型（Mixture of Experts，MoE）” 和 “分层生成”，实现 4K 分辨率图像的实时生成（之前模型生成 4K 图像需分钟级，GigaGAN 仅需秒级）。

*   **结构设计**：

1.  MoE 生成器：将生成器分为多个 “专家子网络”，每个子网络负责生成特定类型的特征（如纹理、结构），动态选择相关专家参与计算，兼顾效率和质量。

2.  分层生成：先生成 64×64 的低分辨率图像，再通过 “超分辨率专家” 逐步放大到 4K，确保细节丰富且无模糊。

*   **效果**：生成 4K 图像的 FID 值低至 2.2，推理速度比 StyleGAN2 快 50 倍，支持 “文本到 4K 图像” 生成（如输入 “夕阳下的海滩，有白色帆船和棕榈树”，生成 4K 高清图像）。

### 第 52 页：扩散模型的进阶发展（2021-2023）

#### 1. 潜在扩散模型（Latent Diffusion Models，LDMs）



*   **核心问题**：传统扩散模型在像素空间扩散，计算量大（1024×1024 图像需处理百万级像素），训练和推理速度慢。

*   **解决方案**：在 “潜在空间”（而非像素空间）进行扩散：

1.  用预训练的自动编码器（如 VAE）将图像压缩到低维潜在空间（如 1024×1024 图像→64×64 潜在向量），扩散过程仅在潜在空间进行，计算量减少约 1000 倍。

2.  生成阶段：先在潜在空间完成反向扩散，再通过解码器将潜在向量恢复为高分辨率图像。

*   **代表模型**：Stable Diffusion（2022，Stability AI），开源后成为最流行的扩散模型，支持：


    *   文本到图像生成（512×512 图像，消费级 GPU 推理时间 < 10 秒）。
    
    *   图像编辑（如 “修复”“超分辨率”“风格迁移”）。
    
    *   插件扩展（如 ControlNet，通过边缘图、深度图控制生成图像的结构）。

#### 2. 高效扩散模型：加速推理



*   **核心改进方向**：减少反向扩散的步数（T），在保证生成质量的前提下提升速度：

1.  **知识蒸馏（Knowledge Distillation）**：用大模型（T=1000）蒸馏小模型（T=50），使小模型在少步数下达到接近大模型的质量（如 Fast Diffusion，T=50，速度提升 20 倍）。

2.  **自适应步长（Adaptive Steps）**：根据噪声强度动态调整步数（噪声大时多步去噪，噪声小时少步去噪），平均步数减少至 20-30（如 DDIM，Denoising Diffusion Implicit Models）。

3.  **模型压缩**：用轻量级网络（如 MobileNet）替代原有的 UNet 噪声预测器，减少参数数量（如 MobileDiffusion，参数量减少 70%，推理速度提升 3 倍）。

#### 3. 条件扩散模型的扩展



*   **文本条件**：结合大语言模型（如 CLIP、GPT），提升文本理解能力，支持更复杂的文本描述（如 “一只穿着红色斗篷的猫，坐在中世纪城堡的窗台上，窗外有月亮和星星”）。

*   **图像条件**：支持 “图像到图像” 转换（如 “草图→照片”“低清→高清”“白天→黑夜”），通过 “条件注入” 将输入图像特征融入扩散过程，确保生成图像与输入结构一致。

*   **3D 条件**：输入 3D 模型的点云或深度图，生成 3D 渲染图像（如 DreamFusion，2022，Google），实现 “3D 到 2D” 的真实感渲染。

### 第 53 页：生成模型的产业应用

#### 1. 数字内容创作



*   **游戏与影视**：


    *   自动生成游戏场景（如森林、城堡）、角色皮肤、道具模型，减少美术制作成本（如 NVIDIA 的 GameGAN，生成的场景可直接用于游戏引擎）。
    
    *   影视特效：生成 “虚拟背景”“ crowds（人群）”“爆炸效果”，替代传统实拍（如《阿凡达 2》用 GAN 生成海洋生物，节省数百万美元制作费用）。

*   **广告与设计**：


    *   生成产品宣传图（如服装、家具），支持 “实时换色、换材质”（如阿里巴巴的 “AI Design”，设计师输入草图，AI 生成多种风格的商品图）。
    
    *   平面设计：自动生成海报、LOGO，根据品牌风格调整颜色和布局。

#### 2. 医疗健康



*   **医学影像生成**：


    *   生成罕见病例的医学影像（如脑瘤 CT 图像），扩大训练集，提升 AI 诊断模型的泛化能力（如 DeepMind 的 MedGAN，生成的影像被用于肺癌早期诊断模型训练）。
    
    *   影像修复：去除医学影像中的噪声、伪影（如 MRI 图像的运动伪影），提升图像质量，辅助医生诊断。

*   **药物研发**：


    *   生成新型分子结构（如 GAN 生成潜在的药物分子），预测分子的活性和毒性，加速药物筛选过程（如 Insilico Medicine 的 GAN 模型，已用于开发治疗纤维化的候选药物）。

#### 3. 虚拟现实（VR）与增强现实（AR）



*   **虚拟形象生成（Avatar）**：


    *   输入用户照片，生成 3D 虚拟形象（如 Meta 的 Avatar Generator，用于 VR 社交平台 Horizon Worlds），支持表情、动作实时驱动。

*   **场景生成**：


    *   实时生成 VR 场景（如 “虚拟公园”“虚拟城市”），根据用户的移动轨迹动态扩展场景，避免重复和加载延迟（如 NVIDIA 的 VRGAN，支持无限大场景生成）。

*   **AR 滤镜**：


    *   生成实时 AR 特效（如 “换脸”“妆容”“虚拟道具”），通过 GAN 实时优化特效的真实感，使虚拟元素与真实场景融合更自然（如 TikTok 的 AR 滤镜，日活用户超 10 亿）。

#### 4. 数据安全与隐私保护



*   **合成数据生成**：


    *   生成与真实数据分布一致的 “合成数据”（如用户行为数据、金融交易数据），用于模型训练或算法测试，避免真实数据泄露（如 Google 的 Synthea，生成合成电子病历，用于医疗 AI 研发）。

*   **隐私保护图像**：


    *   将真实人脸图像转换为 “匿名化合成人脸”（如 Microsoft 的 FaceGAN），保留人脸的特征（如年龄、性别）但无法识别具体个人，用于公共数据集构建（如人脸识别算法测试集）。

### 第 54 页：生成模型面临的挑战与伦理问题

#### 1. 技术挑战



*   **生成质量与效率的平衡**：


    *   高分辨率（4K+）生成仍需大量计算资源（如 GigaGAN 生成 4K 图像需 GPU 显存≥24GB），难以在移动端部署。
    
    *   复杂场景生成（如 “多人互动、动态场景”）易出现 “结构错误”（如人物手脚扭曲、物体遮挡异常）。

*   **可控性与创造性的平衡**：


    *   现有模型对 “精细结构” 的控制能力不足（如生成 “特定字体的文字”“特定品牌的 LOGO”），易出现模糊或错误。
    
    *   过度控制可能限制模型的创造性，如何在 “可控” 和 “创新” 之间找到平衡是关键。

*   **泛化能力**：


    *   模型在训练数据分布外的场景表现差（如用 “白天风景” 训练的模型，生成 “夜晚风景” 时细节缺失）。
    
    *   跨领域迁移能力弱（如从 “2D 图像生成” 迁移到 “3D 模型生成”，需重新训练大量数据）。

#### 2. 伦理与安全问题



*   **虚假信息（Deepfake）**：


    *   生成逼真的虚假视频（如伪造名人讲话、虚假新闻），可能被用于造谣、诈骗、政治操纵（如 2020 年美国大选期间出现的伪造候选人视频）。
    
    *   解决方案：研发 Deepfake 检测技术（如分析图像的 “生理信号”“光照不一致性”），建立虚假信息溯源机制。

*   **版权问题**：


    *   生成模型训练数据包含大量受版权保护的图像、文本（如艺术作品、摄影作品），未经授权使用可能侵犯知识产权。
    
    *   争议：生成的图像是否属于 “原创”？训练数据的版权如何界定？（如 2023 年 Getty Images 起诉 Stability AI，指控其使用版权图像训练 Stable Diffusion）。

*   **偏见与歧视**：


    *   训练数据中的偏见（如性别、种族、地域偏见）会被模型学习并放大（如生成 “医生” 图像时，90% 为男性；生成 “护士” 图像时，90% 为女性）。
    
    *   解决方案：构建均衡的训练数据集，在模型训练中加入 “去偏见损失”，定期评估模型的公平性。

*   **滥用风险**：


    *   生成色情、暴力、恐怖图像，传播不良信息（如 “深度伪造色情视频” 侵犯个人隐私）。
    
    *   应对措施：建立内容审核机制，开发 “内容过滤插件”，限制模型的恶意使用。

### 第 55 页：本讲总结（CNN 与生成模型）

#### 1. 卷积神经网络（CNN）核心回顾



*   **核心优势**：通过 “局部连接、权重共享、层级特征提取”，高效处理视觉数据，解决了 MLP 丢失空间信息、参数爆炸的问题。

*   **关键组件**：


    *   卷积层（Conv Layer）：提取局部特征，通过多卷积核捕捉不同模式（边缘、纹理、部件）。
    
    *   池化层（Pool Layer）：降采样，增强空间不变性（Max Pooling 保留强特征，Mean Pooling 保留全局信息）。
    
    *   全连接层（FC Layer）：融合全局特征，实现分类决策（现代网络中逐渐被全局池化替代，减少参数）。

*   **经典模型演进**：


    *   LeNet（1989）：CNN 雏形，用于手写数字识别。
    
    *   AlexNet（2012）：深度 CNN 里程碑，ReLU+GPU 训练，开启深度学习视觉时代。
    
    *   VGG（2014）：小核堆叠（3×3），增强特征表达，成为迁移学习基础。
    
    *   GoogLeNet（2014）：Inception 模块，多尺度特征融合，高效轻量化。
    
    *   ResNet（2015）：残差连接，解决深层网络退化，实现超深网络（1000 + 层）训练。

#### 2. 生成模型核心回顾



*   **核心目标**：学习真实数据分布，生成全新的、逼真的样本，从 “判别” 走向 “创造”。

*   **三大模型家族**：


    *   GAN：对抗博弈，生成质量高、速度快，但训练不稳定、易模式崩溃（变体：DCGAN、CGAN、CycleGAN、StyleGAN）。
    
    *   VAE：概率生成，训练稳定、潜在空间可解释，但生成质量低、细节模糊（用于数据压缩、重构）。
    
    *   扩散模型：逐步去噪，生成质量最高、稳定无崩溃，但速度慢（变体：Stable Diffusion、DreamFusion，通过潜在扩散加速）。

*   **技术趋势**：


    *   高效化：减少计算量，适配移动端、边缘设备。
    
    *   可控化：通过文本、图像、3D 等条件，精确控制生成内容。
    
    *   多模态：融合文本、图像、音频、3D，实现跨模态生成（如 “文本→图像→3D 模型”）。

#### 3. 关键技术启示



*   **数据驱动**：高质量、大规模数据是模型性能的基础，数据增强、合成数据可缓解数据稀缺问题。

*   **网络设计**：从 “手工设计”（如 VGG 的小核、ResNet 的残差连接）到 “自动搜索”（如神经架构搜索 NAS，Neural Architecture Search），不断优化网络效率与性能。例如，NAS 通过强化学习或进化算法自动搜索最优网络结构（如 Google 的 NASNet，性能超越手工设计的 ResNet），减少人工经验依赖。

*   **训练技巧**：批量归一化（BN）、梯度裁剪（Gradient Clipping）、学习率调度（Learning Rate Scheduling）等技巧，是深度网络稳定训练的关键。例如，BN 通过标准化层输入，使 ResNet 等超深网络的训练收敛速度提升 50% 以上；梯度裁剪通过限制梯度范数（如 max\_norm=1.0），避免梯度爆炸。

*   **跨领域融合**：CNN 与生成模型的结合，推动了计算机视觉从 “判别任务” 向 “生成 + 判别融合任务” 发展。例如，用生成模型（如 GAN）生成合成数据，补充训练 CNN 判别模型，提升小数据集场景下的泛化能力；用 CNN 提取的特征指导生成模型，提升生成样本的结构合理性。

### 第 56 页：下一讲预告 —— 目标检测（Object Detection）

#### 1. 目标检测的任务定义

目标检测是计算机视觉的核心任务之一，旨在**同时解决 “是什么（类别）” 和 “在哪里（位置）”** 两个问题：



*   “是什么”：识别图像中每个目标所属的类别（如 “人”“车”“猫”“狗”，通常为预定义的 N 个类别）。

*   “在哪里”：用**边界框（Bounding Box）** 标记每个目标在图像中的位置，边界框通常用 “左上角坐标 (x1,y1) + 右下角坐标 (x2,y2)” 或 “中心点坐标 ( cx, cy ) + 宽高 ( w, h )” 表示。

**示例**：输入一张街景图像，目标检测模型需输出：



*   类别 1：“车”，置信度（Confidence）0.92，边界框 (x1=100,y1=200,x2=300,y2=400)

*   类别 2：“人”，置信度 0.87，边界框 (x1=400,y1=350,x2=450,y2=500)

*   类别 3：“交通灯”，置信度 0.75，边界框 (x1=250,y1=50,x2=280,y2=120)

#### 2. 目标检测与分类任务的核心差异



| 任务类型                     | 输入     | 输出内容                                | 核心挑战                                                     |
| ---------------------------- | -------- | --------------------------------------- | ------------------------------------------------------------ |
| 图像分类（Classification）   | 单张图像 | 1 个类别标签 + 1 个置信度               | 区分不同类别，无需定位目标                                   |
| 目标检测（Object Detection） | 单张图像 | N 个目标的 “类别标签 + 置信度 + 边界框” | 1. 检测图像中所有目标（数量不固定）；2. 精确预测每个目标的边界框；3. 处理目标重叠、尺度变化、遮挡等场景 |

#### 3. 目标检测的典型应用场景



*   **智能交通**：检测道路中的车辆、行人、非机动车，实现闯红灯识别、超速抓拍、交通流量统计（如特斯拉 Autopilot 的视觉感知模块，依赖目标检测识别前方车辆和行人）。

*   **安防监控**：检测监控画面中的异常目标（如陌生人、危险物品），触发报警（如商场监控中检测到未戴口罩的人员，自动提醒）。

*   **自动驾驶**：实时检测周围环境中的车辆、行人、交通标志、车道线，为决策系统提供环境信息（如 Waymo 的自动驾驶系统，每秒处理数十帧图像，检测上百个目标）。

*   **医疗影像**：检测医学影像中的病变区域（如 CT 图像中的肿瘤、X 光图像中的骨折部位），辅助医生诊断（如肺结节检测模型，灵敏度可达 95% 以上）。

#### 4. 下一讲核心内容预告



*   **经典目标检测算法**：从传统方法（如 HOG+SVM）到深度学习方法（如 R-CNN 系列、YOLO 系列、SSD），梳理算法演进脉络。

*   **Two-Stage 检测框架**：详解 R-CNN、Fast R-CNN、Faster R-CNN 的原理，包括 “候选区域生成（Region Proposal）”“特征提取”“分类与回归” 三个核心步骤。

*   **One-Stage 检测框架**：解析 YOLO（You Only Look Once）、SSD（Single Shot MultiBox Detector）的 “端到端检测” 思路，对比 Two-Stage 的精度与速度权衡。

*   **实践与优化**：介绍目标检测的评估指标（如 mAP、IOU）、数据增强技巧（如水平翻转、随机裁剪、MixUp），以及工程化部署中的性能优化方法。

#### 5. 目标检测的关键评估指标预告



*   **交并比（IOU，Intersection over Union）**：衡量预测边界框与真实边界框的重叠程度，公式为$IOU = \frac{Area(预测框 \cap 真实框)}{Area(预测框 \cup 真实框)}$，通常 IOU≥0.5 时认为检测成功。

*   **平均精度（AP，Average Precision）**：针对单个类别，计算 Precision-Recall 曲线下的面积，衡量该类别检测的 “精度” 与 “召回率” 平衡。

*   **均值平均精度（mAP，mean Average Precision）**：计算所有类别的 AP 平均值，是目标检测任务的核心评估指标（如 COCO 数据集评估标准中，mAP@IOU=0.5:0.95 为综合衡量指标）。